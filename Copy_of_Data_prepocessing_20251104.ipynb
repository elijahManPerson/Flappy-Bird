{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3hJ6Z9sLWWCyYmOg0iUs0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elijahManPerson/Flappy-Bird/blob/master/Copy_of_Data_prepocessing_20251104.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# AES Step 1→9: Clean, run-ready Colab block (v2025-11-03.cleaned)\n",
        "#\n",
        "# This script wraps the entire AES text processing pipeline into a single file.\n",
        "# It removes duplicated cells, removes leaked keys, keeps the deterministic\n",
        "# dialogue alignment, and keeps the sentence boundary flags.\n",
        "\n",
        "# ---------- 0) Drive + imports ----------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, time, re, io, csv, json, math, logging, zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def status(msg, ok=True):\n",
        "    print((\"✅ \" if ok else \"❌ \") + msg)\n",
        "\n",
        "# Mount check\n",
        "root_mount = '/content/drive'\n",
        "root_mydrive = '/content/drive/MyDrive'\n",
        "status(\"Drive mount detected at /content/drive\", os.path.ismount(root_mount))\n",
        "status(\"MyDrive folder present\", os.path.isdir(root_mydrive))\n",
        "\n",
        "# Probe read/write\n",
        "try:\n",
        "    sample = os.listdir(root_mydrive)[:5]\n",
        "    status(\"Read test passed (listed MyDrive)\")\n",
        "except Exception as e:\n",
        "    status(f\"Read test failed: {e}\", False)\n",
        "\n",
        "probe = os.path.join(root_mydrive, \"_colab_mount_check.txt\")\n",
        "try:\n",
        "    with open(probe, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"colab mount check {time.time()}\\n\")\n",
        "    status(\"Write test passed (created file)\")\n",
        "    os.remove(probe)\n",
        "    status(\"Cleanup passed (deleted file)\")\n",
        "except Exception as e:\n",
        "    status(f\"Write test failed: {e}\", False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BkRyR5bjMO4",
        "outputId": "1d14e7af-6e48-476f-99c4-f4c4a9728e87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Drive mount detected at /content/drive\n",
            "✅ MyDrive folder present\n",
            "✅ Read test passed (listed MyDrive)\n",
            "✅ Write test passed (created file)\n",
            "✅ Cleanup passed (deleted file)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ---------- 1) Data in ----------\n",
        "DATA_PATH = \"/content/drive/MyDrive/JM/Sandbox/1.Training Data/Data for Testing avg short.csv\"\n",
        "RAW_TEXT_ALIASES = {\"raw text\",\"raw_text\",\"rawtext\"}\n",
        "\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    status(f\"File not found: {DATA_PATH}\", False)\n",
        "    raise FileNotFoundError(DATA_PATH)\n",
        "\n",
        "def try_read(path, sep, engine=None):\n",
        "    kw = dict(encoding=\"utf-8-sig\", on_bad_lines=\"skip\", low_memory=False)\n",
        "    if sep is None:\n",
        "        kw[\"sep\"] = None\n",
        "        kw[\"engine\"] = \"python\"\n",
        "    else:\n",
        "        kw[\"sep\"] = sep\n",
        "        if engine:\n",
        "            kw[\"engine\"] = engine\n",
        "    try:\n",
        "        return pd.read_csv(path, **kw)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "cands = [\n",
        "    (\"auto\", None, \"python\"),\n",
        "    (\"comma\", \",\" , None),\n",
        "    (\"semicolon\", \";\", None),\n",
        "    (\"tab\", \"\\t\", None),\n",
        "    (\"pipe\", \"|\", None)\n",
        "]\n",
        "best, best_score, parsed_by = None, (-1, -1), None\n",
        "\n",
        "def score(df):\n",
        "    if df is None or df.empty:\n",
        "        return (-1, -1)\n",
        "    cols = [c.strip().lower() for c in df.columns]\n",
        "    has_raw = int(any(c in RAW_TEXT_ALIASES for c in cols) or (\"raw\" in cols and \"text\" in cols))\n",
        "    return (has_raw, len(cols))\n",
        "\n",
        "for name, sep, eng in cands:\n",
        "    df_ = try_read(DATA_PATH, sep, eng)\n",
        "    s = score(df_)\n",
        "    if s > best_score:\n",
        "        best, best_score, parsed_by = df_, s, name\n",
        "\n",
        "if best is None or best.empty:\n",
        "    status(\"Failed to read CSV\", False)\n",
        "    raise ValueError(\"Could not parse CSV\")\n",
        "\n",
        "status(f\"Parsed using: {parsed_by}. Columns: {len(best.columns)}\")\n",
        "df_pre = best.copy()\n",
        "\n",
        "# Ensure 'Raw text'\n",
        "cols_norm = {c: c.strip().lower() for c in df_pre.columns}\n",
        "raw_col = None\n",
        "for c, n in cols_norm.items():\n",
        "    if n in RAW_TEXT_ALIASES:\n",
        "        raw_col = c\n",
        "        break\n",
        "\n",
        "if raw_col is None and \"raw\" in cols_norm.values() and \"text\" in cols_norm.values():\n",
        "    raw_name  = next(k for k,v in cols_norm.items() if v==\"raw\")\n",
        "    text_name = next(k for k,v in cols_norm.items() if v==\"text\")\n",
        "    df_pre[\"Raw text\"] = (\n",
        "        df_pre[raw_name].astype(str).fillna(\"\") + \" \" +\n",
        "        df_pre[text_name].astype(str).fillna(\"\")\n",
        "    ).str.strip()\n",
        "    status(f\"Merged '{raw_name}' + '{text_name}' into 'Raw text'\")\n",
        "else:\n",
        "    if raw_col is None:\n",
        "        status(\"Raw text column not found after parsing\", False)\n",
        "        print(\"Columns present:\", list(df_pre.columns))\n",
        "        raise KeyError(\"'Raw text' column is missing\")\n",
        "    if raw_col != \"Raw text\":\n",
        "        df_pre.rename(columns={raw_col: \"Raw text\"}, inplace=True)\n",
        "        status(f\"Renamed '{raw_col}' to 'Raw text'\")\n",
        "\n",
        "df_pre[\"Raw text\"] = df_pre[\"Raw text\"].fillna(\"\").astype(str)\n",
        "\n",
        "# Canonical ID\n",
        "def _normalize_id_series(s: pd.Series) -> pd.Series:\n",
        "    s = s.astype(str).str.replace(r\"\\.0$\", \"\", regex=True)\n",
        "    def _fix(x):\n",
        "        if any(c.isalpha() for c in x):\n",
        "            return x\n",
        "        try:\n",
        "            if \".\" in x or \"e\" in x.lower():\n",
        "                f = float(x)\n",
        "                if f.is_integer():\n",
        "                    return str(int(f))\n",
        "        except Exception:\n",
        "            pass\n",
        "        return x\n",
        "    return s.map(_fix)\n",
        "\n",
        "def find_id_column(cols):\n",
        "    cols = list(cols)\n",
        "    if not cols:\n",
        "        return None\n",
        "    if re.search(r\"id|identifier\", cols[0], flags=re.I):\n",
        "        return cols[0]\n",
        "    for name in cols[1:]:\n",
        "        if re.search(r\"id|identifier\", name, flags=re.I):\n",
        "            return name\n",
        "    return cols[0]\n",
        "\n",
        "CANON_ID = find_id_column(df_pre.columns)\n",
        "df_pre[\"ID\"] = _normalize_id_series(df_pre[CANON_ID])\n",
        "\n",
        "total = len(df_pre)\n",
        "usable = total - df_pre[\"Raw text\"].str.strip().eq(\"\").sum()\n",
        "status(f\"Non-empty 'Raw text' rows: {usable} of {total}\")\n",
        "print(df_pre[[\"ID\",\"Raw text\"]].head(3))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ise_UnIit4D",
        "outputId": "3312bdb4-249e-4493-92a8-8a3dba4015ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Parsed using: comma. Columns: 15\n",
            "✅ Non-empty 'Raw text' rows: 21 of 21\n",
            "         ID                                           Raw text\n",
            "0  BBCMHJPT  There once was a girl called lilly she had pet...\n",
            "1  BBKBYNDW  wrire a narrative story abouta search for some...\n",
            "2  BBRWTLYV  The Failed Submarine I had always wanted go on...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------- 2) Deps ----------\n",
        "!pip -q install --upgrade \"openai==1.*\" tqdm nltk tiktoken spacy pandas==2.2.2 ipywidgets openpyxl jsonschema backoff\n",
        "!python -m spacy download en_core_web_sm -q\n",
        "\n",
        "import importlib, spacy, nltk, tiktoken, backoff\n",
        "from tqdm import tqdm\n",
        "\n",
        "def v(name):\n",
        "    try:\n",
        "        m = importlib.import_module(name)\n",
        "        return getattr(m, \"__version__\", \"unknown\")\n",
        "    except Exception as e:\n",
        "        return f\"import failed: {e}\"\n",
        "\n",
        "mods = [\"openai\",\"pandas\",\"spacy\",\"nltk\",\"tiktoken\",\"tqdm\",\"backoff\"]\n",
        "print(\"Versions:\", {m: v(m) for m in mods})\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "print(\"spaCy ok:\", [t.text for t in nlp(\"A tiny sanity check.\")])\n",
        "\n",
        "os.makedirs(\"/content/nltk_data\", exist_ok=True)\n",
        "if \"/content/nltk_data\" not in nltk.data.path:\n",
        "    nltk.data.path.insert(0, \"/content/nltk_data\")\n",
        "try:\n",
        "    nltk.data.find(\"tokenizers/punkt\")\n",
        "    print(\"NLTK punkt ok\")\n",
        "except LookupError:\n",
        "    nltk.download(\"punkt\", download_dir=\"/content/nltk_data\", quiet=False)\n",
        "    nltk.data.find(\"tokenizers/punkt\")\n",
        "    print(\"NLTK punkt downloaded\")\n",
        "\n",
        "try:\n",
        "    enc = tiktoken.get_encoding(\"o200k_base\")\n",
        "except Exception:\n",
        "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "print(\"tiktoken ok, sample tokens:\", len(enc.encode(\"Tokenization sanity check.\")))\n",
        "\n",
        "\n",
        "# ---------- 3) OpenAI config ----------\n",
        "import os\n",
        "from getpass import getpass\n",
        "from openai import OpenAI, BadRequestError\n",
        "\n",
        "MODEL_ID = os.environ.get(\"AES_MODEL_ID\", \"gpt-4o\")\n",
        "USE_MOCK = os.environ.get(\"AES_USE_MOCK\", \"0\").lower() in {\"1\",\"true\",\"yes\"}\n",
        "\n",
        "def ensure_api_key():\n",
        "    key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "    if key:\n",
        "        return key\n",
        "    print(\"Enter your OpenAI API key (hidden). Leave blank for mock mode.\")\n",
        "    key = getpass(\"API key: \").strip()\n",
        "    if key:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = key\n",
        "    return key or None\n",
        "\n",
        "def verify_openai(model_id: str = MODEL_ID):\n",
        "    key = ensure_api_key()\n",
        "    if not key:\n",
        "        print(\"No key supplied. Using mock corrections.\")\n",
        "        return None\n",
        "    client = OpenAI()\n",
        "    try:\n",
        "        _ = client.models.list().data[:1]\n",
        "        try:\n",
        "            _ = client.models.retrieve(model_id)\n",
        "        except Exception:\n",
        "            print(f\"Model '{model_id}' not retrieved. Will still try.\")\n",
        "        print(\"API key verified.\")\n",
        "        return client\n",
        "    except Exception as e:\n",
        "        print(\"Verification failed:\", type(e).__name__, str(e))\n",
        "        return None\n",
        "\n",
        "client = None if USE_MOCK else verify_openai(MODEL_ID)\n",
        "if client is None:\n",
        "    USE_MOCK = True\n",
        "    print(\"Running in MOCK mode.\")\n",
        "\n",
        "\n",
        "# ---------- 4) Text utils ----------\n",
        "_MOJIBAKE_FIXES = [\n",
        "    (r\"â€”\",\"—\"), (r\"â€“\",\"–\"), (r\"â€˜\",\"‘\"), (r\"â€™\",\"’\"),\n",
        "    (r\"â€œ\",\"“\"), (r\"â€\",\"”\"), (r\"â€¦\",\"…\"), (r\"Â \",\" \")\n",
        "]\n",
        "def normalize_mojibake(s: str) -> str:\n",
        "    out = str(s or \"\")\n",
        "    for pat, rep in _MOJIBAKE_FIXES:\n",
        "        out = re.sub(pat, rep, out)\n",
        "    return out\n",
        "\n",
        "def normalise_punct(s: str) -> str:\n",
        "    s = normalize_mojibake(str(s))\n",
        "    s = s.replace(\"’\", \"'\")\n",
        "    s = s.replace(\"––\", \"—\").replace(\"–\", \"—\").replace(\"--\", \"—\").replace(\"―\", \"—\")\n",
        "    return s\n",
        "\n",
        "def collapse_ellipsis(text: str) -> str:\n",
        "    return re.sub(r\"\\.{3,}\", \"…\", str(text))\n",
        "\n",
        "def _extract_first_json_object(txt: str):\n",
        "    if not txt:\n",
        "        return None\n",
        "    start = txt.find(\"{\")\n",
        "    if start < 0:\n",
        "        return None\n",
        "    depth, in_str, esc = 0, False, False\n",
        "    for i in range(start, len(txt)):\n",
        "        ch = txt[i]\n",
        "        if in_str:\n",
        "            if esc:\n",
        "                esc = False\n",
        "            elif ch == \"\\\\\":\n",
        "                esc = True\n",
        "            elif ch == '\"':\n",
        "                in_str = False\n",
        "        else:\n",
        "            if ch == '\"':\n",
        "                in_str = True\n",
        "            elif ch == \"{\":\n",
        "                depth += 1\n",
        "            elif ch == \"}\":\n",
        "                depth -= 1\n",
        "                if depth == 0:\n",
        "                    frag = txt[start:i+1]\n",
        "                    try:\n",
        "                        return json.loads(frag)\n",
        "                    except Exception:\n",
        "                        return None\n",
        "    return None\n",
        "\n",
        "# Dialogue helpers\n",
        "QUOTE_OPENERS = {'\"', '“'}\n",
        "QUOTE_CLOSERS = {'\"', '”'}\n",
        "SQUOTE_OPENERS = {'‘'}\n",
        "SQUOTE_CLOSERS = {'’'}\n",
        "_WORD_CHAR = re.compile(r\"\\w\", flags=re.UNICODE)\n",
        "\n",
        "def _looks_like_apostrophe(text, i):\n",
        "    if i <= 0 or i >= len(text) - 1:\n",
        "        return False\n",
        "    return (_WORD_CHAR.match(text[i-1] or \"\") and _WORD_CHAR.match(text[i+1] or \"\"))\n",
        "\n",
        "def extract_dialogue_spans_deterministic(text: str):\n",
        "    s = str(text or \"\")\n",
        "    spans = []\n",
        "    i = 0\n",
        "    open_char = None\n",
        "    start_idx = None\n",
        "    while i < len(s):\n",
        "        ch = s[i]\n",
        "        is_double_open  = (ch in QUOTE_OPENERS or ch == '\"') and not _looks_like_apostrophe(s, i)\n",
        "        is_double_close = ch in QUOTE_CLOSERS or ch == '\"'\n",
        "        is_single_open  = ch in SQUOTE_OPENERS and not _looks_like_apostrophe(s, i)\n",
        "        is_single_close = ch in SQUOTE_CLOSERS\n",
        "        if open_char is None:\n",
        "            if is_double_open or is_single_open:\n",
        "                open_char = ch\n",
        "                start_idx = i\n",
        "        else:\n",
        "            if open_char in QUOTE_OPENERS or open_char == '\"':\n",
        "                if is_double_close:\n",
        "                    spans.append({\"start\": start_idx, \"end\": i+1})\n",
        "                    open_char, start_idx = None, None\n",
        "            elif open_char in SQUOTE_OPENERS:\n",
        "                if is_single_close:\n",
        "                    spans.append({\"start\": start_idx, \"end\": i+1})\n",
        "                    open_char, start_idx = None, None\n",
        "        i += 1\n",
        "    if start_idx is not None:\n",
        "        spans.append({\"start\": start_idx, \"end\": len(s)})\n",
        "    out = []\n",
        "    for sp in spans:\n",
        "        st = max(0, min(int(sp[\"start\"]), len(s)))\n",
        "        en = max(st+1, min(int(sp[\"end\"]), len(s)))\n",
        "        out.append({\"start\": st, \"end\": en})\n",
        "    return out\n",
        "\n",
        "def _safe_load_json_list(x):\n",
        "    if isinstance(x, list):\n",
        "        return x\n",
        "    if isinstance(x, str):\n",
        "        s = x.strip()\n",
        "        if not s:\n",
        "            return []\n",
        "        try:\n",
        "            val = json.loads(s)\n",
        "            return val if isinstance(val, list) else []\n",
        "        except Exception:\n",
        "            return []\n",
        "    return []\n",
        "\n",
        "def _spans_align_with_quotes(text, spans):\n",
        "    t = str(text or \"\")\n",
        "    for sp in spans:\n",
        "        st = sp.get(\"start\", -1)\n",
        "        en = sp.get(\"end\", -1)\n",
        "        if not (0 <= st < en <= len(t)):\n",
        "            return False\n",
        "        if t[st] not in QUOTE_OPENERS.union({'\"'}).union(SQUOTE_OPENERS):\n",
        "            return False\n",
        "        if t[en-1] not in QUOTE_CLOSERS.union({'\"'}).union(SQUOTE_CLOSERS):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def ensure_dialogue_json(df_texts, corrected_col=\"Corrected text (8)\"):\n",
        "    df = df_texts.copy()\n",
        "    if \"DialogueSpansJSON\" not in df.columns:\n",
        "        df[\"DialogueSpansJSON\"] = \"[]\"\n",
        "    clean_col = []\n",
        "    corr_texts = df.get(corrected_col, pd.Series([\"\"]*len(df), dtype=object)).astype(str)\n",
        "    for s, corr in zip(df[\"DialogueSpansJSON\"], corr_texts):\n",
        "        spans = _safe_load_json_list(s)\n",
        "        if not spans or not _spans_align_with_quotes(corr, spans):\n",
        "            spans = extract_dialogue_spans_deterministic(corr)\n",
        "        spans = [\n",
        "            {\n",
        "                \"start\": int(max(0, min(sp[\"start\"], len(corr)))),\n",
        "                \"end\":   int(max(0, min(sp[\"end\"],   len(corr))))\n",
        "            }\n",
        "            for sp in spans\n",
        "            if sp.get(\"end\",0) > sp.get(\"start\",0)\n",
        "        ]\n",
        "        clean_col.append(json.dumps(spans, ensure_ascii=False))\n",
        "    df[\"DialogueSpansJSON\"] = clean_col\n",
        "    if \"NarrativeTagsJSON\" not in df.columns:\n",
        "        df[\"NarrativeTagsJSON\"] = \"[]\"\n",
        "    return df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccROgywWiv2j",
        "outputId": "2172f380-7e27-4584-ffb5-ec161368a5b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.5 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Versions: {'openai': '1.109.1', 'pandas': '2.2.2', 'spacy': '3.8.7', 'nltk': '3.9.2', 'tiktoken': '0.12.0', 'tqdm': '4.67.1', 'backoff': '2.2.1'}\n",
            "spaCy ok: ['A', 'tiny', 'sanity', 'check', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /content/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK punkt downloaded\n",
            "tiktoken ok, sample tokens: 5\n",
            "Enter your OpenAI API key (hidden). Leave blank for mock mode.\n",
            "API key: ··········\n",
            "API key verified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QatPlgm6QRgw"
      },
      "source": [
        "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\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ---------- 5) Step 8 correction ----------\n",
        "_DEFAULT_MAX_TOKENS = 1500\n",
        "\n",
        "@backoff.on_exception(backoff.expo, (Exception,), max_tries=5, factor=2)\n",
        "def _openai_chat_complete(client: OpenAI, model: str, prompt: str, max_tokens: int = _DEFAULT_MAX_TOKENS):\n",
        "    def _compat_obj(text: str):\n",
        "        return type(\"Compat\", (), {\"text\": (text or \"\").strip()})\n",
        "\n",
        "    def _chat_try(params):\n",
        "        return client.chat.completions.create(**params)\n",
        "\n",
        "    def _responses_try(params):\n",
        "        r = client.responses.create(**params)\n",
        "        text = getattr(r, \"output_text\", None)\n",
        "        if text:\n",
        "            return _compat_obj(text)\n",
        "        out = []\n",
        "        for item in getattr(r, \"output\", []) or []:\n",
        "            if getattr(item, \"type\", \"\") == \"output_text\":\n",
        "                out.append(item.text)\n",
        "        return _compat_obj(\"\".join(out))\n",
        "\n",
        "    chat_base = dict(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        resp = _chat_try({**chat_base, \"temperature\": 0.0, \"max_tokens\": max_tokens})\n",
        "        return _compat_obj(resp.choices[0].message.content)\n",
        "    except BadRequestError as e:\n",
        "        msg = str(e)\n",
        "        if \"Unsupported value: 'temperature'\" in msg:\n",
        "            resp = _chat_try({**chat_base, \"max_tokens\": max_tokens})\n",
        "            return _compat_obj(resp.choices[0].message.content)\n",
        "        if \"Unsupported parameter\" in msg and \"'max_tokens'\" in msg:\n",
        "            try:\n",
        "                resp = _chat_try({**chat_base, \"temperature\": 0.0, \"max_completion_tokens\": max_tokens})\n",
        "                return _compat_obj(resp.choices[0].message.content)\n",
        "            except BadRequestError as e2:\n",
        "                msg2 = str(e2)\n",
        "                if \"Unsupported value: 'temperature'\" in msg2:\n",
        "                    resp = _chat_try({**chat_base, \"max_completion_tokens\": max_tokens})\n",
        "                    return _compat_obj(resp.choices[0].message.content)\n",
        "                raise\n",
        "        raise\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        return _responses_try(dict(\n",
        "            model=model,\n",
        "            input=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.0,\n",
        "            max_completion_tokens=max_tokens,\n",
        "        ))\n",
        "    except BadRequestError as e:\n",
        "        msg = str(e)\n",
        "        if \"Unsupported value: 'temperature'\" in msg:\n",
        "            return _responses_try(dict(\n",
        "                model=model,\n",
        "                input=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_completion_tokens=max_tokens,\n",
        "            ))\n",
        "        if \"Unsupported parameter\" in msg and \"'max_completion_tokens'\" in msg:\n",
        "            try:\n",
        "                return _responses_try(dict(\n",
        "                    model=model,\n",
        "                    input=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    temperature=0.0,\n",
        "                    max_tokens=max_tokens,\n",
        "                ))\n",
        "            except BadRequestError as e2:\n",
        "                if \"Unsupported value: 'temperature'\" in str(e2):\n",
        "                    return _responses_try(dict(\n",
        "                        model=model,\n",
        "                        input=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                        max_tokens=max_tokens,\n",
        "                    ))\n",
        "                raise\n",
        "        raise\n",
        "\n",
        "def _model_complete_text(client: OpenAI, model: str, prompt: str, max_tokens: int = _DEFAULT_MAX_TOKENS) -> str:\n",
        "    resp = _openai_chat_complete(client, model, prompt, max_tokens=max_tokens)\n",
        "    return resp.text\n",
        "\n",
        "def correct_with_tags(raw: str, client=None, model=MODEL_ID, use_mock=USE_MOCK, max_tokens=1500):\n",
        "    s = normalize_mojibake(str(raw or \"\"))\n",
        "    if use_mock or client is None:\n",
        "        t = s.strip()\n",
        "        m = re.search(r\"[A-Za-z0-9]\", t)\n",
        "        if m:\n",
        "            i = m.start()\n",
        "            ch = t[i]\n",
        "            if ch.isalpha():\n",
        "                t = t[:i] + ch.upper() + t[i+1:]\n",
        "        if t and not re.search(r\"[.!?…]\\s*$\", t):\n",
        "            t += \".\"\n",
        "        t = collapse_ellipsis(normalise_punct(t))\n",
        "        return t, [], [], \"mock\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a meticulous copy editor. Fix punctuation, grammar, and spelling.\n",
        "Keep meaning and paragraphing. Use standard English punctuation.\n",
        "\n",
        "Then return character-offset spans for:\n",
        "1) narrative_tags: list of objects with fields:\n",
        "   - type: one of \"title\", \"temporal\", \"closure\"\n",
        "   - text: exact substring from corrected_text\n",
        "   - start: integer (0-based, inclusive)\n",
        "   - end: integer (0-based, exclusive)\n",
        "2) dialogue_spans: list of objects with fields:\n",
        "   - start: integer (0-based, inclusive)\n",
        "   - end: integer (0-based, exclusive)\n",
        "\n",
        "CRITICAL RULES:\n",
        "- All offsets refer to corrected_text AFTER your edits.\n",
        "- 0 <= start < end <= len(corrected_text)\n",
        "- narrative_tags[i].text MUST equal corrected_text[start:end] exactly.\n",
        "- If there are no tags or dialogue, return empty lists.\n",
        "\n",
        "DIALOGUE SPAN RULES:\n",
        "- Each span must begin at an opening quote character and end just after the matching closing quote character.\n",
        "- Include all characters between the quotes.\n",
        "- Treat \" “ ” ‘ ’ as quotes. Do not treat apostrophes in words as quotes.\n",
        "- If a quote is unclosed, span ends at the first terminal clause or the end.\n",
        "\n",
        "Output JSON only in this schema:\n",
        "{{\n",
        "  \"corrected_text\": \"…\",\n",
        "  \"narrative_tags\": [],\n",
        "  \"dialogue_spans\": []\n",
        "}}\n",
        "\n",
        "Text:\n",
        "<<<BEGIN>>>\n",
        "{s}\n",
        "<<<END>>>\n",
        "\"\"\".strip()\n",
        "\n",
        "    out = _model_complete_text(client, model, prompt, max_tokens=max_tokens)\n",
        "    js = _extract_first_json_object(out)\n",
        "    if not isinstance(js, dict):\n",
        "        t = collapse_ellipsis(normalise_punct(s))\n",
        "        return t, [], [], \"fallback\"\n",
        "\n",
        "    corrected = (js.get(\"corrected_text\") or \"\").rstrip()\n",
        "    tags = js.get(\"narrative_tags\") or []\n",
        "    spans = js.get(\"dialogue_spans\") or []\n",
        "\n",
        "    N = len(corrected)\n",
        "    def _clip(a, b):\n",
        "        a = max(0, int(a))\n",
        "        b = max(a, int(b))\n",
        "        return (a if a <= N else N, b if b <= N else N)\n",
        "\n",
        "    clean_tags, clean_spans = [], []\n",
        "    for t in tags:\n",
        "        try:\n",
        "            st, en = _clip(t.get(\"start\", 0), t.get(\"end\", 0))\n",
        "            if en > st and t.get(\"type\") in {\"title\", \"temporal\", \"closure\"}:\n",
        "                clean_tags.append({\n",
        "                    \"type\": t[\"type\"],\n",
        "                    \"text\": corrected[st:en],\n",
        "                    \"start\": st,\n",
        "                    \"end\": en\n",
        "                })\n",
        "        except Exception:\n",
        "            pass\n",
        "    for d in spans:\n",
        "        try:\n",
        "            st, en = _clip(d.get(\"start\", 0), d.get(\"end\", 0))\n",
        "            if en > st:\n",
        "                clean_spans.append({\"start\": st, \"end\": en})\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    return corrected, clean_tags, clean_spans, model\n",
        "\n",
        "def run_correct_only(df_in, text_col=\"Raw text\", id_col=\"ID\", client=None, model=MODEL_ID, use_mock=USE_MOCK, out_col=\"Corrected text (8)\"):\n",
        "    if text_col not in df_in.columns:\n",
        "        raise KeyError(f\"Missing required column: {text_col}\")\n",
        "    df = df_in.copy()\n",
        "    if id_col not in df.columns:\n",
        "        df[id_col] = pd.RangeIndex(len(df)).astype(str)\n",
        "    else:\n",
        "        df[id_col] = df[id_col].astype(str).str.replace(r\"\\.0$\",\"\",regex=True)\n",
        "\n",
        "    cache = {}\n",
        "    corrected, tags_json, dlg_json, sources = [], [], [], []\n",
        "    for raw in tqdm(df[text_col].astype(str).tolist(), desc=\"Step 8: correcting\"):\n",
        "        if raw in cache:\n",
        "            c, tags, spans, src = cache[raw]\n",
        "        else:\n",
        "            c, tags, spans, src = correct_with_tags(raw, client=client, model=model, use_mock=use_mock)\n",
        "            cache[raw] = (c, tags, spans, src)\n",
        "        corrected.append(c)\n",
        "        tags_json.append(json.dumps(tags, ensure_ascii=False))\n",
        "        dlg_json.append(json.dumps(spans, ensure_ascii=False))\n",
        "        sources.append(src)\n",
        "\n",
        "    df[out_col] = corrected\n",
        "    df[\"NarrativeTagsJSON\"] = tags_json\n",
        "    df[\"DialogueSpansJSON\"] = dlg_json\n",
        "    df[\"CorrectedBy\"] = sources\n",
        "    return df\n",
        "\n"
      ],
      "metadata": {
        "id": "diFgp5m8ir-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------- 6) Token map (runner) ----------\n",
        "from difflib import SequenceMatcher\n",
        "_WORD_RX = re.compile(r\"\\w\", flags=re.UNICODE)\n",
        "\n",
        "def _split_merged_word(tok: str):\n",
        "    if not tok or not tok.isalpha():\n",
        "        return [tok]\n",
        "    m = re.match(r\"^([A-Z]{2,})([a-z].*)$\", tok)\n",
        "    return [m.group(1), m.group(2)] if m else [tok]\n",
        "\n",
        "def _tokenize_with_split(s: str):\n",
        "    base = re.findall(r\"\\w+|[^\\w\\s]\", s or \"\", flags=re.UNICODE)\n",
        "    out = []\n",
        "    for t in base:\n",
        "        if re.fullmatch(r\"\\w+\", t):\n",
        "            out.extend(_split_merged_word(t))\n",
        "        else:\n",
        "            out.append(t)\n",
        "    return out\n",
        "\n",
        "def _rebuild_offsets_with_splitting(text, tokens):\n",
        "    spans, i, n = [], 0, len(text or \"\")\n",
        "    text = text or \"\"\n",
        "    for tok in tokens:\n",
        "        if not tok:\n",
        "            spans.append((i, i))\n",
        "            continue\n",
        "        pos = text.find(tok, i)\n",
        "        if pos >= 0:\n",
        "            start, end = pos, pos + len(tok)\n",
        "        else:\n",
        "            j = i\n",
        "            while j < n and text[j].isspace():\n",
        "                j += 1\n",
        "            start = j\n",
        "            end = min(n, start + len(tok))\n",
        "        spans.append((start, end))\n",
        "        i = end\n",
        "    return spans\n",
        "\n",
        "def _is_word(tok: str) -> bool:\n",
        "    return bool(tok) and bool(_WORD_RX.search(tok))\n",
        "\n",
        "def _canon(tok: str) -> str:\n",
        "    if tok is None:\n",
        "        return \"\"\n",
        "    u = str(tok).upper()\n",
        "    return re.sub(r\"(.)\\1+\", r\"\\1\", u)\n",
        "\n",
        "def build_word_map(raw_text, corr_text):\n",
        "    raw_text  = str(raw_text or \"\")\n",
        "    corr_text = str(corr_text or \"\")\n",
        "    raw_tokens  = _tokenize_with_split(raw_text)\n",
        "    corr_tokens = _tokenize_with_split(corr_text)\n",
        "    raw_spans  = _rebuild_offsets_with_splitting(raw_text,  raw_tokens)\n",
        "    corr_spans = _rebuild_offsets_with_splitting(corr_text, corr_tokens)\n",
        "    sm = SequenceMatcher(\n",
        "        a=[_canon(t) for t in raw_tokens],\n",
        "        b=[_canon(t) for t in corr_tokens],\n",
        "        autojunk=False\n",
        "    )\n",
        "    rows = []\n",
        "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
        "        if tag == \"equal\":\n",
        "            for k in range(i2 - i1):\n",
        "                ri, ci = i1 + k, j1 + k\n",
        "                r_tok, c_tok = raw_tokens[ri], corr_tokens[ci]\n",
        "                r_start, r_end = raw_spans[ri]; c_start, c_end = corr_spans[ci]\n",
        "                rows.append({\n",
        "                    \"raw_index\": ri, \"raw_token\": r_tok, \"raw_start\": r_start, \"raw_end\": r_end,\n",
        "                    \"corr_index\": ci, \"corr_token\": c_tok, \"corr_start\": c_start, \"corr_end\": c_end,\n",
        "                    \"op\": \"equal\", \"equal_ci\": (r_tok == c_tok), \"error_type\": \"Equal\"\n",
        "                })\n",
        "        elif tag == \"replace\":\n",
        "            ii, jj = i1, j1\n",
        "            while ii < i2 or jj < j2:\n",
        "                if ii < i2 and jj < j2:\n",
        "                    r_tok = raw_tokens[ii]; c_tok = corr_tokens[jj]\n",
        "                    r_start, r_end = raw_spans[ii]; c_start, c_end = corr_spans[jj]\n",
        "                    if _is_word(r_tok) and not _is_word(c_tok) and (jj + 1) < j2 and _is_word(corr_tokens[jj + 1]):\n",
        "                        rows.append({\n",
        "                            \"raw_index\": None, \"raw_token\": None, \"raw_start\": None, \"raw_end\": None,\n",
        "                            \"corr_index\": jj, \"corr_token\": c_tok, \"corr_start\": c_start, \"corr_end\": c_end,\n",
        "                            \"op\": \"insert\", \"equal_ci\": False, \"error_type\": \"PunctuationInsertion\"\n",
        "                        })\n",
        "                        jj += 1\n",
        "                        c2_tok = corr_tokens[jj]; c2_start, c2_end = corr_spans[jj]\n",
        "                        err = \"Spelling\" if (str(r_tok).isalpha() and str(c2_tok).isalpha() and _canon(r_tok) == _canon(c2_tok)) else \"Replacement\"\n",
        "                        rows.append({\n",
        "                            \"raw_index\": ii, \"raw_token\": r_tok, \"raw_start\": r_start, \"raw_end\": r_end,\n",
        "                            \"corr_index\": jj, \"corr_token\": c2_tok, \"corr_start\": c2_start, \"corr_end\": c2_end,\n",
        "                            \"op\": \"replace\", \"equal_ci\": (_canon(r_tok) == _canon(c2_tok)), \"error_type\": err\n",
        "                        })\n",
        "                        ii += 1; jj += 1\n",
        "                        continue\n",
        "                    err = \"Spelling\" if (str(r_tok).isalpha() and str(c_tok).isalpha() and _canon(r_tok) == _canon(c_tok)) else \"Replacement\"\n",
        "                    rows.append({\n",
        "                        \"raw_index\": ii, \"raw_token\": r_tok, \"raw_start\": r_start, \"raw_end\": r_end,\n",
        "                        \"corr_index\": jj, \"corr_token\": c_tok, \"corr_start\": c_start, \"corr_end\": c_end,\n",
        "                        \"op\": \"replace\", \"equal_ci\": (_canon(r_tok) == _canon(c_tok)), \"error_type\": err\n",
        "                    })\n",
        "                    ii += 1; jj += 1\n",
        "                elif ii < i2:\n",
        "                    r_tok = raw_tokens[ii]; r_start, r_end = raw_spans[ii]\n",
        "                    rows.append({\n",
        "                        \"raw_index\": ii, \"raw_token\": r_tok, \"raw_start\": r_start, \"raw_end\": r_end,\n",
        "                        \"corr_index\": None, \"corr_token\": None, \"corr_start\": None, \"corr_end\": None,\n",
        "                        \"op\": \"delete\", \"equal_ci\": False,\n",
        "                        \"error_type\": \"PunctuationDeletion\" if not _is_word(r_tok) else \"Deletion\"\n",
        "                    })\n",
        "                    ii += 1\n",
        "                else:\n",
        "                    c_tok = corr_tokens[jj]; c_start, c_end = corr_spans[jj]\n",
        "                    rows.append({\n",
        "                        \"raw_index\": None, \"raw_token\": None, \"raw_start\": None, \"raw_end\": None,\n",
        "                        \"corr_index\": jj, \"corr_token\": c_tok, \"corr_start\": c_start, \"corr_end\": c_end,\n",
        "                        \"op\": \"insert\", \"equal_ci\": False,\n",
        "                        \"error_type\": \"PunctuationInsertion\" if not _is_word(c_tok) else \"Insertion\"\n",
        "                    })\n",
        "                    jj += 1\n",
        "        elif tag == \"delete\":\n",
        "            for ri in range(i1, i2):\n",
        "                r_tok = raw_tokens[ri]; r_start, r_end = raw_spans[ri]\n",
        "                rows.append({\n",
        "                    \"raw_index\": ri, \"raw_token\": r_tok, \"raw_start\": r_start, \"raw_end\": r_end,\n",
        "                    \"corr_index\": None, \"corr_token\": None, \"corr_start\": None, \"corr_end\": None,\n",
        "                    \"op\": \"delete\", \"equal_ci\": False,\n",
        "                    \"error_type\": \"PunctuationDeletion\" if not _is_word(r_tok) else \"Deletion\"\n",
        "                })\n",
        "        elif tag == \"insert\":\n",
        "            for ci in range(j1, j2):\n",
        "                c_tok = corr_tokens[ci]; c_start, c_end = corr_spans[ci]\n",
        "                rows.append({\n",
        "                    \"raw_index\": None, \"raw_token\": None, \"raw_start\": None, \"raw_end\": None,\n",
        "                    \"corr_index\": ci, \"corr_token\": c_tok, \"corr_start\": c_start, \"corr_end\": c_end,\n",
        "                    \"op\": \"insert\", \"equal_ci\": False,\n",
        "                    \"error_type\": \"PunctuationInsertion\" if not _is_word(c_tok) else \"Insertion\"\n",
        "                })\n",
        "    return rows\n",
        "\n",
        "def run_mapping_only(df_corr: pd.DataFrame, *, id_col=\"ID\", raw_col=\"Raw text\", corr_col=\"Corrected text (8)\"):\n",
        "    if not isinstance(df_corr, pd.DataFrame):\n",
        "        raise TypeError(\"run_mapping_only: df_corr\")\n",
        "    need = {id_col, raw_col, corr_col}\n",
        "    missing = need - set(df_corr.columns)\n",
        "    if missing:\n",
        "        raise KeyError(f\"run_mapping_only missing columns: {missing}\")\n",
        "\n",
        "    rows = []\n",
        "    texts_rows = []\n",
        "    for _, r in df_corr.iterrows():\n",
        "        ID = str(r[id_col])\n",
        "        raw = str(r[raw_col]) if not pd.isna(r[raw_col]) else \"\"\n",
        "        corr = str(r[corr_col]) if not pd.isna(r[corr_col]) else \"\"\n",
        "        mapped = build_word_map(raw, corr)\n",
        "        for rec in mapped:\n",
        "            rows.append({\n",
        "                \"ID\": ID,\n",
        "                \"raw_index\": rec.get(\"raw_index\"),\n",
        "                \"raw_token\": rec.get(\"raw_token\"),\n",
        "                \"raw_start\": rec.get(\"raw_start\"),\n",
        "                \"raw_end\":   rec.get(\"raw_end\"),\n",
        "                \"corr_index\": rec.get(\"corr_index\"),\n",
        "                \"corr_token\": rec.get(\"corr_token\"),\n",
        "                \"corr_start\": rec.get(\"corr_start\"),\n",
        "                \"corr_end\":   rec.get(\"corr_end\"),\n",
        "                \"op\": rec.get(\"op\"),\n",
        "                \"equal_ci\": rec.get(\"equal_ci\"),\n",
        "                \"error_type\": rec.get(\"error_type\"),\n",
        "            })\n",
        "        texts_rows.append({\n",
        "            \"ID\": ID,\n",
        "            raw_col: raw,\n",
        "            corr_col: corr,\n",
        "            \"NarrativeTagsJSON\": r.get(\"NarrativeTagsJSON\", \"[]\"),\n",
        "            \"DialogueSpansJSON\": r.get(\"DialogueSpansJSON\", \"[]\"),\n",
        "        })\n",
        "\n",
        "    df_map = pd.DataFrame(rows)\n",
        "    for c in [\"corr_index\",\"corr_start\",\"corr_end\",\"raw_index\",\"raw_start\",\"raw_end\"]:\n",
        "        if c in df_map.columns:\n",
        "            df_map[c] = pd.to_numeric(df_map[c], errors=\"coerce\")\n",
        "    for c, default in [\n",
        "        (\"TITLE\", False),\n",
        "        (\"DIALOGUE\", False),\n",
        "        (\"DialogueSpanID\", pd.NA),\n",
        "        (\"Sentence Boundaries\",\"\"),\n",
        "        (\"BoundaryCheck\",\"\")\n",
        "    ]:\n",
        "        if c not in df_map.columns:\n",
        "            df_map[c] = default\n",
        "\n",
        "    df_texts = pd.DataFrame(texts_rows)\n",
        "    if \"NarrativeTagsJSON\" not in df_texts.columns:\n",
        "        df_texts[\"NarrativeTagsJSON\"] = \"[]\"\n",
        "    if \"DialogueSpansJSON\" not in df_texts.columns:\n",
        "        df_texts[\"DialogueSpansJSON\"] = \"[]\"\n",
        "    return df_map, df_texts\n",
        "\n",
        "\n",
        "# ---------- 8) Sentence IDs and flags ----------\n",
        "ABBREV = { \"mr.\",\"mrs.\",\"ms.\",\"dr.\",\"prof.\",\"sr.\",\"jr.\",\"st.\",\"vs.\",\"etc.\",\"e.g.\",\"i.e.\",\"cf.\",\"fig.\",\"ex.\",\"no.\",\n",
        "    \"approx.\",\"circa.\",\"ca.\",\"dept.\",\"est.\",\"misc.\",\"rev.\",\"jan.\",\"feb.\",\"mar.\",\"apr.\",\"jun.\",\"jul.\",\n",
        "    \"aug.\",\"sep.\",\"sept.\",\"oct.\",\"nov.\",\"dec.\" }\n",
        "TERMINALS = {\".\",\"!\",\"?\",\"…\",\"...\",\"?!\",\"!?\"}\n",
        "CLOSERS   = {\")\",\"]\",\"}\",\"”\",\"’\",\"»\"}\n",
        "OPENERS   = {\"(\",\"[\",\"{\",\"“\",\"‘\",\"«\"}\n",
        "\n",
        "def _require_df(df, name):\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(f\"{name} is None or not a pandas.DataFrame\")\n",
        "    return df\n",
        "\n",
        "def assign_corr_sentence_ids(df_map: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = _require_df(df_map, \"assign_corr_sentence_ids: df_map\").copy()\n",
        "    if \"RowID\" in df.columns:\n",
        "        df[\"ID\"] = df[\"RowID\"].astype(str)\n",
        "    elif \"ID\" in df.columns:\n",
        "        df[\"ID\"] = df[\"ID\"].astype(str)\n",
        "    else:\n",
        "        df[\"ID\"] = df.index.astype(str)\n",
        "    if \"corr_index\" not in df.columns:\n",
        "        df[\"corr_index\"] = np.nan\n",
        "    df[\"_rowpos\"] = np.arange(len(df))\n",
        "    df[\"_sort\"] = pd.to_numeric(df[\"corr_index\"], errors=\"coerce\").fillna(1e12) + df[\"_rowpos\"]*1e-9\n",
        "\n",
        "    RE_INITIAL = re.compile(r\"^[A-Z]\\.$\")\n",
        "    RE_INITIAL_PAIR = re.compile(r\"^[A-Z]\\.[A-Z]\\.$\")\n",
        "    RE_NUM_DOT = re.compile(r\"^\\d+\\.$\")\n",
        "    RE_SECTION = re.compile(r\"^\\d+(?:\\.\\d+){1,3}$\")\n",
        "    RE_DOT_TAIL = re.compile(r\"^\\.\\d+$\")\n",
        "\n",
        "    def _is_terminal(tok, prev_tok, next_tok):\n",
        "        t = tok.strip()\n",
        "        if not t:\n",
        "            return False\n",
        "        if t in {\"…\",\"...\",\"?!\",\"!?\"}:\n",
        "            return True\n",
        "        if t in {\"!\",\"?\"}:\n",
        "            return True\n",
        "        if t == \".\":\n",
        "            p = prev_tok.strip().lower()\n",
        "            n = next_tok.strip()\n",
        "            if p in ABBREV:\n",
        "                return False\n",
        "            if RE_INITIAL.fullmatch(prev_tok) or RE_INITIAL_PAIR.fullmatch(prev_tok):\n",
        "                return False\n",
        "            if RE_SECTION.fullmatch(prev_tok):\n",
        "                return False\n",
        "            if RE_NUM_DOT.fullmatch(prev_tok) and (n and re.match(r\"[A-Za-z(“\\\"'\\[]\", n)):\n",
        "                return False\n",
        "            if RE_DOT_TAIL.fullmatch(n):\n",
        "                return False\n",
        "            if n.isdigit():\n",
        "                return False\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    df[\"CorrSentenceID\"] = pd.NA\n",
        "    for ID, g in df.sort_values([\"ID\",\"_sort\"], kind=\"mergesort\").groupby(\"ID\", sort=False):\n",
        "        toks = (g[\"corr_token\"] if \"corr_token\" in g.columns else g[\"raw_token\"]).astype(str).tolist()\n",
        "        sids, sid, pending = [], 0, False\n",
        "        i = 0\n",
        "        while i < len(toks):\n",
        "            tok = toks[i].strip()\n",
        "            prev_tok = toks[i-1].strip() if i > 0 else \"\"\n",
        "            next_tok = toks[i+1].strip() if i+1 < len(toks) else \"\"\n",
        "            if i+2 < len(toks) and toks[i]==\".\" and toks[i+1]==\".\" and toks[i+2]==\".\":\n",
        "                pending = True\n",
        "                sids.append(sid)\n",
        "                i += 1\n",
        "                continue\n",
        "            if pending:\n",
        "                if tok in CLOSERS or (tok=='\"' and not (prev_tok == \"\" or prev_tok in TERMINALS or prev_tok in OPENERS)):\n",
        "                    sids.append(sid)\n",
        "                    i += 1\n",
        "                    continue\n",
        "                if tok in OPENERS or (tok=='\"' and (prev_tok == \"\" or prev_tok in TERMINALS or prev_tok in OPENERS)):\n",
        "                    sid += 1\n",
        "                    pending = False\n",
        "                    sids.append(sid)\n",
        "                    i += 1\n",
        "                    continue\n",
        "                sid += 1\n",
        "                pending = False\n",
        "                sids.append(sid)\n",
        "                i += 1\n",
        "                continue\n",
        "            else:\n",
        "                sids.append(sid)\n",
        "                i += 1\n",
        "            if _is_terminal(tok, prev_tok, next_tok):\n",
        "                pending = True\n",
        "        df.loc[g.index, \"CorrSentenceID\"] = pd.Series(sids, index=g.index).astype(\"Int64\")\n",
        "\n",
        "    df.drop(columns=[\"_rowpos\",\"_sort\"], inplace=True, errors=\"ignore\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def ensure_dialogue_and_tags(df_texts):\n",
        "    df = df_texts.copy()\n",
        "    if \"DialogueSpansJSON\" not in df.columns:\n",
        "        df[\"DialogueSpansJSON\"] = \"[]\"\n",
        "    if \"NarrativeTagsJSON\" not in df.columns:\n",
        "        df[\"NarrativeTagsJSON\"] = \"[]\"\n",
        "    return ensure_dialogue_json(df, corrected_col=\"Corrected text (8)\")\n",
        "\n",
        "def mark_title_and_dialogue(df_map: pd.DataFrame, df_texts: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df_map.copy()\n",
        "    for c, default in [\n",
        "        (\"TITLE\", False),\n",
        "        (\"DIALOGUE\", False),\n",
        "        (\"DialogueSpanID\", pd.NA),\n",
        "        (\"Sentence Boundaries\", \"\"),\n",
        "        (\"BoundaryCheck\", \"\"),\n",
        "    ]:\n",
        "        if c not in df.columns:\n",
        "            df[c] = default\n",
        "\n",
        "    df_texts = ensure_dialogue_json(df_texts, corrected_col=\"Corrected text (8)\")\n",
        "    tags_by_id = dict(zip(df_texts[\"ID\"].astype(str), df_texts[\"NarrativeTagsJSON\"]))\n",
        "    dlg_by_id  = dict(zip(df_texts[\"ID\"].astype(str), df_texts[\"DialogueSpansJSON\"]))\n",
        "\n",
        "    def _loads(x):\n",
        "        try:\n",
        "            return json.loads(x) if isinstance(x, str) else (x or [])\n",
        "        except Exception:\n",
        "            return []\n",
        "\n",
        "    if \"ID\" not in df.columns and \"RowID\" in df.columns:\n",
        "        df[\"ID\"] = df[\"RowID\"].astype(str)\n",
        "\n",
        "    def per_id(block: pd.DataFrame) -> pd.DataFrame:\n",
        "        g  = block.sort_values([\"CorrSentenceID\", \"corr_index\"], kind=\"mergesort\")\n",
        "        ID = str(g[\"ID\"].iat[0])\n",
        "        starts = pd.to_numeric(g[\"corr_start\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "        ends   = pd.to_numeric(g[\"corr_end\"],   errors=\"coerce\").fillna(0).astype(int)\n",
        "        for t in _loads(tags_by_id.get(ID, \"[]\")):\n",
        "            if isinstance(t, dict) and t.get(\"type\") == \"title\":\n",
        "                st, en = int(t.get(\"start\", 0) or 0), int(t.get(\"end\", 0) or 0)\n",
        "                g.loc[(starts < en) & (ends > st), \"TITLE\"] = True\n",
        "        g.loc[g[\"TITLE\"], \"Sentence Boundaries\"] = g.loc[g[\"TITLE\"], \"Sentence Boundaries\"].replace({\"\": \"Title\"})\n",
        "        for span_idx, sp in enumerate(_loads(dlg_by_id.get(ID, \"[]\")), start=1):\n",
        "            st, en = int(sp.get(\"start\", 0) or 0), int(sp.get(\"end\", 0) or 0)\n",
        "            mask = (starts < en) & (ends > st)\n",
        "            g.loc[mask, \"DIALOGUE\"] = True\n",
        "            g.loc[mask, \"DialogueSpanID\"] = span_idx\n",
        "        return g\n",
        "\n",
        "    df = (\n",
        "        df.groupby(\"ID\", group_keys=False)\n",
        "          .apply(per_id)\n",
        "          .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    df[\"SentenceRef\"] = (\n",
        "        df[\"ID\"].astype(str) + \"_s\" +\n",
        "        df[\"CorrSentenceID\"].apply(lambda x: f\"{int(x):03d}\" if pd.notna(x) else \"000\")\n",
        "    )\n",
        "    return df\n",
        "\n",
        "def add_dialogue_boundary_flags(df_map_in: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df_map_in.copy()\n",
        "    for c in [\"Dialogue Boundaries\", \"DialogueBoundaryCheck\"]:\n",
        "        if c not in df.columns:\n",
        "            df[c] = \"\"\n",
        "    if \"DialogueSpanID\" not in df.columns or \"DIALOGUE\" not in df.columns:\n",
        "        return df\n",
        "\n",
        "    def _is_open_quote(tok):\n",
        "        t = str(tok or \"\").strip()\n",
        "        return t in QUOTE_OPENERS or t == '\"'\n",
        "    def _is_close_quote(tok):\n",
        "        t = str(tok or \"\").strip()\n",
        "        return t in QUOTE_CLOSERS or t == '\"'\n",
        "\n",
        "    sort_cols = [\"ID\",\"CorrSentenceID\"]\n",
        "    if \"corr_index\" in df.columns:\n",
        "        sort_cols.append(\"corr_index\")\n",
        "    df = df.sort_values(sort_cols, kind=\"mergesort\")\n",
        "\n",
        "    updates_boundaries = {}\n",
        "    updates_checks = {}\n",
        "\n",
        "    q = df[df[\"DIALOGUE\"].astype(bool) & df[\"DialogueSpanID\"].notna()]\n",
        "    for (ID, SID, DID), g in q.groupby([\"ID\",\"CorrSentenceID\",\"DialogueSpanID\"], sort=False):\n",
        "        begin_idx = None\n",
        "        for idx, tok in zip(g.index, g[\"corr_token\"].astype(str)):\n",
        "            if not _is_open_quote(tok):\n",
        "                begin_idx = idx\n",
        "                break\n",
        "        end_idx = None\n",
        "        for idx, tok in zip(reversed(g.index.tolist()), reversed(g[\"corr_token\"].astype(str).tolist())):\n",
        "            if not _is_close_quote(tok):\n",
        "                end_idx = idx\n",
        "                break\n",
        "        if begin_idx is not None:\n",
        "            updates_boundaries.setdefault(begin_idx, []).append(\"Dialogue Beginning\")\n",
        "            updates_checks.setdefault(begin_idx, []).append(\"Correct Dialogue Beginning\")\n",
        "        if end_idx is not None:\n",
        "            updates_boundaries.setdefault(end_idx, []).append(\"Dialogue Ending\")\n",
        "            updates_checks.setdefault(end_idx, []).append(\"Correct Dialogue Ending\")\n",
        "\n",
        "    if updates_boundaries:\n",
        "        col = df[\"Dialogue Boundaries\"].astype(str).fillna(\"\")\n",
        "        for idx, labs in updates_boundaries.items():\n",
        "            joined = \" | \".join(sorted(set(labs)))\n",
        "            col.loc[idx] = (col.loc[idx] + \" | \" if col.loc[idx] else \"\") + joined\n",
        "        df[\"Dialogue Boundaries\"] = col\n",
        "\n",
        "    if updates_checks:\n",
        "        col = df[\"DialogueBoundaryCheck\"].astype(str).fillna(\"\")\n",
        "        for idx, labs in updates_checks.items():\n",
        "            joined = \" | \".join(sorted(set(labs)))\n",
        "            col.loc[idx] = (col.loc[idx] + \" | \" if col.loc[idx] else \"\") + joined\n",
        "        df[\"DialogueBoundaryCheck\"] = col\n",
        "\n",
        "    return df.reset_index(drop=True)\n",
        "\n",
        "# sentence boundary flags\n",
        "TERMINALS_HARD = {\".\",\"!\",\"?\",\"…\",\"...\",\"?!\",\"!?\"}\n",
        "OPENING_PUNCT  = {'\"', \"“\", \"‘\", \"«\", \"(\", \"[\", \"{\"}\n",
        "CLOSING_PUNCT  = {'\"', \"”\", \"’\", \"»\", \")\", \"]\", \"}\"}\n",
        "\n",
        "def _first_cap_or_digit(s: str) -> bool:\n",
        "    if not isinstance(s, str):\n",
        "        return False\n",
        "    m = re.search(r\"[A-Za-z0-9]\", s)\n",
        "    if not m:\n",
        "        return False\n",
        "    ch = s[m.start()]\n",
        "    return ch.isdigit() or (ch.isalpha() and ch.isupper())\n",
        "\n",
        "def _first_begin_row(g: pd.DataFrame):\n",
        "    g = g.copy()\n",
        "    toks = g[\"corr_token\"].astype(str).tolist()\n",
        "    titles = g[\"TITLE\"].astype(bool).tolist()\n",
        "    for idx, tok, is_title in zip(g.index, toks, titles):\n",
        "        if is_title:\n",
        "            continue\n",
        "        t = tok.strip()\n",
        "        if t in OPENING_PUNCT:\n",
        "            continue\n",
        "        if _first_cap_or_digit(t):\n",
        "            return idx\n",
        "    for idx, tok, is_title in zip(g.index, toks, titles):\n",
        "        if is_title:\n",
        "            continue\n",
        "        if re.search(r\"\\w\", tok or \"\"):\n",
        "            return idx\n",
        "    return None\n",
        "\n",
        "def _last_terminal_row(g: pd.DataFrame):\n",
        "    toks = g[\"corr_token\"].astype(str).tolist()\n",
        "    idxs = list(g.index)\n",
        "    last_term_loc = None\n",
        "    for j in range(len(toks) - 1, -1, -1):\n",
        "        t = toks[j].strip()\n",
        "        if t in TERMINALS_HARD:\n",
        "            last_term_loc = j\n",
        "            break\n",
        "    if last_term_loc is None:\n",
        "        return None\n",
        "    return idxs[last_term_loc]\n",
        "\n",
        "def add_sentence_boundary_flags(df_map_in: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = _require_df(df_map_in, \"add_sentence_boundary_flags: df_map_in\").copy()\n",
        "    for c, default in [(\"Sentence Boundaries\",\"\"), (\"BoundaryCheck\",\"\")]:\n",
        "        if c not in df.columns:\n",
        "            df[c] = default\n",
        "    if \"CorrSentenceID\" not in df.columns:\n",
        "        raise KeyError(\"add_sentence_boundary_flags requires CorrSentenceID. Run assign_corr_sentence_ids first.\")\n",
        "    sort_cols = [\"ID\",\"CorrSentenceID\"]\n",
        "    if \"corr_index\" in df.columns:\n",
        "        sort_cols.append(\"corr_index\")\n",
        "    df = df.sort_values(sort_cols, kind=\"mergesort\")\n",
        "    updates_boundaries = {}\n",
        "    updates_checks = {}\n",
        "    for (ID, SID), g in df.groupby([\"ID\",\"CorrSentenceID\"], sort=False):\n",
        "        if g.empty:\n",
        "            continue\n",
        "        begin_idx = _first_begin_row(g)\n",
        "        end_idx   = _last_terminal_row(g)\n",
        "        if begin_idx is not None:\n",
        "            updates_boundaries.setdefault(begin_idx, []).append(\"Sentence Beginning\")\n",
        "            tok = str(df.loc[begin_idx, \"corr_token\"])\n",
        "            if _first_cap_or_digit(tok):\n",
        "                updates_checks.setdefault(begin_idx, []).append(\"Correct Beginning\")\n",
        "            else:\n",
        "                updates_checks.setdefault(begin_idx, []).append(\"Incorrect Beginning\")\n",
        "        if end_idx is not None:\n",
        "            updates_boundaries.setdefault(end_idx, []).append(\"Sentence Ending\")\n",
        "            later_term = False\n",
        "            toks = g[\"corr_token\"].astype(str).tolist()\n",
        "            idxs = list(g.index)\n",
        "            chosen_pos = idxs.index(end_idx)\n",
        "            for j in range(chosen_pos + 1, len(idxs)):\n",
        "                if toks[j].strip() in TERMINALS_HARD:\n",
        "                    later_term = True\n",
        "                    break\n",
        "            if later_term:\n",
        "                updates_checks.setdefault(end_idx, []).append(\"Incorrect Ending\")\n",
        "            else:\n",
        "                updates_checks.setdefault(end_idx, []).append(\"Correct Ending\")\n",
        "        else:\n",
        "            last_idx = g.index[-1]\n",
        "            updates_boundaries.setdefault(last_idx, []).append(\"Sentence Ending\")\n",
        "            updates_checks.setdefault(last_idx, []).append(\"Incorrect Ending\")\n",
        "\n",
        "    if updates_boundaries:\n",
        "        sb = df[\"Sentence Boundaries\"].astype(str).fillna(\"\")\n",
        "        for idx, lab_list in updates_boundaries.items():\n",
        "            joined = \" | \".join(sorted(set(lab_list)))\n",
        "            sb.loc[idx] = (sb.loc[idx] + \" | \" if sb.loc[idx] else \"\") + joined\n",
        "        df[\"Sentence Boundaries\"] = sb\n",
        "\n",
        "    if updates_checks:\n",
        "        bc = df[\"BoundaryCheck\"].astype(str).fillna(\"\")\n",
        "        for idx, lab_list in updates_checks.items():\n",
        "            joined = \" | \".join(sorted(set(lab_list)))\n",
        "            bc.loc[idx] = (bc.loc[idx] + \" | \" if bc.loc[idx] else \"\") + joined\n",
        "        df[\"BoundaryCheck\"] = bc\n",
        "\n",
        "    return df.reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "ufRU0akhkCjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ---------- 11) Step 9 summariser ----------\n",
        "NO_SPACE_BEFORE = set(list(\".,;:!?)]}\\\"'»”’…\"))\n",
        "NO_SPACE_AFTER  = set(list(\"([{{\\\"'«“‘\"))\n",
        "\n",
        "def _detok(tokens):\n",
        "    out = []\n",
        "    for t in tokens:\n",
        "        if t is None or (isinstance(t, float) and math.isnan(t)):\n",
        "            continue\n",
        "        t = str(t)\n",
        "        if not out:\n",
        "            out.append(t)\n",
        "            continue\n",
        "        prev = out[-1]\n",
        "        if t in NO_SPACE_BEFORE or re.fullmatch(r\"[.]{3}\", t):\n",
        "            out[-1] = prev + t\n",
        "        elif prev in NO_SPACE_AFTER:\n",
        "            out[-1] = prev + t\n",
        "        else:\n",
        "            out.append(\" \" + t)\n",
        "    s = \"\".join(out)\n",
        "    s = re.sub(r\"\\.\\s*\\.\\s*\\.\", \"...\", s)\n",
        "    return s.strip()\n",
        "\n",
        "def _summarize_sentence(g: pd.DataFrame, tags_row: pd.Series) -> pd.Series:\n",
        "    corr_tokens = g[\"corr_token\"].tolist()\n",
        "    raw_tokens  = [x for x in g.get(\"raw_token\", pd.Series([], dtype=object)).tolist() if not pd.isna(x)]\n",
        "    corr_text   = _detok(corr_tokens)\n",
        "    raw_text    = _detok(raw_tokens) if raw_tokens else \"\"\n",
        "    b_rows = g[g[\"Sentence Boundaries\"].str.contains(\"Sentence Beginning\", na=False)]\n",
        "    e_rows = g[g[\"Sentence Boundaries\"].str.contains(\"Sentence Ending\",   na=False)]\n",
        "    def _resolve(check_series: pd.Series, good: str, bad: str):\n",
        "        if check_series.empty:\n",
        "            return np.nan\n",
        "        joined = \" | \".join(check_series.dropna().astype(str))\n",
        "        if good in joined:\n",
        "            return 1\n",
        "        if bad in joined:\n",
        "            return 0\n",
        "        return np.nan\n",
        "    begin_ok = _resolve(b_rows.get(\"BoundaryCheck\", pd.Series([], dtype=object)), \"Correct Beginning\", \"Incorrect Beginning\")\n",
        "    end_ok   = _resolve(e_rows.get(\"BoundaryCheck\", pd.Series([], dtype=object)), \"Correct Ending\", \"Incorrect Ending\")\n",
        "    ops = g.get(\"op\", pd.Series([], dtype=object))\n",
        "    rec = {\n",
        "        \"SentenceRef\": g[\"SentenceRef\"].iloc[0],\n",
        "        \"CorrectedSentence\": corr_text,\n",
        "        \"RawSentence\": raw_text,\n",
        "        \"TokensInSentence\": int(len(g)),\n",
        "        \"EditsInSentence\": int((ops != \"equal\").sum()) if not ops.empty else np.nan,\n",
        "        \"EqualsInSentence\": int((ops == \"equal\").sum()) if not ops.empty else np.nan,\n",
        "        \"Insertions\": int((ops == \"insert\").sum()) if not ops.empty else np.nan,\n",
        "        \"Deletions\": int((ops == \"delete\").sum()) if not ops.empty else np.nan,\n",
        "        \"Replacements\": int((ops == \"replace\").sum()) if not ops.empty else np.nan,\n",
        "        \"CorrectBeginning\": begin_ok,\n",
        "        \"CorrectEnding\": end_ok,\n",
        "        \"NarrativeTagsJSON\": tags_row.get(\"NarrativeTagsJSON\",\"[]\"),\n",
        "        \"DialogueSpansJSON\": tags_row.get(\"DialogueSpansJSON\",\"[]\"),\n",
        "    }\n",
        "    return pd.Series(rec)\n"
      ],
      "metadata": {
        "id": "d4Ucc6x2inU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ---------- 12) Step 8 runner ----------\n",
        "def run_step8(df_preprocessed, raw_col=\"Raw text\", id_col=\"ID\", client=None, model=MODEL_ID, use_mock=USE_MOCK):\n",
        "    df_corr = run_correct_only(\n",
        "        df_preprocessed, text_col=raw_col, id_col=id_col,\n",
        "        client=None if use_mock else client, model=model, use_mock=use_mock,\n",
        "        out_col=\"Corrected text (8)\"\n",
        "    )\n",
        "    df_map, df_texts = run_mapping_only(df_corr, id_col=id_col, raw_col=raw_col, corr_col=\"Corrected text (8)\")\n",
        "    df_texts = ensure_dialogue_json(df_texts, corrected_col=\"Corrected text (8)\")\n",
        "    df_map = assign_corr_sentence_ids(df_map)\n",
        "    df_map = mark_title_and_dialogue(df_map, df_texts)\n",
        "    df_map = add_sentence_boundary_flags(df_map)\n",
        "    df_map = add_dialogue_boundary_flags(df_map)\n",
        "    return df_texts, df_map\n",
        "\n",
        "\n",
        "# ---------- 13) Step 9 runner ----------\n",
        "def run_step9(df_map, df_texts_with_tags):\n",
        "    need = {\"ID\",\"CorrSentenceID\",\"corr_token\",\"Sentence Boundaries\",\"BoundaryCheck\",\"SentenceRef\"}\n",
        "    missing = need - set(df_map.columns)\n",
        "    if missing:\n",
        "        raise KeyError(f\"df_map missing: {missing}\")\n",
        "    sort_cols = [\"ID\",\"CorrSentenceID\"]\n",
        "    if \"corr_index\" in df_map.columns:\n",
        "        sort_cols.append(\"corr_index\")\n",
        "    wm = df_map.sort_values(sort_cols, kind=\"mergesort\").copy()\n",
        "    wm_nontitle = wm[~wm[\"TITLE\"].astype(bool)].copy()\n",
        "    tags_by_id = df_texts_with_tags.set_index(\"ID\")[[\"NarrativeTagsJSON\",\"DialogueSpansJSON\"]]\n",
        "    out = []\n",
        "    for (ID, SID), g in wm_nontitle.groupby([\"ID\",\"CorrSentenceID\"], sort=False):\n",
        "        tags_row = tags_by_id.loc[ID] if ID in tags_by_id.index else pd.Series({}, dtype=object)\n",
        "        out.append(_summarize_sentence(g, tags_row))\n",
        "    sent_df = pd.DataFrame(out).sort_values([\"SentenceRef\"], kind=\"mergesort\").reset_index(drop=True)\n",
        "    return sent_df\n"
      ],
      "metadata": {
        "id": "Fd_oWVnakIzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ---------- 14) Save and download ----------\n",
        "from google.colab import files\n",
        "\n",
        "def _ensure_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "    return path\n",
        "\n",
        "\n",
        "# ---------- 14) Save and download ----------\n",
        "from google.colab import files\n",
        "\n",
        "def _ensure_dir(path): os.makedirs(path, exist_ok=True); return path\n",
        "\n",
        "def save_and_download_step8_9(df_preprocessed, *, raw_col=\"Raw text\", id_col=\"ID\", client=None, model=MODEL_ID, use_mock=USE_MOCK, out_dir=\"/content\"):\n",
        "    ts = time.strftime(\"%Y%m%d_%H%M%S\"); _ensure_dir(out_dir)\n",
        "    df_texts_8, df_map_8 = run_step8(df_preprocessed, raw_col=raw_col, id_col=id_col,\n",
        "                                     client=client if not use_mock else None, model=model, use_mock=use_mock)\n",
        "    sent_df = run_step9(df_map_8, df_texts_8)\n",
        "    p_texts = os.path.join(out_dir, f\"step8_texts_{ts}.csv\")\n",
        "    p_map   = os.path.join(out_dir, f\"step8_wordmap_checked_{ts}.csv\")\n",
        "    p_sent  = os.path.join(out_dir, f\"step9_sentence_mapping_with_boundaries_{ts}.csv\")\n",
        "    p_zip   = os.path.join(out_dir, f\"step8_9_outputs_{ts}.zip\")\n",
        "    df_texts_8.to_csv(p_texts, index=False, encoding=\"utf-8\")\n",
        "    df_map_8.to_csv(p_map,   index=False, encoding=\"utf-8\")\n",
        "    sent_df.to_csv(p_sent,   index=False, encoding=\"utf-8\")\n",
        "    with zipfile.ZipFile(p_zip, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "        zf.write(p_texts, arcname=os.path.basename(p_texts))\n",
        "        zf.write(p_map,   arcname=os.path.basename(p_map))\n",
        "        zf.write(p_sent,  arcname=os.path.basename(p_sent))\n",
        "    print(\"🗂️  Outputs:\")\n",
        "    print(\"   step8 texts:  \", p_texts)\n",
        "    print(\"   step8 map:    \", p_map)\n",
        "    print(\"   step9 table:  \", p_sent)\n",
        "    print(\"   zip bundle:   \", p_zip)\n",
        "    try:\n",
        "        files.download(p_texts); files.download(p_map); files.download(p_sent); files.download(p_zip)\n",
        "    except Exception as e:\n",
        "        print(\"Download hint:\", e)\n",
        "    return dict(step8_texts_path=p_texts, step8_map_path=p_map, step9_sentences_path=p_sent, zip_path=p_zip,\n",
        "                df_texts_8=df_texts_8, df_map_8=df_map_8, sent_df=sent_df)"
      ],
      "metadata": {
        "id": "kL4P7SiGkKGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------- 15) Run ----------\n",
        "if __name__ == '__main__':\n",
        "    results = save_and_download_step8_9(\n",
        "        df_pre,\n",
        "        raw_col=\"Raw text\",\n",
        "        id_col=\"ID\",\n",
        "        client=client,\n",
        "        model=MODEL_ID,\n",
        "        use_mock=USE_MOCK,\n",
        "        out_dir=\"/content\"\n",
        "    )\n",
        "    print(\"Shapes — step8:\", results[\"df_texts_8\"].shape, results[\"df_map_8\"].shape)\n",
        "    print(\"Shape — step9 sentences:\", results[\"sent_df\"].shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "DRFyIMpEkYnz",
        "outputId": "4d6cac43-fa18-4fa5-b881-2c6c1a74c5e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Step 8: correcting: 100%|██████████| 21/21 [02:11<00:00,  6.24s/it]\n",
            "/tmp/ipython-input-2606591141.py:352: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(per_id)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🗂️  Outputs:\n",
            "   step8 texts:   /content/step8_texts_20251106_130040.csv\n",
            "   step8 map:     /content/step8_wordmap_checked_20251106_130040.csv\n",
            "   step9 table:   /content/step9_sentence_mapping_with_boundaries_20251106_130040.csv\n",
            "   zip bundle:    /content/step8_9_outputs_20251106_130040.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0dd41c27-9aff-4574-8623-fe04177f381f\", \"step8_texts_20251106_130040.csv\", 69166)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e84b05b4-6b49-4ec2-b65e-3a90bc1cebda\", \"step8_wordmap_checked_20251106_130040.csv\", 832676)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_928dbb69-6b68-4c26-ad4c-60235383e188\", \"step9_sentence_mapping_with_boundaries_20251106_130040.csv\", 329395)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6ffe24e9-bc14-4ffc-a4e2-bc9be287efbb\", \"step8_9_outputs_20251106_130040.zip\", 190760)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes — step8: (21, 5) (7379, 21)\n",
            "Shape — step9 sentences: (561, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gl2co607jjy1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w24VmVULZT5Q"
      },
      "source": [
        "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\n"
      ]
    }
  ]
}