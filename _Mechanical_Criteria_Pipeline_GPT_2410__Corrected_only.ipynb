{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elijahManPerson/Flappy-Bird/blob/master/_Mechanical_Criteria_Pipeline_GPT_2410__Corrected_only.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oW1-qD1EBPP"
      },
      "source": [
        "# Data access and libray set up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbjvTYncD4ck"
      },
      "source": [
        "Mounting the Google Drive to access files and save them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRUXnGKtEOgU"
      },
      "source": [
        "# Step 1: Mount Google Drive\n",
        "## Purpose:\n",
        "To access and manipulate files stored in your Google Drive from the Colab environment.\n",
        "\n",
        "##What each part does\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*  drive.mount('/content/drive') starts the Google auth flow so Colab can access your Drive.\n",
        "* CHECK_PATH is the single place to point at your working folder.\n",
        "*  The helper status() prints clear pass or fail messages.\n",
        "*  Read test lists a few entries to confirm you can read.\n",
        "*  Write test creates and deletes a tiny file to confirm you can write.\n",
        "\n",
        "## Actions:\n",
        "\n",
        "**Import and Mount:** Uses google.colab.drive to mount the drive.\n",
        "Verification: Checks if the drive is successfully mounted by verifying the existence of the /content/drive/MyDrive directory.\n",
        "\n",
        "## Outcome:\n",
        "Access to files within Google Drive is established, allowing the script to read from and write to specific directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In1Zp8P5EHXW",
        "outputId": "a40b7ed5-9299-412d-d291-b2f8fcdb08d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Drive mount detected at /content/drive\n",
            "✅ MyDrive folder present\n",
            "✅ Read test passed (listed MyDrive)\n",
            "   • Sample entries: ['Colab Notebooks', 'Untitled.gdoc', 'Copy of Untitled.gdoc', 'Russelline Doxology.gdoc', 'Application Letter to St. John Bosco.gdoc']\n",
            "✅ Write test passed (created file)\n",
            "✅ Cleanup passed (deleted file)\n"
          ]
        }
      ],
      "source": [
        "# ===============================================\n",
        "# Step 1: Mount Google Drive\n",
        "# ===============================================\n",
        "# ==== Drive mount + verification ====\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  # remove force_remount if you do not want to re-prompt\n",
        "\n",
        "import os, time\n",
        "\n",
        "def status(msg, ok):\n",
        "    print((\"✅ \" if ok else \"❌ \") + msg)\n",
        "\n",
        "root_mount = '/content/drive'\n",
        "root_mydrive = '/content/drive/MyDrive'\n",
        "\n",
        "# 1) Basic mount checks\n",
        "status(\"Drive mount detected at /content/drive\", os.path.ismount(root_mount))\n",
        "status(\"MyDrive folder present\", os.path.isdir(root_mydrive))\n",
        "\n",
        "# 2) Read test: list a few entries in MyDrive\n",
        "try:\n",
        "    entries = os.listdir(root_mydrive)[:5]\n",
        "    status(\"Read test passed (listed MyDrive)\", True)\n",
        "    print(\"   • Sample entries:\", entries if entries else \"(empty)\")\n",
        "except Exception as e:\n",
        "    status(f\"Read test failed: {e}\", False)\n",
        "\n",
        "# 3) Write test: create and remove a tiny file\n",
        "probe_path = os.path.join(root_mydrive, \"_colab_mount_check.txt\")\n",
        "try:\n",
        "    with open(probe_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"colab mount check {time.time()}\\n\")\n",
        "    status(\"Write test passed (created file)\", True)\n",
        "    os.remove(probe_path)\n",
        "    status(\"Cleanup passed (deleted file)\", True)\n",
        "except Exception as e:\n",
        "    status(f\"Write test failed: {e}\", False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2.0 Data upload guide (optional)\n",
        "###Strict loader for files where:\n",
        "- first column = ID\n",
        "- last column  = Raw text\n",
        "- everything else optional\n",
        "\n",
        "###Instructions for preparing your CSV\n",
        "\n",
        "Put your unique identifier in the first column. Name it whatever you like, but “ID” is unambiguous.\n",
        "\n",
        "Put the original writing text in the last column. Name it “Raw text” for readability.\n",
        "\n",
        "Any other fields can live between first and last. They’ll be preserved, but not required.\n",
        "\n",
        "Save as CSV with UTF-8 encoding. If you’re unsure, Excel and Google Sheets default exports are fine; our loader tolerates BOM too.\n"
      ],
      "metadata": {
        "id": "Lk878yx_PRH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from google.colab import files\n",
        "\n",
        "# Build blank DataFrames\n",
        "df_min = pd.DataFrame(columns=[\"ID\", \"Raw text\"])\n",
        "df_ext = pd.DataFrame(columns=[\n",
        "    \"ID\", \"AU\", \"TS\", \"CS/PD\", \"Voc\", \"Coh\", \"Pa\", \"SS\", \"Pun\", \"Spell\",\n",
        "    \"Total\", \"WordCount\", \"yrlev\", \"Prompt ID\", \"Prompt Name\", \"Raw text\"\n",
        "])\n",
        "\n",
        "# Save to local workspace\n",
        "df_min.to_csv(\"/content/texts_template_min.csv\", index=False, encoding=\"utf-8\")\n",
        "df_ext.to_csv(\"/content/texts_template_extended.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "# Button callbacks\n",
        "def on_download_min(_):\n",
        "    files.download(\"/content/texts_template_min.csv\")\n",
        "\n",
        "def on_download_ext(_):\n",
        "    files.download(\"/content/texts_template_extended.csv\")\n",
        "\n",
        "# UI\n",
        "display(widgets.HTML(\"<h4>Download a CSV Template</h4>\"))\n",
        "display(widgets.HTML(\n",
        "    \"Use the first column as <b>ID</b> and the last as <b>Raw text</b>.<br>\"\n",
        "    \"Everything in between is optional.\"\n",
        "))\n",
        "btn_min = widgets.Button(description=\"⬇️ Download Minimal Template (ID + Raw text)\", button_style=\"primary\")\n",
        "btn_ext = widgets.Button(description=\"⬇️ Download Extended Template (Full NAPLAN-style)\", button_style=\"info\")\n",
        "\n",
        "btn_min.on_click(on_download_min)\n",
        "btn_ext.on_click(on_download_ext)\n",
        "display(widgets.HBox([btn_min, btn_ext]))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160,
          "referenced_widgets": [
            "d4f89076d0014a87b9582ed6cb558263",
            "64a2e17abeed4e9fbc896414eb641db4",
            "47f43b36aa64429494e8d36ffe9f61a8",
            "7cd87b1198a44966b2faf9fad40121e9",
            "35e9e180931444ffb9218a5febe1bac4",
            "5a5d436b1ca347ab91f8c58a50316d03",
            "92625d59828e4ae9bfa197d3c2771102",
            "72b9e7a3158b4c61a97ce17495f1b810",
            "6ae7e2b733924c4a8d198873ad11d2f2",
            "0f8569feb49a4581a179b0349898aea3",
            "8cb21460dc8140f6adc8c6f09db4e81e",
            "7bedc70a3b9748789c1e4b1e6bdf62e0",
            "26769bfa26c0453f8f9a4d9386f65fae",
            "cbf8a378dcc44df0822d086935dfe783"
          ]
        },
        "id": "jjbxtPp1MMHE",
        "outputId": "d97d3202-c1d5-45e5-aa12-216ccc84d24c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTML(value='<h4>Download a CSV Template</h4>')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4f89076d0014a87b9582ed6cb558263"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTML(value='Use the first column as <b>ID</b> and the last as <b>Raw text</b>.<br>Everything in between is opt…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7cd87b1198a44966b2faf9fad40121e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(Button(button_style='primary', description='⬇️ Download Minimal Template (ID + Raw text)', styl…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92625d59828e4ae9bfa197d3c2771102"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNU49R2-EVwD"
      },
      "source": [
        "# Step 2 — Load and Preprocess Data\n",
        "\n",
        "###Purpose\n",
        "- Bring a CSV file from Google Drive into Python as a Pandas DataFrame, then ensure the key text column is present, consistently named, and safe to use.\n",
        "- The rest of the pipeline expects a column called \"Raw text\". This step prepares that column so later steps do not break.\n",
        "\n",
        "###What you may need to change\n",
        "- DATA_PATH: set this to the exact path of your CSV inside Google Drive. If you moved the file or renamed folders, update it here.\n",
        "\n",
        "###Inputs\n",
        "- A CSV file located at DATA_PATH. The file should contain one column that holds the original student text or source text. It might be called \"Raw text\", \"Raw Text\", \"raw_text\", or something similar.\n",
        "\n",
        "###Outputs\n",
        "- df_preprocessed: a Pandas DataFrame that definitely has a column named \"Raw text\". If your file used a variant name, the code renames it to \"Raw text\" so the rest of the notebook can rely on one standard.\n",
        "- Printed checks that confirm: the path is valid, the file loaded, how many rows are present, how many are non-empty in \"Raw text\", and a small sample of the first few rows.\n",
        "\n",
        "###Key ideas\n",
        "- CSVs created on different systems sometimes include a special marker at the start of the file called a BOM. Using UTF-8 with BOM support avoids header glitches.\n",
        "- Real-world files often vary in how they label the same concept. We accept common header variants for the text column, then rename to \"Raw text\" so every downstream function can assume one consistent name.\n",
        "- It is better to stop early if the text column is missing or empty rather than let subtle errors propagate. This step fails fast with a clear message if something essential is wrong.\n",
        "\n",
        "###Actions performed in this code\n",
        "1) Validate your CSV path.  \n",
        "2) Load the CSV with safe defaults and BOM handling.  \n",
        "3) Find and standardise the \"Raw text\" column (case and spacing tolerant).  \n",
        "4) Fill missing values and report how many rows are usable.  \n",
        "5) Preview the first few rows.  \n",
        "\n",
        "###Verification prints\n",
        "- \"CSV path exists\" confirms the notebook can see the file. If you get a cross here, fix DATA_PATH.\n",
        "- \"Data loaded. Rows: X, Columns: Y\" confirms the file parsed as a table.\n",
        "- \"Non-empty 'Raw text' rows: A of B\" shows how many rows actually contain usable text after trimming blanks.\n",
        "- A preview of the first five rows lets you eyeball whether the data looks right before moving on.\n",
        "\n",
        "###Common pitfalls\n",
        "- Wrong path: the file was moved, renamed, or the folder hierarchy changed. Update DATA_PATH.\n",
        "- Unusual delimiter or BOM: some CSVs use semicolons or tabs, or include a BOM. The loader handles most cases, but if columns look fused together, specify a delimiter explicitly.\n",
        "- Header named slightly differently: if your text column label is unexpected, the auto-detection usually finds it. If not, either rename the column in the CSV or add your variant to the accepted names in the code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NOTE:\n",
        "####DATA_PATH = \"/content/drive/MyDrive/JM/Sandbox/1.Training Data/Data for Testing avg short.csv\"\n",
        "#### RAW_TEXT_ALIASES = {\"raw text\", \"raw_text\", \"rawtext\"}\n"
      ],
      "metadata": {
        "id": "G6FlfiBvDXJ5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ac1-dfhgC1SO",
        "outputId": "4695f95e-4f01-430d-fc7d-9ceefbaeb919"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ CSV path exists\n",
            "✅ Parsed using: comma. Columns: 15\n",
            "✅ Non-empty 'Raw text' rows: 21 of 21\n",
            "\n",
            "First 5 rows of 'Raw text':\n",
            "                                            Raw text\n",
            "0  There once was a girl called lilly she had pet...\n",
            "1  wrire a narrative story abouta search for some...\n",
            "2  The Failed Submarine I had always wanted go on...\n",
            "3  The diamond ring Emmy was just having breakfas...\n",
            "4  If you are locking for a dimiens go to most di...\n",
            "✅ Average character length across 'Raw text': 1504.6\n"
          ]
        }
      ],
      "source": [
        "# ===============================================\n",
        "# Step 2: Load and Preprocess Data  + optional download\n",
        "# ===============================================\n",
        "\n",
        "\n",
        "import os, io, csv\n",
        "import pandas as pd\n",
        "\n",
        "def status(msg, ok=True):\n",
        "    print((\"✅ \" if ok else \"❌ \") + msg)\n",
        "\n",
        "# ======= EDIT HERE IF NEEDED =======\n",
        "DATA_PATH = \"/content/drive/MyDrive/JM/Sandbox/1.Training Data/Data for Testing avg short.csv\"\n",
        "RAW_TEXT_ALIASES = {\"raw text\", \"raw_text\", \"rawtext\"}\n",
        "# ===================================\n",
        "\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    status(f\"File not found: {DATA_PATH}\", ok=False)\n",
        "    raise FileNotFoundError(DATA_PATH)\n",
        "status(\"CSV path exists\")\n",
        "\n",
        "def try_read(path, sep, engine=None):\n",
        "    kwargs = dict(encoding=\"utf-8-sig\", on_bad_lines=\"skip\", low_memory=False)\n",
        "    if sep is None:\n",
        "        kwargs[\"sep\"] = None\n",
        "        kwargs[\"engine\"] = \"python\"  # auto-sniff\n",
        "    else:\n",
        "        kwargs[\"sep\"] = sep\n",
        "        if engine:\n",
        "            kwargs[\"engine\"] = engine\n",
        "    try:\n",
        "        df = pd.read_csv(path, **kwargs)\n",
        "        return df\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# Try several parsers; keep the best\n",
        "candidates = [\n",
        "    (\"auto-sniff\", None, \"python\"),\n",
        "    (\"comma\", \",\", None),\n",
        "    (\"semicolon\", \";\", None),\n",
        "    (\"tab\", \"\\t\", None),\n",
        "    (\"pipe\", \"|\", None),\n",
        "]\n",
        "\n",
        "best = None\n",
        "best_score = (-1, -1)  # (has_raw_text_like, n_cols)\n",
        "\n",
        "def score_df(df):\n",
        "    if df is None or df.empty:\n",
        "        return (-1, -1)\n",
        "    cols = [c.strip().lower() for c in df.columns]\n",
        "    has_raw_like = int(any(c in RAW_TEXT_ALIASES for c in cols) or (\"raw\" in cols and \"text\" in cols))\n",
        "    return (has_raw_like, len(cols))\n",
        "\n",
        "parsed_by = None\n",
        "for name, sep, eng in candidates:\n",
        "    df = try_read(DATA_PATH, sep, eng)\n",
        "    s = score_df(df)\n",
        "    if s > best_score:\n",
        "        best_score, best, parsed_by = s, df, name\n",
        "\n",
        "if best is None or best.empty:\n",
        "    status(\"Failed to read CSV with all strategies\", ok=False)\n",
        "    raise ValueError(\"Could not parse CSV\")\n",
        "\n",
        "status(f\"Parsed using: {parsed_by}. Columns: {len(best.columns)}\")\n",
        "\n",
        "df_preprocessed = best.copy()\n",
        "\n",
        "# --- Standardise/repair the Raw text column ---\n",
        "cols_norm = {c: c.strip().lower() for c in df_preprocessed.columns}\n",
        "\n",
        "raw_col = None\n",
        "for c, norm in cols_norm.items():\n",
        "    if norm in RAW_TEXT_ALIASES:\n",
        "        raw_col = c\n",
        "        break\n",
        "\n",
        "# If we didn't find it, handle the split-header case: \"Raw\" and \"text\" as separate columns\n",
        "if raw_col is None and \"raw\" in cols_norm.values() and \"text\" in cols_norm.values():\n",
        "    # Find the actual column names that normalise to 'raw' and 'text'\n",
        "    raw_name = next(k for k, v in cols_norm.items() if v == \"raw\")\n",
        "    text_name = next(k for k, v in cols_norm.items() if v == \"text\")\n",
        "\n",
        "    # Merge them into one string column, preserving whichever side has content\n",
        "    df_preprocessed[\"Raw text\"] = (\n",
        "        df_preprocessed[raw_name].astype(str).fillna(\"\").str.rstrip() +\n",
        "        df_preprocessed[text_name].astype(str).fillna(\"\").radd(\n",
        "            df_preprocessed[text_name].astype(str).where(\n",
        "                df_preprocessed[raw_name].astype(str).str.strip().eq(\"\"),\n",
        "                \"\"  # avoid double-joining if 'raw' already holds full text\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # If that was too clever, just do a simple join with a space fallback\n",
        "    mask_all_empty = df_preprocessed[\"Raw text\"].str.strip().eq(\"\")\n",
        "    df_preprocessed.loc[mask_all_empty, \"Raw text\"] = (\n",
        "        df_preprocessed[raw_name].astype(str).str.strip() + \" \" +\n",
        "        df_preprocessed[text_name].astype(str).str.strip()\n",
        "    ).str.strip()\n",
        "\n",
        "    status(f\"Merged split columns '{raw_name}' + '{text_name}' into 'Raw text'\")\n",
        "else:\n",
        "    if raw_col is None:\n",
        "        status(\"Raw text column not found after parsing\", ok=False)\n",
        "        print(\"Columns present:\", list(df_preprocessed.columns))\n",
        "        raise KeyError(\"'Raw text' column is missing\")\n",
        "    if raw_col != \"Raw text\":\n",
        "        df_preprocessed.rename(columns={raw_col: \"Raw text\"}, inplace=True)\n",
        "        status(f\"Renamed '{raw_col}' to 'Raw text'\")\n",
        "\n",
        "# Clean and verify\n",
        "df_preprocessed[\"Raw text\"] = df_preprocessed[\"Raw text\"].fillna(\"\").astype(str)\n",
        "total = len(df_preprocessed)\n",
        "empty = df_preprocessed[\"Raw text\"].str.strip().eq(\"\").sum()\n",
        "usable = total - empty\n",
        "status(f\"Non-empty 'Raw text' rows: {usable} of {total}\")\n",
        "\n",
        "if usable == 0:\n",
        "    status(\"All 'Raw text' entries are empty after cleaning\", ok=False)\n",
        "    raise ValueError(\"No usable text in 'Raw text'\")\n",
        "\n",
        "# Peek and a quick stat\n",
        "print(\"\\nFirst 5 rows of 'Raw text':\")\n",
        "print(df_preprocessed[[\"Raw text\"]].head(5))\n",
        "\n",
        "avg_len = df_preprocessed[\"Raw text\"].str.len().mean()\n",
        "if pd.isna(avg_len):\n",
        "    avg_len = 0.0\n",
        "status(f\"Average character length across 'Raw text': {avg_len:.1f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ===============================================\n",
        "# 2A — Detect and standardise ID\n",
        "# (Start with leftmost column; look rightward for any ID-like name)\n",
        "# ===============================================\n",
        "\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def _normalize_id_series(s: pd.Series) -> pd.Series:\n",
        "    \"\"\"Return all IDs as clean strings (no .0, no float drift).\"\"\"\n",
        "    s = s.astype(str).str.replace(r\"\\.0$\", \"\", regex=True)\n",
        "    def _fix(x):\n",
        "        if any(c.isalpha() for c in x):\n",
        "            return x\n",
        "        try:\n",
        "            if \".\" in x or \"e\" in x.lower():\n",
        "                f = float(x)\n",
        "                if f.is_integer():\n",
        "                    return str(int(f))\n",
        "        except Exception:\n",
        "            pass\n",
        "        return x\n",
        "    return s.map(_fix)\n",
        "\n",
        "def find_id_column(cols):\n",
        "    \"\"\"Start with leftmost column, then move right until one looks like an ID.\"\"\"\n",
        "    # Convert to list if it's a Pandas Index\n",
        "    if isinstance(cols, pd.Index):\n",
        "        cols = cols.tolist()\n",
        "\n",
        "    if len(cols) == 0:\n",
        "        return None\n",
        "\n",
        "    # 1. Start with leftmost\n",
        "    candidate = cols[0]\n",
        "    if re.search(r\"id|identifier\", candidate.lower()):\n",
        "        return candidate\n",
        "\n",
        "    # 2. Search other columns for anything containing 'id'\n",
        "    for name in cols[1:]:\n",
        "        if re.search(r\"id|identifier\", name.lower()):\n",
        "            return name\n",
        "\n",
        "    # 3. Fallback: just use leftmost anyway\n",
        "    return cols[0]\n",
        "\n",
        "# --- run detection ---\n",
        "CANON_ID = find_id_column(df_preprocessed.columns)\n",
        "print(f\"✅ Canonical ID column selected: {CANON_ID}\")\n",
        "\n",
        "# --- assign canonical ID ---\n",
        "df_preprocessed[\"ID\"] = _normalize_id_series(df_preprocessed[CANON_ID])\n",
        "\n",
        "# quick preview\n",
        "print(\"Preview of canonical ID values:\")\n",
        "print(df_preprocessed[[CANON_ID, \"ID\"]].head(10).to_string(index=False))\n",
        "\n",
        "print(df_preprocessed[[CANON_ID, \"ID\"]].head(10).to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQ_wz-jpDRhw",
        "outputId": "3b04cd49-84d7-4ed3-c1d6-8e0baa416370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Canonical ID column selected: Research ID\n",
            "Preview of canonical ID values:\n",
            "Research ID       ID\n",
            "   BBCMHJPT BBCMHJPT\n",
            "   BBKBYNDW BBKBYNDW\n",
            "   BBRWTLYV BBRWTLYV\n",
            "   BCDVWQDF BCDVWQDF\n",
            "   BCXSFTWC BCXSFTWC\n",
            "   BGRRHYPQ BGRRHYPQ\n",
            "   BGZZHTXS BGZZHTXS\n",
            "   BHLQHBRW BHLQHBRW\n",
            "   BPHVBHZV BPHVBHZV\n",
            "   BQTNFJFX BQTNFJFX\n",
            "Research ID       ID\n",
            "   BBCMHJPT BBCMHJPT\n",
            "   BBKBYNDW BBKBYNDW\n",
            "   BBRWTLYV BBRWTLYV\n",
            "   BCDVWQDF BCDVWQDF\n",
            "   BCXSFTWC BCXSFTWC\n",
            "   BGRRHYPQ BGRRHYPQ\n",
            "   BGZZHTXS BGZZHTXS\n",
            "   BHLQHBRW BHLQHBRW\n",
            "   BPHVBHZV BPHVBHZV\n",
            "   BQTNFJFX BQTNFJFX\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NOTATION: Step 2.1 — Word and Token Stats\n",
        "\n",
        "###Purpose\n",
        "- Add quick length metrics to each row of text so you can sanity check size and plan token budgets.\n",
        "- Two columns are added to df_preprocessed: WordCount and TokenCount.\n",
        "- Estimate how much it would cost to send this dataset to the API, using your per-row TokenCount\n",
        "  and a configurable assumption for output size.\n",
        "\n",
        "###What you may need to change\n",
        "- Nothing for most cases. If you target a specific OpenAI model later, we can switch to its matching tokenizer.\n",
        "-You may need to update the cose of the API call (or could this be updated automatically?).\n",
        "\n",
        "###Inputs\n",
        "- df_preprocessed['Raw text'] produced in Step 2.\n",
        "\n",
        "###Outputs\n",
        "- df_preprocessed with two new numeric columns:\n",
        "    - WordCount  count of whitespace separated words per row\n",
        "    - TokenCount approximate token count per row using tiktoken\n",
        "- Printed checks that show which tokenizer is used, averages, a small distribution summary, and a short preview.\n",
        "- Printed summary: total input tokens, estimated output tokens, and costs for 5 models.\n",
        "- Optional: a small widget to download the DataFrame now as CSV or Excel for manual checks and token and word estimates.\n",
        "\n",
        "###Key ideas\n",
        "- Word counts are simple readability and length signals.\n",
        "- Token counts are model dependent. We try o200k_base first and fall back to cl100k_base, which keeps estimates close to how current OpenAI chat models tokenize text.\n",
        "- API pricing bills both input and output tokens.\n",
        "- We use your TokenCount as “input tokens” and estimate “output tokens” with a single ratio so you can quickly forecast spend.\n",
        "- Keep stats light and fast so they scale to larger datasets.\n",
        "\n",
        "###Verification prints\n",
        "- \"Using tokenizer: ...\" confirms the encoding choice.\n",
        "- \"Average words per Raw text: ...\" and \"Average tokens per Raw text (approx.): ...\" confirm central tendencies.\n",
        "- A min, median, max snapshot for both metrics.\n",
        "- A short preview of the DataFrame with counts.\n",
        "- Shows total rows, total/avg tokens, the output ratio used, and a cost table for:\n",
        "  gpt-4o, gpt-4o-mini, gpt-4.1, gpt-4.1-mini, gpt-3.5-turbo (Standard tier).\n",
        "\n",
        "###Common pitfalls\n",
        "- Running this before Step 2 or without a 'Raw text' column.\n",
        "- Non string entries in 'Raw text'  Step 2 already coerces to strings, so you should be safe.\n",
        "\n",
        "###Actions performed in this code\n",
        "1) Choose a tokenizer and report which one is used.\n",
        "2) Compute WordCount and TokenCount for each row.\n",
        "3) Print summary statistics and show a small preview.\n",
        "4) Offer an optional Download DataFrame button with a format picker\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W518jOrrlwnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# Step 2.1: Word and token stats\n",
        "# ===============================================\n",
        "\n",
        "import math\n",
        "import tiktoken\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Optional download widget support\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    WIDGETS_OK = True\n",
        "except Exception:\n",
        "    WIDGETS_OK = False\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "def status(msg, ok=True):\n",
        "    print((\"✅ \" if ok else \"❌ \") + msg)\n",
        "\n",
        "# 1) Choose tokenizer\n",
        "try:\n",
        "    _ENC = tiktoken.get_encoding(\"o200k_base\")\n",
        "    enc_name = \"o200k_base\"\n",
        "except Exception:\n",
        "    _ENC = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    enc_name = \"cl100k_base\"\n",
        "status(f\"Using tokenizer: {enc_name}\")\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    try:\n",
        "        return len(_ENC.encode(text or \"\"))\n",
        "    except Exception:\n",
        "        return 0\n",
        "\n",
        "def count_words(text: str) -> int:\n",
        "    if not isinstance(text, str):\n",
        "        return 0\n",
        "    # Simple whitespace split\n",
        "    return len([w for w in text.split() if w.strip()])\n",
        "\n",
        "# 2) Compute counts\n",
        "if \"Raw text\" not in df_preprocessed.columns:\n",
        "    status(\"Missing 'Raw text' column. Run Step 2 first.\", ok=False)\n",
        "    raise KeyError(\"'Raw text' column is missing\")\n",
        "\n",
        "df_preprocessed[\"WordCount\"] = df_preprocessed[\"Raw text\"].apply(count_words)\n",
        "df_preprocessed[\"TokenCount\"] = df_preprocessed[\"Raw text\"].apply(count_tokens)\n",
        "\n",
        "# 3) Summary statistics\n",
        "avg_words = df_preprocessed[\"WordCount\"].mean()\n",
        "avg_tokens = df_preprocessed[\"TokenCount\"].mean()\n",
        "median_words = df_preprocessed[\"WordCount\"].median()\n",
        "median_tokens = df_preprocessed[\"TokenCount\"].median()\n",
        "min_words = df_preprocessed[\"WordCount\"].min()\n",
        "max_words = df_preprocessed[\"WordCount\"].max()\n",
        "min_tokens = df_preprocessed[\"TokenCount\"].min()\n",
        "max_tokens = df_preprocessed[\"TokenCount\"].max()\n",
        "\n",
        "status(f\"Average words per Raw text: {avg_words:.1f}\")\n",
        "status(f\"Average tokens per Raw text (approx.): {avg_tokens:.1f}\")\n",
        "\n",
        "print(\"\\nQuick distribution check:\")\n",
        "print(f\"  Words  → min {min_words}, median {median_words:.0f}, max {max_words}\")\n",
        "print(f\"  Tokens → min {min_tokens}, median {median_tokens:.0f}, max {max_tokens}\")\n",
        "\n",
        "print(\"\\nPreview with counts:\")\n",
        "display(df_preprocessed[[\"Raw text\", \"WordCount\", \"TokenCount\"]].head(5))\n",
        "\n",
        "# 4) Optional: Download DataFrame for checks\n",
        "def _download_df(df, filename=\"df_preprocessed_counts.csv\", file_format=\"csv\"):\n",
        "    local_path = f\"/content/{filename}\"\n",
        "    if file_format.lower() == \"csv\":\n",
        "        df.to_csv(local_path, index=False, encoding=\"utf-8\")\n",
        "    elif file_format.lower() in {\"xlsx\", \"excel\"}:\n",
        "        df.to_excel(local_path, index=False)\n",
        "    else:\n",
        "        raise ValueError(\"Use 'csv' or 'xlsx'\")\n",
        "    files.download(local_path)\n",
        "\n",
        "if WIDGETS_OK:\n",
        "    fmt_dd = widgets.Dropdown(\n",
        "        options=[(\"CSV\", \"csv\"), (\"Excel (.xlsx)\", \"xlsx\")],\n",
        "        value=\"csv\",\n",
        "        description=\"Format:\",\n",
        "        layout=widgets.Layout(width=\"240px\")\n",
        "    )\n",
        "    dl_btn = widgets.Button(\n",
        "        description=\"Download DataFrame now\",\n",
        "        button_style=\"primary\",\n",
        "        tooltip=\"Click to download the DataFrame with WordCount and TokenCount\"\n",
        "    )\n",
        "    out = widgets.Output()\n",
        "\n",
        "    def on_click_download(_):\n",
        "        with out:\n",
        "            out.clear_output()\n",
        "            try:\n",
        "                _download_df(df_preprocessed, filename=\"df_preprocessed_counts.\" + fmt_dd.value, file_format=fmt_dd.value)\n",
        "                print(f\"Started download as df_preprocessed_counts.{fmt_dd.value}\")\n",
        "            except Exception as e:\n",
        "                print(\"Download failed:\", e)\n",
        "\n",
        "    dl_btn.on_click(on_click_download)\n",
        "    display(HTML(\"<b>Do you want to download the DataFrame now for checks?</b>\"))\n",
        "    display(widgets.HBox([fmt_dd, dl_btn]), out)\n",
        "else:\n",
        "    print(\"\\nWidgets not available. To download manually, run:\")\n",
        "    print(\"  df_preprocessed.to_csv('/content/df_preprocessed_counts.csv', index=False, encoding='utf-8')\")\n",
        "    print(\"  from google.colab import files; files.download('/content/df_preprocessed_counts.csv')\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412,
          "referenced_widgets": [
            "7a413d2005ac4f2cbcc5e61ead188a60",
            "ff522767bea043f89ee150d71167adbb",
            "f7ea14bca55e4fda98460c224eecf8ea",
            "79178bb01ca64c59b35aa0b30b74503b",
            "434d5481eb5543519ad64973b9d9d45a",
            "c6a517473f714b5a9e9714d4e82d953f",
            "aa3568f0b58848cd9ddd8785c1c4c356",
            "e2960dfbce354f6e97986449102eb0b2",
            "5219c9f4cf214eada10d2cee9e77914d",
            "f6c6d8f04637434e8ca943f020e4a205"
          ]
        },
        "id": "NHKls5vxkN5-",
        "outputId": "1963d359-b817-4d6d-faff-fda046d6644b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Using tokenizer: o200k_base\n",
            "✅ Average words per Raw text: 284.1\n",
            "✅ Average tokens per Raw text (approx.): 347.1\n",
            "\n",
            "Quick distribution check:\n",
            "  Words  → min 5, median 243, max 706\n",
            "  Tokens → min 6, median 275, max 822\n",
            "\n",
            "Preview with counts:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                            Raw text  WordCount  TokenCount\n",
              "0  There once was a girl called lilly she had pet...         52          61\n",
              "1  wrire a narrative story abouta search for some...         43          53\n",
              "2  The Failed Submarine I had always wanted go on...        494         578\n",
              "3  The diamond ring Emmy was just having breakfas...        243         275\n",
              "4  If you are locking for a dimiens go to most di...         57          71"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d42613fb-c88f-4933-8806-f004b2a6017a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Raw text</th>\n",
              "      <th>WordCount</th>\n",
              "      <th>TokenCount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>There once was a girl called lilly she had pet...</td>\n",
              "      <td>52</td>\n",
              "      <td>61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wrire a narrative story abouta search for some...</td>\n",
              "      <td>43</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The Failed Submarine I had always wanted go on...</td>\n",
              "      <td>494</td>\n",
              "      <td>578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The diamond ring Emmy was just having breakfas...</td>\n",
              "      <td>243</td>\n",
              "      <td>275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>If you are locking for a dimiens go to most di...</td>\n",
              "      <td>57</td>\n",
              "      <td>71</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d42613fb-c88f-4933-8806-f004b2a6017a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d42613fb-c88f-4933-8806-f004b2a6017a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d42613fb-c88f-4933-8806-f004b2a6017a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-dcc8a6f7-683f-48e7-a0a9-1841292bf275\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dcc8a6f7-683f-48e7-a0a9-1841292bf275')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-dcc8a6f7-683f-48e7-a0a9-1841292bf275 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(\\\"  from google\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Raw text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"wrire a narrative story abouta search for some thing. your story could be about a plaace or a searck for an obj ect. lt could be you can what happens if it isn own idea adout a search for something. information orbs WH\",\n          \"If you are locking for a dimiens go to most diming most dimiens it will tack abort 2 days and will ayiss and mabbing will tack 2 wecks if you are luck and most liy tack for ayissits the bist idere and do not git my stared with the ufvor was to fined dimiens im dun telling.\",\n          \"The Failed Submarine I had always wanted go on a submarine, especially a Sub 2, a type of submarine that has every thing you can think of. Floating chairs, luxurious king sized beds, rooms as large as a house, those were all average features on a Sub 2. But the major problem was the 'bargain' price. It was only about a few thousand dollars a night to go on an underwater and moving 'hotel'. But that became possible after we got an email from the NSW Lotteries giving us a few million dollars after we won. The first thing I did was persuade my mum into going on a Sub 2 trip for a few nights, and she, at last, reluctantly agreed to go on a few nights around Australia. Hopes high, I boarded the luxury submarine, and the first thing my family and I did was go to the dining room. We sat on floating recliners and ate pricey food made by a private chef on a floating table the size of a bus. But the highlight of the trip around Australia was the rooms we had booked for rooms for the journey. The door was unlocked by Face ID, and we were greated to a spectacular dining room with an underwater view. As we plonked down onto the hovering king-sized beds, the boat stopped gliding through the ocean like a knife. Assuming that it was normal, we started jumping on the bed. Suddenly, disaster struck, like lightening to a tree. The glass pane that gave us an underwater view cracked like hammer to a rice cracker. Pools of water rushed into the immense room as the waves pushed against us without mercy. We paddled out the window, and for a vertiginous moment, my family and I felt like drowning as we realised we were about a hundred meters below sea level, but the moment left as soon as it had come. I kicked my legs as brutally as I could, one hand paddling and the other clutched onto Mum. At last, I broke free of the nightmare and I was inhaling lungfulls of fresh, salty air, overwhelmed to be at the surface rather than the bottom. But it wasn't over. We were still hundreds of meters away from a sandy shore. I paddled for hours, my thigh and arms screaming at my head in pain. My family was in the same boat as me as Mum just decided to glide smoothly through the cool water. After a few hours of breathing and kicking and breathing and kicking, a familiar beach came in site. It was Bondi. I was fulled to the brim of joy, and I paddked hard with the strength I suddenly knew I had. I stood like a hero. I wanted to fell the highest spruce and strip it and trim it clean with my bare hands with the strength I knew I had, before collapsing in pools of tears.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"WordCount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 195,\n        \"min\": 43,\n        \"max\": 494,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          43,\n          57,\n          494\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TokenCount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 226,\n        \"min\": 53,\n        \"max\": 578,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          53,\n          71,\n          578\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<b>Do you want to download the DataFrame now for checks?</b>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(Dropdown(description='Format:', layout=Layout(width='240px'), options=(('CSV', 'csv'), ('Excel …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a413d2005ac4f2cbcc5e61ead188a60"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5219c9f4cf214eada10d2cee9e77914d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lARfpftGu1Lw"
      },
      "source": [
        "# Step 3: Install and Import Required Libraries\n",
        "\n",
        "###Purpose\n",
        "- Ensure the exact Python libraries and language models you need are installed and working.\n",
        "- Verify imports and show clear version numbers so you can troubleshoot quickly.\n",
        "\n",
        "###What you may need to change\n",
        "- Library versions, if you want to pin different ones for compatibility.\n",
        "- You can remove the openai uninstall if you are sure the environment is clean.\n",
        "\n",
        "###Inputs\n",
        "- Internet access in the Colab runtime to download packages and the spaCy model.\n",
        "\n",
        "###Outputs\n",
        "- Installed packages: openai, tqdm, nltk, tiktoken, spacy, pandas.\n",
        "- Downloaded spaCy model: en_core_web_sm.\n",
        "- Verified imports with printed version numbers.\n",
        "- NLTK tokenizers available: punkt and punkt_tab.\n",
        "\n",
        "###Key ideas\n",
        "- Pin versions when you want reproducibility. The defaults below are stable and pair well with Colab.\n",
        "- spaCy needs a separate model download. We fetch en_core_web_sm and then test a tiny parse.\n",
        "- NLTK recently split tokenizers, so we check both punkt and punkt_tab.\n",
        "\n",
        "###Verification prints\n",
        "- Package versions for openai, pandas, spacy, nltk, tiktoken, tqdm.\n",
        "- Confirmation that spaCy model loads and can process a sentence.\n",
        "- Confirmation that NLTK tokenizers are present.\n",
        "- A tiny tiktoken encode test to confirm the tokenizer is usable.\n",
        "\n",
        "###Common pitfalls\n",
        "- Conflicting preinstalled openai versions. We uninstall first to avoid API mismatch.\n",
        "- Missing spaCy model. Installing the library is not enough, you must download a model.\n",
        "- NLTK data not present. We fetch tokenizers to avoid runtime errors later.\n",
        "\n",
        "###Actions performed in this code\n",
        "1) Uninstall any preinstalled openai to avoid conflicts.\n",
        "2) Install required libraries with pinned versions.\n",
        "3) Download the spaCy English model en_core_web_sm.\n",
        "4) Import libraries and print versions.\n",
        "5) Verify spaCy model load.\n",
        "6) Verify NLTK tokenizers punkt and punkt_tab.\n",
        "7) Verify tiktoken by encoding a short string.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "#3.1 — Install / Upgrade (run once)\n",
        "# -----------------------\n",
        "# Uninstall preinstalled openai versions to avoid conflicts, then install v1 + dependencies\n",
        "!pip -q uninstall -y openai\n",
        "!pip -q install --upgrade \"openai==1.*\" tqdm nltk tiktoken spacy pandas==2.2.2 ipywidgets openpyxl jsonschema\n",
        "\n",
        "# Download spaCy small model\n",
        "!python -m spacy download en_core_web_sm -q\n",
        "\n",
        "\n",
        "\n",
        "print(\"⬇️ Install step finished. IMPORTANT: Restart the runtime (Runtime > Restart runtime) and then run the verification cell.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7ATUrujfg4L",
        "outputId": "4189e75d-b511-486f-d48f-9de36261a02f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/948.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/139.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m147.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "⬇️ Install step finished. IMPORTANT: Restart the runtime (Runtime > Restart runtime) and then run the verification cell.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yozAyKwAETk-",
        "outputId": "4e1368ac-b32d-43ca-8819-8b4d87f008a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Versions:\n",
            "  openai: 1.109.1\n",
            "  pandas: 2.2.2\n",
            "  spacy: 3.8.7\n",
            "  nltk: 3.9.2\n",
            "  tiktoken: 0.12.0\n",
            "  tqdm: 4.67.1\n",
            "  ipywidgets: 7.7.1\n",
            "  openpyxl: 3.1.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-862085380.py:11: DeprecationWarning: Accessing jsonschema.__version__ is deprecated and will be removed in a future release. Use importlib.metadata directly to query for jsonschema's version.\n",
            "  return getattr(m, \"__version__\", \"unknown\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  jsonschema: 4.25.1\n",
            "✅ spaCy loaded and tokenized sample: ['A', 'tiny', 'sanity', 'check', '.']\n",
            "⬇️ Downloading NLTK 'punkt' tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /content/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 'punkt' downloaded.\n",
            "✅ tiktoken ready with o200k_base. Sample tokens: 5\n"
          ]
        }
      ],
      "source": [
        "# -----------------------\n",
        "#3.2 — Imports, version checks, and tokenizer / model verification\n",
        "# -----------------------\n",
        "import importlib, sys, os, logging\n",
        "from getpass import getpass\n",
        "\n",
        "# Helper to report versions safely\n",
        "def v(name):\n",
        "    try:\n",
        "        m = importlib.import_module(name)\n",
        "        return getattr(m, \"__version__\", \"unknown\")\n",
        "    except Exception as e:\n",
        "        return f\"import failed: {e}\"\n",
        "\n",
        "modules = [\"openai\",\"pandas\",\"spacy\",\"nltk\",\"tiktoken\",\"tqdm\",\"ipywidgets\",\"openpyxl\",\"jsonschema\"]\n",
        "print(\"Versions:\")\n",
        "for name in modules:\n",
        "    print(f\"  {name}: {v(name)}\")\n",
        "\n",
        "# spaCy load check\n",
        "import spacy\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(\"A tiny sanity check.\")\n",
        "    print(\"✅ spaCy loaded and tokenized sample:\", [t.text for t in doc])\n",
        "except Exception as e:\n",
        "    print(\"❌ spaCy failed to load:\", e)\n",
        "    raise\n",
        "\n",
        "# NLTK tokenizers\n",
        "import nltk\n",
        "NLTK_DIR = \"/content/nltk_data\"\n",
        "os.makedirs(NLTK_DIR, exist_ok=True)\n",
        "if NLTK_DIR not in nltk.data.path:\n",
        "    nltk.data.path.insert(0, NLTK_DIR)\n",
        "\n",
        "try:\n",
        "    nltk.data.find(\"tokenizers/punkt\")\n",
        "    print(\"✅ NLTK 'punkt' tokenizer is present.\")\n",
        "except LookupError:\n",
        "    print(\"⬇️ Downloading NLTK 'punkt' tokenizer...\")\n",
        "    nltk.download(\"punkt\", download_dir=NLTK_DIR, quiet=False)\n",
        "    try:\n",
        "        nltk.data.find(\"tokenizers/punkt\")\n",
        "        print(\"✅ 'punkt' downloaded.\")\n",
        "    except LookupError:\n",
        "        print(\"❌ Failed to download 'punkt'.\")\n",
        "\n",
        "# tiktoken check\n",
        "import tiktoken\n",
        "try:\n",
        "    enc = None\n",
        "    try:\n",
        "        enc = tiktoken.get_encoding(\"o200k_base\")\n",
        "        enc_name = \"o200k_base\"\n",
        "    except Exception:\n",
        "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "        enc_name = \"cl100k_base\"\n",
        "    n_tokens = len(enc.encode(\"Tokenization sanity check.\"))\n",
        "    print(f\"✅ tiktoken ready with {enc_name}. Sample tokens: {n_tokens}\")\n",
        "except Exception as e:\n",
        "    print(\"❌ tiktoken failed:\", e)\n",
        "    raise\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 4.1: Optional Preempt — install common extras up front\n",
        "###Purpose\n",
        "- Preemptively install/verify common libraries and language data you’re likely to need later\n",
        "  so downstream steps don’t break mid-run.\n",
        "\n",
        "#What you may need to change\n",
        "- Toggle WANT_SPACY_MD to True if you want the larger 'en_core_web_md' spaCy model.\n",
        "- Adjust VERS pins if you prefer different versions.\n",
        "\n",
        "#Installs/Verifies\n",
        "- Libraries: ipywidgets, openpyxl (Excel export), matplotlib (plots), chardet (encoding detect),\n",
        "             pyarrow (faster IO). xlsxwriter optional as an alternate Excel engine.\n",
        "- NLTK data: punkt, punkt_tab (if available), stopwords, wordnet, omw-1.4.\n",
        "- spaCy model: ensures 'en_core_web_sm' is present; optionally downloads 'en_core_web_md'.\n",
        "\n",
        "#Outputs\n",
        "- Clear prints of what was installed or already present, plus quick sanity checks."
      ],
      "metadata": {
        "id": "SKn7iR-k_MhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ----------------- toggles -----------------\n",
        "WANT_SPACY_MD = False   # True to also download/load en_core_web_md\n",
        "# ------------------------------------------\n",
        "\n",
        "import sys, os, shutil, subprocess, importlib\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def _pip_install(spec):\n",
        "    print(f\"⬇️  Installing {spec} ...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", spec])\n",
        "\n",
        "def ensure_pkg(mod_name, spec=None):\n",
        "    try:\n",
        "        m = importlib.import_module(mod_name)\n",
        "        print(f\"✅ {mod_name} already available\")\n",
        "    except Exception:\n",
        "        _pip_install(spec or mod_name)\n",
        "        m = importlib.import_module(mod_name)\n",
        "        print(f\"✅ {mod_name} installed\")\n",
        "    return m\n",
        "\n",
        "def print_version(mod_name):\n",
        "    try:\n",
        "        m = importlib.import_module(mod_name)\n",
        "        v = getattr(m, \"__version__\", \"unknown\")\n",
        "        print(f\"   • {mod_name} {v}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   • {mod_name} version check failed: {e}\")\n",
        "\n",
        "# ---------- core convenience libraries ----------\n",
        "ipywidgets = ensure_pkg(\"ipywidgets\", \"ipywidgets==8.1.1\")\n",
        "openpyxl   = ensure_pkg(\"openpyxl\",   \"openpyxl>=3.1.2\")\n",
        "matplotlib = ensure_pkg(\"matplotlib\", \"matplotlib>=3.8.0\")\n",
        "chardet    = ensure_pkg(\"chardet\",    \"chardet>=5.2.0\")\n",
        "pyarrow    = ensure_pkg(\"pyarrow\",    \"pyarrow>=16.1.0\")\n",
        "# Optional alternative Excel writer:\n",
        "# xlsxwriter = ensure_pkg(\"xlsxwriter\", \"XlsxWriter>=3.2.0\")\n",
        "\n",
        "print(\"\\nVersions:\")\n",
        "for name in [\"ipywidgets\", \"openpyxl\", \"matplotlib\", \"chardet\", \"pyarrow\"]:\n",
        "    print_version(name)\n",
        "\n",
        "# ---------- NLTK: one directory, robust downloads ----------\n",
        "nltk = ensure_pkg(\"nltk\", \"nltk>=3.8.1\")\n",
        "\n",
        "NLTK_DIR = \"/content/nltk_data\"\n",
        "os.makedirs(NLTK_DIR, exist_ok=True)\n",
        "os.environ[\"NLTK_DATA\"] = NLTK_DIR\n",
        "# put our folder at the front of the search path\n",
        "if NLTK_DIR in nltk.data.path:\n",
        "    nltk.data.path.remove(NLTK_DIR)\n",
        "nltk.data.path.insert(0, NLTK_DIR)\n",
        "\n",
        "print(\"\\nNLTK search paths (in order):\")\n",
        "for p in nltk.data.path:\n",
        "    print(\"  -\", p)\n",
        "\n",
        "def ensure_nltk_resource(resource, kind=\"corpora\", retries=2, clean_if_stuck=True):\n",
        "    \"\"\"\n",
        "    Ensure NLTK resource (e.g. 'wordnet') exists under NLTK_DIR/kind.\n",
        "    Retries and optionally removes a partial folder if verification fails.\n",
        "    \"\"\"\n",
        "    resource_path = f\"{kind}/{resource}\"\n",
        "    target_folder = os.path.join(NLTK_DIR, kind, resource)\n",
        "\n",
        "    def _verified():\n",
        "        try:\n",
        "            nltk.data.find(resource_path)\n",
        "            return True\n",
        "        except LookupError:\n",
        "            return False\n",
        "\n",
        "    if _verified():\n",
        "        print(f\"✅ NLTK {kind} '{resource}' available\")\n",
        "        return\n",
        "\n",
        "    if clean_if_stuck and os.path.isdir(target_folder):\n",
        "        print(f\"🧹 Removing partial folder: {target_folder}\")\n",
        "        shutil.rmtree(target_folder, ignore_errors=True)\n",
        "\n",
        "    for attempt in range(1, retries + 1):\n",
        "        print(f\"⬇️  Downloading NLTK {kind} '{resource}' (attempt {attempt}/{retries}) ...\")\n",
        "        ok = nltk.download(resource, download_dir=NLTK_DIR, quiet=False)\n",
        "        exists_flag = os.path.isdir(target_folder)\n",
        "        verified = _verified()\n",
        "        print(f\"   ↳ folder exists: {exists_flag}; verified: {verified}; downloader_returned: {ok}\")\n",
        "        if exists_flag and verified:\n",
        "            print(f\"✅ NLTK {kind} '{resource}' ready at {target_folder}\")\n",
        "            return\n",
        "\n",
        "    # Last resort: show directory state and fail\n",
        "    parent = os.path.join(NLTK_DIR, kind)\n",
        "    print(f\"❌ Could not verify NLTK {kind} '{resource}' after {retries} attempts.\")\n",
        "    print(\"   Contents of\", parent, \":\", os.listdir(parent) if os.path.isdir(parent) else \"(missing)\")\n",
        "    raise LookupError(f\"Failed to ensure NLTK {resource_path}\")\n",
        "\n",
        "# Tokenizers\n",
        "ensure_nltk_resource(\"punkt\", kind=\"tokenizers\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNSlQwSI_JrW",
        "outputId": "72cbb23f-c8ed-473a-832c-bd3aac79d084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ ipywidgets already available\n",
            "✅ openpyxl already available\n",
            "✅ matplotlib already available\n",
            "✅ chardet already available\n",
            "✅ pyarrow already available\n",
            "\n",
            "Versions:\n",
            "   • ipywidgets 7.7.1\n",
            "   • openpyxl 3.1.5\n",
            "   • matplotlib 3.10.0\n",
            "   • chardet 5.2.0\n",
            "   • pyarrow 18.1.0\n",
            "✅ nltk already available\n",
            "\n",
            "NLTK search paths (in order):\n",
            "  - /content/nltk_data\n",
            "  - /root/nltk_data\n",
            "  - /usr/nltk_data\n",
            "  - /usr/share/nltk_data\n",
            "  - /usr/lib/nltk_data\n",
            "  - /usr/share/nltk_data\n",
            "  - /usr/local/share/nltk_data\n",
            "  - /usr/lib/nltk_data\n",
            "  - /usr/local/lib/nltk_data\n",
            "✅ NLTK tokenizers 'punkt' available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoZ8yJw4vZMZ"
      },
      "source": [
        "# Step 4: Import Required Libraries\n",
        "###Purpose\n",
        "- Import the libraries used for data processing, NLP, tokenisation, logging, and progress bars.\n",
        "- Load the spaCy English model prepared in Step 3.\n",
        "- Run quick sanity checks so you know everything is working before you proceed.\n",
        "\n",
        "###What you may need to change\n",
        "- Nothing in most cases. If you installed a different spaCy model name, update MODEL_NAME below.\n",
        "\n",
        "###Inputs\n",
        "- Installed packages from Step 3. SpaCy model en_core_web_sm should already be present.\n",
        "\n",
        "###Outputs\n",
        "- Imported modules in memory.\n",
        "- tqdm progress bars enabled for pandas operations.\n",
        "- A loaded spaCy pipeline in the variable `nlp`.\n",
        "- Short verification prints from spaCy, NLTK, and tiktoken.\n",
        "\n",
        "###Key ideas\n",
        "- Keep imports in one place so the rest of the notebook can assume they exist.\n",
        "- Fail early with clear messages if a critical import or model is missing.\n",
        "\n",
        "###Verification prints\n",
        "- Confirms tqdm was enabled.\n",
        "- Confirms spaCy loaded and tokenised a tiny sentence.\n",
        "- Confirms NLTK tokeniser works.\n",
        "- Confirms tiktoken can encode a short string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB_87Io4Ehog",
        "outputId": "d56ec763-8b99-4e62-d698-870f169a930c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎉 Step 4 imports and model load verified.\n"
          ]
        }
      ],
      "source": [
        "# ===============================================\n",
        "# Step 4: Import Required Libraries\n",
        "# ===============================================\n",
        "\n",
        "# ================================\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import string\n",
        "import random\n",
        "import threading\n",
        "import logging\n",
        "import difflib\n",
        "from functools import lru_cache\n",
        "\n",
        "# Set up logging for helpful debug output\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
        "\n",
        "# Ensure deterministic-ish behaviour for debugging\n",
        "random.seed(1)\n",
        "\n",
        "# Pandas + tqdm\n",
        "import pandas as pd\n",
        "try:\n",
        "    # prefer notebook tqdm if available\n",
        "    from tqdm.notebook import tqdm\n",
        "except Exception:\n",
        "    from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "logging.info(\"✅ tqdm progress bars enabled for pandas\")\n",
        "\n",
        "# NLTK setup\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "NLTK_DIR = \"/content/nltk_data\" if os.path.exists(\"/content\") else os.path.join(os.getcwd(), \"nltk_data\")\n",
        "os.makedirs(NLTK_DIR, exist_ok=True)\n",
        "if NLTK_DIR not in nltk.data.path:\n",
        "    nltk.data.path.insert(0, NLTK_DIR)\n",
        "\n",
        "# Download 'punkt' if missing, but keep output quiet unless it fails\n",
        "try:\n",
        "    nltk.data.find(\"tokenizers/punkt\")\n",
        "    logging.info(\"✅ NLTK 'punkt' tokenizer present\")\n",
        "except LookupError:\n",
        "    logging.info(\"⬇️ Downloading NLTK 'punkt' tokenizer...\")\n",
        "    nltk.download(\"punkt\", download_dir=NLTK_DIR, quiet=True)\n",
        "    try:\n",
        "        nltk.data.find(\"tokenizers/punkt\")\n",
        "        logging.info(\"✅ NLTK 'punkt' downloaded\")\n",
        "    except LookupError:\n",
        "        raise RuntimeError(\"NLTK 'punkt' tokenizer download failed. Check network or permissions.\")\n",
        "\n",
        "# spaCy + model load (single load only)\n",
        "import spacy\n",
        "MODEL_NAME = \"en_core_web_sm\"\n",
        "try:\n",
        "    nlp = spacy.load(MODEL_NAME)\n",
        "    _doc = nlp(\"Quick spaCy check.\")\n",
        "    logging.info(\"✅ spaCy model loaded (%s). Tokens: %s\", MODEL_NAME, [t.text for t in _doc])\n",
        "except Exception as e:\n",
        "    logging.error(\"❌ Could not load spaCy model '%s': %s\", MODEL_NAME, e)\n",
        "    logging.info(\"Tip: re-run your install cell to fetch the model, then restart the runtime.\")\n",
        "    raise\n",
        "\n",
        "# tiktoken robust selection\n",
        "import tiktoken\n",
        "def get_token_encoder(preferred=(\"o200k_base\", \"cl100k_base\")):\n",
        "    names = []\n",
        "    try:\n",
        "        names = tiktoken.list_encoding_names()\n",
        "    except Exception:\n",
        "        # older tiktoken versions may not expose list_encoding_names\n",
        "        pass\n",
        "    for enc in preferred:\n",
        "        try:\n",
        "            if names and enc not in names:\n",
        "                continue\n",
        "            return tiktoken.get_encoding(enc)\n",
        "        except Exception:\n",
        "            continue\n",
        "    # final fallback to a known encoding name if available\n",
        "    try:\n",
        "        return tiktoken.get_encoding(\"cl100k_base\")\n",
        "    except Exception as e:\n",
        "        logging.error(\"❌ tiktoken encoders unavailable: %s\", e)\n",
        "        raise\n",
        "\n",
        "ENC = get_token_encoder()\n",
        "_enc_name = getattr(ENC, \"__name__\", \"encoding\")\n",
        "_sample_len = len(ENC.encode(\"Tiny tiktoken check.\"))\n",
        "logging.info(\"✅ tiktoken ready (%s). Sample length: %d\", _enc_name, _sample_len)\n",
        "\n",
        "print(\"🎉 Step 4 imports and model load verified.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YYHYg-lvkQ_"
      },
      "source": [
        "# Step 5 — Configure Logging\n",
        "\n",
        "###Purpose\n",
        "- Capture info, warnings, and errors to both the Colab console and a log file for later debugging.\n",
        "- Adjust logging settings from a small UI: filenames, console/file levels, rotation size/backups.\n",
        "- Make log levels easy to change for noisy vs quiet runs.\n",
        "\n",
        "- Apply the config, run a tiny self-test, optionally preview the log tail, and download the log.\n",
        "\n",
        "\n",
        "###What you can change via UI\n",
        "- LOG_FILE: filename of the rotating log\n",
        "- CONSOLE_LEVEL: how noisy the notebook output is\n",
        "- FILE_LEVEL: how detailed the on-disk log is\n",
        "- ROTATE_MAX_MB: size per log file before rotation\n",
        "- ROTATE_BACKUPS: how many rotated files to keep\n",
        "\n",
        "###Outputs\n",
        "- Reconfigured root logger with a StreamHandler (console) and RotatingFileHandler (file)\n",
        "- Self-test entries written to the log\n",
        "- Optional log tail preview\n",
        "\n",
        "###Key ideas\n",
        "- Set the root logger level high enough to allow through what handlers need.\n",
        "- Handlers have their own levels. Console can be quieter than file.\n",
        "- Rotating logs prevent a single giant file.\n",
        "\n",
        "###Verification prints\n",
        "- A quick self test logs INFO, WARNING, ERROR, and an example exception.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "27fcaf055eb1429db925e3c6c683d178",
            "6b8460e0c6424fc8b72d7604f862eadd",
            "d5e8edc5018d421c8fe9f69feecf382d",
            "7017c3a5f42e4947b35dbca068c97b10",
            "2db8955796e54fb185461a83837d05b3",
            "e09538b3076f44d48ef9c8977d505adc",
            "ffbe3d0376de4810ad3d891c28b9d228",
            "d9238a7c4dda44689c2d99bcb4b8f664",
            "d882205f28804c9790f518f99c9a259c",
            "ff7d720013fc4549bc1363afc40853bb",
            "c91a9bcfb177498dbf270e9130376904",
            "9d3919a18ab5458da205ad354a0a8932",
            "096a5c8749034475be7bb41eb0ffb7d9",
            "31e6d092e79c47099de1e61cf6f56a04",
            "b31adef57a01463189c274a7298e514b",
            "27ab465b8c30476e858dac4778e8d69d",
            "16323fc0498244b0bebb5f757592844e",
            "9626750eb4944c90bead5f005db6dc90",
            "6b2a29a858b24ca2aff04abb8b51cb4e",
            "cfb6b36cd7374daf8ba866f9f0b0ea67",
            "bf674f21c260434387d7ab9bc1c285fc",
            "59ed1d3577b84053ac65ad5bfcfac94d",
            "8ea5835ed77444beaaefeab148a839e3",
            "8954110dcfe844e88d4b33d76ae67951",
            "53f82bd2f03c467981b2598c3d19a33f",
            "bf6193f186cd473bb5ffb70c7c5072fb",
            "8c0f453912ad4f7bbc7bb185f7dc36e3",
            "6170857048a344b6889ed45df0882bbf",
            "2eb7c3e50526407c8395cbf25c673445",
            "3330ec0e435f45779a6a56154410258a",
            "2002b828679d427bbdfc1cf4b1289ba9",
            "5e5d9f96c0f94fa4a1fcefdc42be93c2",
            "ba1f49a8c1424d79bae7967563884e34",
            "7a51a1f593fb4a1ea986286ece75ed85"
          ]
        },
        "id": "l0m7W001J_fm",
        "outputId": "384fc873-1ca0-47d8-d960-44e22c347a0a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h4>Logging Control Panel</h4>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Text(value='text_correction.log', description='LOG_FILE:', layout=Layout(width='420px')), HBox(…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27fcaf055eb1429db925e3c6c683d178"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "\n",
        "import os, io, logging, traceback\n",
        "from logging.handlers import RotatingFileHandler\n",
        "from IPython.display import display, HTML\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    WIDGETS_OK = True\n",
        "except Exception:\n",
        "    WIDGETS_OK = False\n",
        "\n",
        "# ---- helper: make or update logging based on UI values ----\n",
        "def configure_logging(log_file: str,\n",
        "                      console_level: int,\n",
        "                      file_level: int,\n",
        "                      rotate_max_mb: int,\n",
        "                      rotate_backups: int) -> logging.Logger:\n",
        "    logger = logging.getLogger()\n",
        "    logger.setLevel(logging.DEBUG)  # always let handlers filter\n",
        "\n",
        "    # Remove old handlers (avoids duplicates on re-run)\n",
        "    for h in list(logger.handlers):\n",
        "        logger.removeHandler(h)\n",
        "\n",
        "    fmt = logging.Formatter(\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\")\n",
        "\n",
        "    # Console\n",
        "    ch = logging.StreamHandler()\n",
        "    ch.setLevel(console_level)\n",
        "    ch.setFormatter(fmt)\n",
        "    logger.addHandler(ch)\n",
        "\n",
        "    # Rotating file\n",
        "    fh = RotatingFileHandler(\n",
        "        log_file,\n",
        "        maxBytes=rotate_max_mb * 1024 * 1024,\n",
        "        backupCount=rotate_backups,\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "    fh.setLevel(file_level)\n",
        "    fh.setFormatter(fmt)\n",
        "    logger.addHandler(fh)\n",
        "\n",
        "    # Chattery libs can be quieted if you like\n",
        "    logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
        "    logging.getLogger(\"tqdm\").setLevel(logging.WARNING)\n",
        "\n",
        "    # Self-test\n",
        "    logger.info(\"Logging control panel: INFO test\")\n",
        "    logger.warning(\"Logging control panel: WARNING test\")\n",
        "    try:\n",
        "        1/0\n",
        "    except ZeroDivisionError:\n",
        "        logger.error(\"Logging control panel: ERROR test with traceback\")\n",
        "        logger.error(traceback.format_exc())\n",
        "\n",
        "    return logger\n",
        "\n",
        "# ---- level maps for dropdowns ----\n",
        "LEVELS = {\n",
        "    \"DEBUG (most verbose)\": logging.DEBUG,\n",
        "    \"INFO (standard)\": logging.INFO,\n",
        "    \"WARNING (only important)\": logging.WARNING,\n",
        "    \"ERROR (failures only)\": logging.ERROR,\n",
        "    \"CRITICAL\": logging.CRITICAL\n",
        "}\n",
        "\n",
        "# ---- defaults (you can change here if you want different initial values) ----\n",
        "DEFAULT_LOG_FILE = \"text_correction.log\"\n",
        "DEFAULT_CONSOLE = \"INFO (standard)\"\n",
        "DEFAULT_FILE = \"DEBUG (most verbose)\"\n",
        "DEFAULT_ROTATE_MB = 5\n",
        "DEFAULT_BACKUPS = 3\n",
        "\n",
        "if not WIDGETS_OK:\n",
        "    print(\"ipywidgets not available. Install it first or run the preempt cell. \"\n",
        "          \"Meanwhile, you can configure logging programmatically via Step 5.\")\n",
        "else:\n",
        "    # ---- UI controls ----\n",
        "    log_file_text = widgets.Text(\n",
        "        value=DEFAULT_LOG_FILE,\n",
        "        description=\"LOG_FILE:\",\n",
        "        layout=widgets.Layout(width=\"420px\")\n",
        "    )\n",
        "    console_level_dd = widgets.Dropdown(\n",
        "        options=list(LEVELS.keys()),\n",
        "        value=DEFAULT_CONSOLE,\n",
        "        description=\"Console:\",\n",
        "        layout=widgets.Layout(width=\"420px\")\n",
        "    )\n",
        "    file_level_dd = widgets.Dropdown(\n",
        "        options=list(LEVELS.keys()),\n",
        "        value=DEFAULT_FILE,\n",
        "        description=\"File:\",\n",
        "        layout=widgets.Layout(width=\"420px\")\n",
        "    )\n",
        "    rotate_mb_slider = widgets.IntSlider(\n",
        "        value=DEFAULT_ROTATE_MB, min=1, max=50, step=1,\n",
        "        description=\"Rotate MB:\",\n",
        "        readout=True, continuous_update=False\n",
        "    )\n",
        "    backups_slider = widgets.IntSlider(\n",
        "        value=DEFAULT_BACKUPS, min=0, max=20, step=1,\n",
        "        description=\"Backups:\",\n",
        "        readout=True, continuous_update=False\n",
        "    )\n",
        "\n",
        "    apply_btn = widgets.Button(\n",
        "        description=\"Apply logging settings\",\n",
        "        button_style=\"primary\",\n",
        "        tooltip=\"Configure handlers and run a quick self-test\"\n",
        "    )\n",
        "    show_tail_btn = widgets.Button(\n",
        "        description=\"Show log tail\",\n",
        "        tooltip=\"Display the last lines of the current log file\"\n",
        "    )\n",
        "    download_btn = widgets.Button(\n",
        "        description=\"Download log\",\n",
        "        tooltip=\"Download the current log file\"\n",
        "    )\n",
        "    out = widgets.Output()\n",
        "\n",
        "    # ---- callbacks ----\n",
        "    def on_apply_clicked(_):\n",
        "        with out:\n",
        "            out.clear_output()\n",
        "            log_file = log_file_text.value.strip() or DEFAULT_LOG_FILE\n",
        "            console_level = LEVELS[console_level_dd.value]\n",
        "            file_level = LEVELS[file_level_dd.value]\n",
        "            rotate_mb = int(rotate_mb_slider.value)\n",
        "            backups = int(backups_slider.value)\n",
        "\n",
        "            logger = configure_logging(\n",
        "                log_file=log_file,\n",
        "                console_level=console_level,\n",
        "                file_level=file_level,\n",
        "                rotate_max_mb=rotate_mb,\n",
        "                rotate_backups=backups\n",
        "            )\n",
        "            print(\"✅ Applied logging settings\")\n",
        "            print(f\"   LOG_FILE: {log_file}\")\n",
        "            print(f\"   CONSOLE_LEVEL: {console_level_dd.value}\")\n",
        "            print(f\"   FILE_LEVEL: {file_level_dd.value}\")\n",
        "            print(f\"   ROTATE_MAX_MB: {rotate_mb}\")\n",
        "            print(f\"   ROTATE_BACKUPS: {backups}\")\n",
        "            print(\"\\nWrote self-test lines. Use 'Show log tail' to preview.\")\n",
        "\n",
        "    def on_show_tail_clicked(_):\n",
        "        with out:\n",
        "            log_file = log_file_text.value.strip() or DEFAULT_LOG_FILE\n",
        "            out.clear_output()\n",
        "            if not os.path.exists(log_file):\n",
        "                print(f\"❌ Log file not found yet: {log_file}\")\n",
        "                print(\"   Click 'Apply logging settings' first.\")\n",
        "                return\n",
        "            try:\n",
        "                # read last ~100 lines\n",
        "                with open(log_file, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "                    lines = f.readlines()[-100:]\n",
        "                print(f\"--- tail of {log_file} (last {len(lines)} lines) ---\")\n",
        "                for line in lines:\n",
        "                    print(line.rstrip())\n",
        "            except Exception as e:\n",
        "                print(\"❌ Failed to read log:\", e)\n",
        "\n",
        "    def on_download_clicked(_):\n",
        "        from google.colab import files\n",
        "        with out:\n",
        "            out.clear_output()\n",
        "            log_file = log_file_text.value.strip() or DEFAULT_LOG_FILE\n",
        "            if not os.path.exists(log_file):\n",
        "                print(f\"❌ Log file not found: {log_file}\")\n",
        "                print(\"   Click 'Apply logging settings' first.\")\n",
        "                return\n",
        "            try:\n",
        "                files.download(log_file)\n",
        "                print(f\"Started download: {log_file}\")\n",
        "            except Exception as e:\n",
        "                print(\"❌ Download failed:\", e)\n",
        "\n",
        "    apply_btn.on_click(on_apply_clicked)\n",
        "    show_tail_btn.on_click(on_show_tail_clicked)\n",
        "    download_btn.on_click(on_download_clicked)\n",
        "\n",
        "    # ---- layout ----\n",
        "    display(HTML(\"<h4>Logging Control Panel</h4>\"))\n",
        "    display(\n",
        "        widgets.VBox([\n",
        "            log_file_text,\n",
        "            widgets.HBox([console_level_dd, file_level_dd]),\n",
        "            widgets.HBox([rotate_mb_slider, backups_slider]),\n",
        "            widgets.HBox([apply_btn, show_tail_btn, download_btn]),\n",
        "            out\n",
        "        ])\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg-HSr-nvz8a"
      },
      "source": [
        "# Step 6: Securely Prompt for OpenAI API Key and test OpenAI API key\n",
        "\n",
        "###Purpose\n",
        "- Obtain the OpenAI API key securely (without exposing it in code cells).\n",
        "- Verify that the key works by attempting a harmless API call (listing models).\n",
        "\n",
        "###What you may need to change\n",
        "- Nothing. Just run this cell; it will prompt you for the key if it’s not already in memory.\n",
        "\n",
        "###Inputs\n",
        "- User-entered API key (via getpass prompt).\n",
        "\n",
        "###Outputs\n",
        "- Environment variable OPENAI_API_KEY set for this session.\n",
        "- Verification message confirming that the key works, or an error if it doesn’t.\n",
        "\n",
        "###Key ideas\n",
        "- Never hard-code your API key into notebooks.\n",
        "- The key is stored only in the temporary runtime environment variable, not in the notebook file.\n",
        "- Verification uses a lightweight call (model listing).\n",
        "\n",
        "###Verification prints\n",
        "- “✅ OpenAI API key is valid.” if the call succeeds.\n",
        "- A clear “❌ Invalid or failed verification” message if not."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "try:\n",
        "    import openai\n",
        "    print(\"openai version:\", getattr(openai, \"__version__\", \"unknown\"))\n",
        "except Exception as e:\n",
        "    print(\"openai import failed:\", e)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwMpyfVlhAl3",
        "outputId": "5509e1af-a971-4407-ea12-2b5c48af09bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openai version: 1.109.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9TSbkS06NoP",
        "outputId": "d3e4731f-db24-4a1e-baae-1e5bb35f5df2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenAI API key (input hidden). Leave blank to skip verification.\n",
            "API key: ··········\n",
            "✅ OpenAI API key verified (v1). Sample models: ['gpt-4-0613', 'gpt-4', 'gpt-3.5-turbo', 'gpt-5-search-api-2025-10-14', 'gpt-realtime-mini', 'gpt-realtime-mini-2025-10-06', 'sora-2', 'sora-2-pro']\n"
          ]
        }
      ],
      "source": [
        "# ===============================================\n",
        "# Step 6: Securely Prompt for OpenAI API Key\n",
        "# ===============================================\n",
        "# Step 6 (fixed for openai v1)\n",
        "import os\n",
        "from getpass import getpass\n",
        "from openai import OpenAI\n",
        "\n",
        "def ensure_api_key():\n",
        "    key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "    if key:\n",
        "        return key\n",
        "    print(\"Enter your OpenAI API key (input hidden). Leave blank to skip verification.\")\n",
        "    key = getpass(\"API key: \").strip()\n",
        "    if not key:\n",
        "        return None\n",
        "    os.environ[\"OPENAI_API_KEY\"] = key\n",
        "    return key\n",
        "\n",
        "def verify_openai_api_key_v1():\n",
        "    key = ensure_api_key()\n",
        "    if not key:\n",
        "        print(\"Skipped OpenAI verification. Set OPENAI_API_KEY to run API calls.\")\n",
        "        return None\n",
        "    client = OpenAI()  # reads OPENAI_API_KEY from environment\n",
        "    try:\n",
        "        models = client.models.list()\n",
        "        sample = [m.id for m in models.data[:8]]\n",
        "        print(\"✅ OpenAI API key verified (v1). Sample models:\", sample)\n",
        "        return client\n",
        "    except Exception as e:\n",
        "        print(\"❌ Verification failed:\", type(e).__name__, str(e))\n",
        "        print(\"\\nCommon fixes:\")\n",
        "        print(\"  • Ensure the key is correct and has required permissions.\")\n",
        "        print(\"  • Check network access from the runtime.\")\n",
        "        print(\"  • If your code still uses old v0 calls (openai.Model, openai.ChatCompletion.create), update them.\")\n",
        "        raise\n",
        "\n",
        "# Run it and store client in the variable `client` for later use\n",
        "client = verify_openai_api_key_v1()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "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\n"
      ],
      "metadata": {
        "id": "HOM7fiq7SMfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "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\n"
      ],
      "metadata": {
        "id": "NCLX7kAeSKcv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0zTUEyvpILFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "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\n"
      ],
      "metadata": {
        "id": "AV2CL_bqCraR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "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\n"
      ],
      "metadata": {
        "id": "h0GfxegWcuXv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB3FLRKUwQAV"
      },
      "source": [
        "# Step 6.2: Ensure NLTK Data is Available\n",
        "\n",
        "\n",
        "###Purpose\n",
        "- Confirm that the required NLTK datasets, particularly the 'punkt' tokenizer,\n",
        "  are available for sentence and word tokenization.\n",
        "- Automatically download them into a local or Colab-safe directory if missing.\n",
        "\n",
        "###What you may need to change\n",
        "- nltk_data_path: set this to a writable directory if running outside Colab\n",
        "  (e.g., './nltk_data' for local use).\n",
        "\n",
        "###Inputs\n",
        "- None (downloads handled internally if needed).\n",
        "\n",
        "###Outputs\n",
        "- Verified or newly downloaded 'punkt' tokenizer package.\n",
        "\n",
        "###Key ideas\n",
        "- NLTK looks for data in a set of known paths; adding a custom directory avoids permission issues.\n",
        "- Downloading into /content/nltk_data keeps notebooks portable and clean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUWxluoWG4ez",
        "outputId": "b83d12b7-d540-443a-f98b-b660f63ad416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ NLTK 'punkt' tokenizer is already available.\n",
            "✅ 'punkt_tab' (improved tokenizer) also downloaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /content/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ===============================================\n",
        "# Step 6.2: Ensure NLTK Data is Available\n",
        "# ===============================================\n",
        "import nltk, os\n",
        "\n",
        "# Specify a safe directory for NLTK data (works in Colab or local)\n",
        "nltk_data_path = \"/content/nltk_data\" if os.path.exists(\"/content\") else \"./nltk_data\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(nltk_data_path, exist_ok=True)\n",
        "\n",
        "# Ensure our path is part of NLTK’s search list\n",
        "if nltk_data_path not in nltk.data.path:\n",
        "    nltk.data.path.append(nltk_data_path)\n",
        "\n",
        "# Try locating the punkt tokenizer\n",
        "try:\n",
        "    nltk.data.find(\"tokenizers/punkt\")\n",
        "    print(\"✅ NLTK 'punkt' tokenizer is already available.\")\n",
        "except LookupError:\n",
        "    print(\"⚙️ Downloading NLTK 'punkt' tokenizer...\")\n",
        "    nltk.download(\"punkt\", download_dir=nltk_data_path)\n",
        "    try:\n",
        "        nltk.data.find(\"tokenizers/punkt\")\n",
        "        print(\"✅ 'punkt' tokenizer downloaded successfully.\")\n",
        "    except LookupError:\n",
        "        print(\"❌ Failed to download 'punkt' tokenizer. Check network or permissions.\")\n",
        "\n",
        "# Optional: newer punkt tokenizer for NLTK ≥ 3.8\n",
        "try:\n",
        "    nltk.data.find(\"tokenizers/punkt_tab\")\n",
        "except LookupError:\n",
        "    try:\n",
        "        nltk.download(\"punkt_tab\", download_dir=nltk_data_path)\n",
        "        print(\"✅ 'punkt_tab' (improved tokenizer) also downloaded.\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTZjaIlJwYxy"
      },
      "source": [
        "# Step 7: Define Utility Functions for ChatGPT API Interaction\n",
        "## Purpose:\n",
        "To create reusable functions that handle interactions with the OpenAI API, including making requests with retry mechanisms to handle potential API errors.\n",
        "\n",
        "##Input: Function parameters (things you pass in)\n",
        "\n",
        "-prompt — the text you want the model to read and reply to. Required.\n",
        "\n",
        "-model=\"gpt-4o\" — which model to call if you don’t give one. Default is gpt-4o.For cost:  https://platform.openai.com/settings/organization/limits\n",
        "\n",
        "-max_retries=5 — how many times to try again if the API fails temporarily. Five tries is a common default.\n",
        "\n",
        "-backoff_factor=2 — controls how long to wait between retries. Bigger numbers make you wait longer each retry.\n",
        "\n",
        "-temperature=0 — controls randomness. Zero aims for consistent, repeatable answers.\n",
        "\n",
        "-max_tokens=8192 — the maximum number of tokens you allow the model to output.\n",
        "\n",
        "## Output: Local variables inside the function\n",
        "\n",
        "attempt — which retry number you are on in the loop.\n",
        "\n",
        "response — the raw object the API returns.\n",
        "\n",
        "content — the actual text reply you extract from the response.\n",
        "\n",
        "usage — token usage information (helps with billing and diagnostics).\n",
        "\n",
        "e — the caught exception when something goes wrong.\n",
        "\n",
        "wait — how many seconds the code sleeps before retrying.\n",
        "\n",
        "## Actions:\n",
        "\n",
        "* Define call_chatgpt Function:\n",
        "Parameters: Accepts prompt, model, max_retries, backoff_factor, temperature, and max_tokens.\n",
        "* Functionality: Attempts to call the OpenAI API with exponential backoff in case of failures like rate limits or timeouts.\n",
        "* Error Handling: Catches specific OpenAI errors and retries the request after waiting for a calculated duration.\n",
        "* Returns: The content of the API response or None if all retries fail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONQmC4wvtzoD"
      },
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# Step 7: Define Utility Functions for ChatGPT API Interaction\n",
        "# ===============================================\n",
        "# ---- v1-safe, timeout + retries, correct exception classes ----\n",
        "import time, random, logging\n",
        "from openai import OpenAI\n",
        "from openai import (\n",
        "    APIError, APIConnectionError, RateLimitError, APITimeoutError,\n",
        "    AuthenticationError\n",
        ")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def _token_len(text, encoder):\n",
        "    try:\n",
        "        return len(encoder.encode(text or \"\"))\n",
        "    except Exception:\n",
        "        return 0\n",
        "\n",
        "def call_chatgpt_v1(\n",
        "    prompt,\n",
        "    model=\"gpt-4o\",\n",
        "    max_retries=5,\n",
        "    backoff_factor=2.0,\n",
        "    temperature=0.0,\n",
        "    max_tokens=None,\n",
        "    client: OpenAI = None,\n",
        "    token_encoder=None,\n",
        "    model_context_limit=131072,\n",
        "    request_timeout=30  # seconds\n",
        "):\n",
        "    client = client or OpenAI()\n",
        "    # per-request timeout (prevents hanging)\n",
        "    if hasattr(client, \"with_options\"):\n",
        "        client = client.with_options(timeout=request_timeout)\n",
        "\n",
        "    if max_tokens is None and token_encoder is not None:\n",
        "        prompt_toks = _token_len(prompt, token_encoder)\n",
        "        max_tokens = max(256, min(4096, model_context_limit - prompt_toks - 1024))\n",
        "    elif max_tokens is None:\n",
        "        max_tokens = 1024\n",
        "\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=temperature,\n",
        "                max_tokens=max_tokens,\n",
        "            )\n",
        "            content = resp.choices[0].message.content\n",
        "            usage = getattr(resp, \"usage\", {}) or {}\n",
        "            return content, usage\n",
        "\n",
        "        except (RateLimitError, APIConnectionError, APITimeoutError, APIError) as e:\n",
        "            wait = min(60, (backoff_factor ** attempt) + random.uniform(0, 1))\n",
        "            logger.warning(\n",
        "                \"API transient error (attempt %d/%d): %s. Retrying in %.1fs\",\n",
        "                attempt, max_retries, str(e), wait\n",
        "            )\n",
        "            time.sleep(wait)\n",
        "            continue\n",
        "\n",
        "        except AuthenticationError as e:\n",
        "            logger.error(\"Authentication failed: %s\", e)\n",
        "            raise\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            logger.error(\"Interrupted by user. Aborting cleanly.\")\n",
        "            raise\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.exception(\"Unexpected error on attempt %d: %s\", attempt, e)\n",
        "            break\n",
        "\n",
        "    logger.error(\"Max retries exceeded. Returning (None, {}).\")\n",
        "    return None, {}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtofZzoKwscq"
      },
      "source": [
        "# Step 8: Correct Text by Sentence\n",
        "## Purpose:\n",
        "To process raw text by correcting punctuation, grammar, and spelling. Then creating a new field with the corrected text. Also, itm maaps the correctext back to the raw text.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Actions:\n",
        "* Important all helpers and loggers.\n",
        "* Define corrrect_and_mmap Function:\n",
        " * Parameters: Accepts text (the raw input text).\n",
        " * Prompt Creation: Constructs a detailed prompt with instructions for the AI to perform specific tasks on the text: correcting punctuation, grammar, and spelling. Providing bencharks to map corrected to segmented by word.\n",
        " * API Call: Uses the previously defined call_chatgpt function to send the prompt to the OpenAI API.\n",
        " * Response Handling: Extracts JSON from the API response and parses it into a Python dictionary.\n",
        " * Error Handling: Catches JSON decoding errors and returns an empty dictionary if parsing fails.\n",
        "*Mock Text for Testing:\n",
        "at the end, create mock version of correct_and_map is used to simulate API behavior for testing purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DjTMA1gic_yC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# STEP 8 & 9 — Final rebuild (titles, dialogue, alignment, boundaries)\n",
        "# ===============================================\n",
        "\n",
        "import re, json, math, time, os, io, zipfile, logging, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# -------------------------\n",
        "# Config & logger\n",
        "# -------------------------\n",
        "MODEL_ID   = \"gpt-4o\"\n",
        "MAX_TOKENS = 1200\n",
        "USE_MOCK   = False  # flip to True for offline tests\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    _COLAB = True\n",
        "except Exception:\n",
        "    _COLAB = False\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Utilities\n",
        "# -------------------------\n",
        "def _extract_first_json_object(txt: str):\n",
        "    \"\"\"Robustly pull the first {...} as JSON.\"\"\"\n",
        "    if not txt:\n",
        "        return None\n",
        "    start = txt.find(\"{\")\n",
        "    if start < 0:\n",
        "        return None\n",
        "    depth, in_str, esc = 0, False, False\n",
        "    for i in range(start, len(txt)):\n",
        "        ch = txt[i]\n",
        "        if in_str:\n",
        "            if esc: esc = False\n",
        "            elif ch == \"\\\\\": esc = True\n",
        "            elif ch == '\"': in_str = False\n",
        "        else:\n",
        "            if ch == '\"': in_str = True\n",
        "            elif ch == \"{\": depth += 1\n",
        "            elif ch == \"}\":\n",
        "                depth -= 1\n",
        "                if depth == 0:\n",
        "                    frag = txt[start:i+1]\n",
        "                    try:\n",
        "                        return json.loads(frag)\n",
        "                    except Exception:\n",
        "                        return None\n",
        "    return None\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Mojibake normalization\n",
        "# -------------------------\n",
        "_MOJIBAKE_FIXES = [\n",
        "    (r\"â€”\", \"—\"),   # em dash\n",
        "    (r\"â€“\", \"–\"),   # en dash\n",
        "    (r\"â€˜\", \"‘\"), (r\"â€™\", \"’\"),  # single quotes\n",
        "    (r\"â€œ\", \"“\"), (r\"â€\\u009d\", \"”\"), (r\"â€\\u009D\", \"”\"), (r\"â€�\", \"”\"), # double quotes\n",
        "    (r\"â€¦\", \"…\"),   # ellipsis\n",
        "    (r\"Â \", \" \"),    # stray non-breaking space\n",
        "]\n",
        "\n",
        "def normalize_mojibake(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return s\n",
        "    out = s\n",
        "    for pat, repl in _MOJIBAKE_FIXES:\n",
        "        out = re.sub(pat, repl)\n",
        "    return out\n",
        "\n",
        "def apply_mojibake_normalization(df_texts: pd.DataFrame,\n",
        "                                 corrected_col: str = \"Corrected text (8)\") -> pd.DataFrame:\n",
        "    df = df_texts.copy()\n",
        "    if corrected_col in df.columns:\n",
        "        df[corrected_col] = df[corrected_col].map(normalize_mojibake)\n",
        "    return df\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Step 8A: Correction (LLM) with narrative & dialogue extraction\n",
        "# -------------------------\n",
        "def _mock_correct_and_tag(s: str):\n",
        "    \"\"\"Deterministic mock: simple caps + final punct, no tags.\"\"\"\n",
        "    t = (s or \"\").strip()\n",
        "    m = re.search(r\"[A-Za-z]\", t)\n",
        "    if m:\n",
        "        i = m.start()\n",
        "        t = t[:i] + t[i].upper() + t[i+1:]\n",
        "    if t and not re.search(r\"[.!?…]\\s*$\", t):\n",
        "        t += \".\"\n",
        "    return {\n",
        "        \"corrected_text\": t,\n",
        "        \"narrative_tags\": [],\n",
        "        \"dialogue_spans\": []\n",
        "    }\n",
        "\n",
        "def correct_text_and_tags(raw: str, client=None, model=MODEL_ID, use_mock=USE_MOCK):\n",
        "    s = str(raw or \"\")\n",
        "    if use_mock or client is None:\n",
        "        js = _mock_correct_and_tag(s)\n",
        "        return js[\"corrected_text\"], js[\"narrative_tags\"], js[\"dialogue_spans\"], \"mock\"\n",
        "\n",
        "    from openai import OpenAI\n",
        "    _client = client or OpenAI()\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a meticulous copy-editor. Fix punctuation, grammar, and spelling.\n",
        "Keep meaning and paragraphing. Use standard English punctuation.\n",
        "\n",
        "Also detect and return:\n",
        "- Title(s): a short heading at the very beginning (if present).\n",
        "- Temporal transitions (e.g., \"The next day\", \"Two weeks later\", explicit dates like \"March 19, 2032\").\n",
        "- Closure tags (e.g., \"THE END\", \"To be continued\").\n",
        "- Dialogue spans: character start/end offsets (0-based, end-exclusive) for direct speech (quoted speech).\n",
        "\n",
        "Return ONLY strict JSON:\n",
        "{{\n",
        "  \"corrected_text\": \"...\",\n",
        "  \"narrative_tags\": [\n",
        "     {{\"type\":\"title\"|\"temporal\"|\"closure\",\"text\":\"...\",\"start\":int,\"end\":int}}, ...\n",
        "  ],\n",
        "  \"dialogue_spans\": [\n",
        "     {{\"start\":int,\"end\":int}}, ...\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Text:\n",
        "<<<BEGIN>>>\n",
        "{s}\n",
        "<<<END>>>\n",
        "\"\"\".strip()\n",
        "\n",
        "    resp = _client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "        temperature=0.0,\n",
        "        max_tokens=MAX_TOKENS\n",
        "    )\n",
        "    out = (resp.choices[0].message.content or \"\").strip()\n",
        "    js = _extract_first_json_object(out)\n",
        "    if not isinstance(js, dict):\n",
        "        # fallback minimal\n",
        "        return s, [], [], model\n",
        "\n",
        "    corrected = normalize_mojibake(str(js.get(\"corrected_text\",\"\")).strip())\n",
        "    tags = js.get(\"narrative_tags\") or []\n",
        "    spans = js.get(\"dialogue_spans\") or []\n",
        "    # sanity types\n",
        "    if not isinstance(tags, list): tags = []\n",
        "    if not isinstance(spans, list): spans = []\n",
        "    return corrected, tags, spans, model\n",
        "\n",
        "\n",
        "def run_correct_only(\n",
        "    df_in: pd.DataFrame,\n",
        "    text_col=\"Raw text\",\n",
        "    id_col=\"ID\",\n",
        "    client=None,\n",
        "    model=MODEL_ID,\n",
        "    use_mock=USE_MOCK,\n",
        "    out_col=\"Corrected text (8)\"\n",
        ") -> pd.DataFrame:\n",
        "    if text_col not in df_in.columns:\n",
        "        raise KeyError(f\"Missing required column: {text_col}\")\n",
        "    df = df_in.copy()\n",
        "\n",
        "    # Normalize ID strings\n",
        "    def _norm_id_series(s: pd.Series) -> pd.Series:\n",
        "        s = s.astype(str).str.replace(r\"\\.0$\", \"\", regex=True)\n",
        "        def _fix(x):\n",
        "            if any(c.isalpha() for c in x):\n",
        "                return x\n",
        "            try:\n",
        "                if \".\" in x or \"e\" in x.lower():\n",
        "                    f = float(x)\n",
        "                    if f.is_integer():\n",
        "                        return str(int(f))\n",
        "            except Exception:\n",
        "                pass\n",
        "            return x\n",
        "        return s.map(_fix)\n",
        "\n",
        "    if id_col not in df.columns:\n",
        "        df[id_col] = pd.RangeIndex(len(df)).astype(str)\n",
        "    else:\n",
        "        df[id_col] = _norm_id_series(df[id_col])\n",
        "\n",
        "    corr, tags_json, dial_json = [], [], []\n",
        "    for raw in df[text_col].astype(str).tolist():\n",
        "        fixed, tags, spans, _src = correct_text_and_tags(raw, client=client, model=model, use_mock=use_mock)\n",
        "        corr.append(fixed)\n",
        "        tags_json.append(json.dumps(tags, ensure_ascii=False))\n",
        "        dial_json.append(json.dumps(spans, ensure_ascii=False))\n",
        "    df[out_col] = corr\n",
        "    df[\"NarrativeTagsJSON\"] = tags_json\n",
        "    df[\"DialogueSpansJSON\"] = dial_json\n",
        "    return df\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Step 8B: tokenize + map (with merged-word split)\n",
        "# -------------------------\n",
        "_WORD_RX = re.compile(r\"\\w\", flags=re.UNICODE)\n",
        "\n",
        "def _split_merged_word(tok: str):\n",
        "    \"\"\"\n",
        "    Heuristic: split once when ALLCAPS is followed by lowercase (YAAAYwe -> YAAAY + we).\n",
        "    \"\"\"\n",
        "    if not tok or not tok.isalpha():\n",
        "        return [tok]\n",
        "    m = re.match(r\"^([A-Z]{2,})([a-z].*)$\", tok)\n",
        "    if m:\n",
        "        left, right = m.group(1), m.group(2)\n",
        "        return [left, right]\n",
        "    return [tok]\n",
        "\n",
        "def _simple_tokenize_with_splitting(s: str):\n",
        "    base = re.findall(r\"\\w+|[^\\w\\s]\", s or \"\", flags=re.UNICODE)\n",
        "    out = []\n",
        "    for t in base:\n",
        "        if re.fullmatch(r\"\\w+\", t):\n",
        "            out.extend(_split_merged_word(t))\n",
        "        else:\n",
        "            out.append(t)\n",
        "    return out\n",
        "\n",
        "def _rebuild_offsets_with_splitting(text, tokens):\n",
        "    spans = []\n",
        "    i = 0\n",
        "    n = len(text)\n",
        "    for tok in tokens:\n",
        "        if tok == \"\" or tok is None:\n",
        "            spans.append((i, i))\n",
        "            continue\n",
        "        pos = text.find(tok, i)\n",
        "        if pos >= 0:\n",
        "            start, end = pos, pos + len(tok)\n",
        "            spans.append((start, end))\n",
        "            i = end\n",
        "        else:\n",
        "            # best-effort slice to keep indices monotone\n",
        "            j = i\n",
        "            while j < n and text[j].isspace():\n",
        "                j += 1\n",
        "            start = j\n",
        "            end = min(n, start + len(tok))\n",
        "            spans.append((start, end))\n",
        "            i = end\n",
        "    return spans\n",
        "\n",
        "def _is_word(tok: str) -> bool:\n",
        "    return bool(tok) and bool(_WORD_RX.search(tok))\n",
        "\n",
        "def build_word_map(raw_text, corr_text):\n",
        "    raw_tokens  = _simple_tokenize_with_splitting(raw_text or \"\")\n",
        "    corr_tokens = _simple_tokenize_with_splitting(corr_text or \"\")\n",
        "\n",
        "    raw_spans  = _rebuild_offsets_with_splitting(raw_text or \"\", raw_tokens)\n",
        "    corr_spans = _rebuild_offsets_with_splitting(corr_text or \"\", corr_tokens)\n",
        "\n",
        "    sm = SequenceMatcher(a=[t.lower() for t in raw_tokens],\n",
        "                         b=[t.lower() for t in corr_tokens],\n",
        "                         autojunk=False)\n",
        "\n",
        "    rows = []\n",
        "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
        "        if tag == \"equal\":\n",
        "            for k in range(i2 - i1):\n",
        "                r_tok = raw_tokens[i1 + k]; c_tok = corr_tokens[j1 + k]\n",
        "                r_start, r_end = raw_spans[i1 + k]\n",
        "                c_start, c_end = corr_spans[j1 + k]\n",
        "                rows.append({\n",
        "                    \"raw_index\": i1 + k, \"raw_token\": r_tok, \"raw_start\": r_start, \"raw_end\": r_end,\n",
        "                    \"corr_index\": j1 + k, \"corr_token\": c_tok, \"corr_start\": c_start, \"corr_end\": c_end,\n",
        "                    \"op\": \"equal\", \"equal_ci\": (r_tok == c_tok), \"error_type\": \"Equal\"\n",
        "                })\n",
        "\n",
        "        elif tag == \"replace\":\n",
        "            m = min(i2 - i1, j2 - j1)\n",
        "            for k in range(m):\n",
        "                r_tok = raw_tokens[i1 + k]; c_tok = corr_tokens[j1 + k]\n",
        "                r_start, r_end = raw_spans[i1 + k]\n",
        "                c_start, c_end = corr_spans[j1 + k]\n",
        "                err = \"Spelling\" if (r_tok.lower() != c_tok.lower() and r_tok.isalpha() and c_tok.isalpha()) else \"Replacement\"\n",
        "                rows.append({\n",
        "                    \"raw_index\": i1 + k, \"raw_token\": r_tok, \"raw_start\": r_start, \"raw_end\": r_end,\n",
        "                    \"corr_index\": j1 + k, \"corr_token\": c_tok, \"corr_start\": c_start, \"corr_end\": c_end,\n",
        "                    \"op\": \"replace\", \"equal_ci\": (r_tok.lower() == c_tok.lower()), \"error_type\": err\n",
        "                })\n",
        "            for k in range(i1 + m, i2):  # deletions\n",
        "                r_tok = raw_tokens[k]; r_start, r_end = raw_spans[k]\n",
        "                rows.append({\n",
        "                    \"raw_index\": k, \"raw_token\": r_tok, \"raw_start\": r_start, \"raw_end\": r_end,\n",
        "                    \"corr_index\": None, \"corr_token\": None, \"corr_start\": None, \"corr_end\": None,\n",
        "                    \"op\": \"delete\", \"equal_ci\": False,\n",
        "                    \"error_type\": \"PunctuationDeletion\" if not _is_word(r_tok) else \"Deletion\"\n",
        "                })\n",
        "            for k in range(j1 + m, j2):  # insertions\n",
        "                c_tok = corr_tokens[k]; c_start, c_end = corr_spans[k]\n",
        "                rows.append({\n",
        "                    \"raw_index\": None, \"raw_token\": None, \"raw_start\": None, \"raw_end\": None,\n",
        "                    \"corr_index\": k, \"corr_token\": c_tok, \"corr_start\": c_start, \"corr_end\": c_end,\n",
        "                    \"op\": \"insert\", \"equal_ci\": False,\n",
        "                    \"error_type\": \"PunctuationInsertion\" if not _is_word(c_tok) else \"Insertion\"\n",
        "                })\n",
        "\n",
        "        elif tag == \"delete\":\n",
        "            for k in range(i1, i2):\n",
        "                r_tok = raw_tokens[k]; r_start, r_end = raw_spans[k]\n",
        "                rows.append({\n",
        "                    \"raw_index\": k, \"raw_token\": r_tok, \"raw_start\": r_start, \"raw_end\": r_end,\n",
        "                    \"corr_index\": None, \"corr_token\": None, \"corr_start\": None, \"corr_end\": None,\n",
        "                    \"op\": \"delete\", \"equal_ci\": False,\n",
        "                    \"error_type\": \"PunctuationDeletion\" if not _is_word(r_tok) else \"Deletion\"\n",
        "                })\n",
        "\n",
        "        elif tag == \"insert\":\n",
        "            for k in range(j1, j2):\n",
        "                c_tok = corr_tokens[k]; c_start, c_end = corr_spans[k]\n",
        "                rows.append({\n",
        "                    \"raw_index\": None, \"raw_token\": None, \"raw_start\": None, \"raw_end\": None,\n",
        "                    \"corr_index\": k, \"corr_token\": c_tok, \"corr_start\": c_start, \"corr_end\": c_end,\n",
        "                    \"op\": \"insert\", \"equal_ci\": False,\n",
        "                    \"error_type\": \"PunctuationInsertion\" if not _is_word(c_tok) else \"Insertion\"\n",
        "                })\n",
        "\n",
        "    return rows\n",
        "\n",
        "def run_mapping_only(df_with_corr,\n",
        "                     id_col=\"ID\",\n",
        "                     raw_col=\"Raw text\",\n",
        "                     corr_col=\"Corrected text (8)\"):\n",
        "    if raw_col not in df_with_corr.columns or corr_col not in df_with_corr.columns:\n",
        "        raise KeyError(f\"Need both {raw_col} and {corr_col} present\")\n",
        "\n",
        "    df = df_with_corr.copy()\n",
        "    if id_col not in df.columns:\n",
        "        df[id_col] = pd.RangeIndex(len(df)).astype(str)\n",
        "    df[id_col] = df[id_col].astype(str)\n",
        "\n",
        "    all_rows = []\n",
        "    for order, (rid, raw, cor) in enumerate(zip(df[id_col].tolist(),\n",
        "                                                df[raw_col].astype(str).tolist(),\n",
        "                                                df[corr_col].astype(str).tolist())):\n",
        "        rows = build_word_map(raw, cor)\n",
        "        if not rows:\n",
        "            rows = [{\n",
        "                \"raw_index\": np.nan, \"raw_token\": None, \"raw_start\": np.nan, \"raw_end\": np.nan,\n",
        "                \"corr_index\": np.nan, \"corr_token\": None, \"corr_start\": np.nan, \"corr_end\": np.nan,\n",
        "                \"op\": \"empty\", \"equal_ci\": False, \"error_type\": \"EmptyText\"\n",
        "            }]\n",
        "        for r in rows:\n",
        "            rec = {\"RowID\": rid, \"DocOrder\": order, **r}\n",
        "            rec[\"Changed\"] = (r.get(\"op\") != \"equal\")\n",
        "            all_rows.append(rec)\n",
        "\n",
        "    map_df = pd.DataFrame(all_rows)\n",
        "    texts_out = df.copy()\n",
        "    return map_df, texts_out\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Alignment repair: pair delete+insert → replace\n",
        "# -------------------------\n",
        "def _jw_ratio(a: str, b: str) -> float:\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "def _is_alpha_word(s: str) -> bool:\n",
        "    return bool(s) and s.isalpha()\n",
        "\n",
        "def postprocess_pair_deletions_and_insertions(df_map: pd.DataFrame,\n",
        "                                              max_window: int = 6,\n",
        "                                              min_sim: float = 0.55) -> pd.DataFrame:\n",
        "    if df_map.empty:\n",
        "        return df_map\n",
        "\n",
        "    df = df_map.copy()\n",
        "    sort_cols = [\"ID\"]\n",
        "    if \"CorrSentenceID\" in df.columns: sort_cols.append(\"CorrSentenceID\")\n",
        "    if \"corr_index\" in df.columns:     sort_cols.append(\"corr_index\")\n",
        "    sort_cols = [c for c in sort_cols if c in df.columns]\n",
        "    if not sort_cols: sort_cols = [\"RowID\"]\n",
        "\n",
        "    df[\"_ord\"] = np.arange(len(df))\n",
        "    df = df.sort_values(sort_cols + [\"_ord\"], kind=\"mergesort\")\n",
        "\n",
        "    to_drop = set()\n",
        "\n",
        "    def handle_group(g):\n",
        "        idxs = list(g.index)\n",
        "        deletes = [i for i in idxs if g.at[i, \"op\"] == \"delete\" and _is_alpha_word(str(g.at[i, \"raw_token\"] or \"\"))]\n",
        "        inserts = [i for i in idxs if g.at[i, \"op\"] == \"insert\" and _is_alpha_word(str(g.at[i, \"corr_token\"] or \"\"))]\n",
        "        used_insert = set()\n",
        "\n",
        "        for di in deletes:\n",
        "            if di in to_drop:\n",
        "                continue\n",
        "            raw_tok = str(g.at[di, \"raw_token\"] or \"\")\n",
        "            pos_di = idxs.index(di)\n",
        "            best, best_sim = None, 0.0\n",
        "            for ii in inserts:\n",
        "                if ii in used_insert or ii in to_drop:\n",
        "                    continue\n",
        "                pos_ii = idxs.index(ii)\n",
        "                if abs(pos_ii - pos_di) > max_window:\n",
        "                    continue\n",
        "                corr_tok = str(g.at[ii, \"corr_token\"] or \"\")\n",
        "                sim = _jw_ratio(raw_tok.lower(), corr_tok.lower())\n",
        "                if sim >= min_sim and (best is None or sim > best_sim or (sim == best_sim and abs(pos_ii - pos_di) < abs(idxs.index(best)-pos_di))):\n",
        "                    best, best_sim = ii, sim\n",
        "            if best is not None:\n",
        "                # convert delete→replace using corr fields from insert\n",
        "                g.at[di, \"op\"] = \"replace\"\n",
        "                g.at[di, \"equal_ci\"] = (raw_tok.lower() == str(g.at[best, \"corr_token\"] or \"\").lower())\n",
        "                g.at[di, \"error_type\"] = \"Replacement\"\n",
        "                for c in [\"corr_index\", \"corr_token\", \"corr_start\", \"corr_end\"]:\n",
        "                    if c in g.columns:\n",
        "                        g.at[di, c] = g.at[best, c]\n",
        "                used_insert.add(best)\n",
        "                to_drop.add(best)\n",
        "        return g\n",
        "\n",
        "    df = df.groupby(\"ID\", group_keys=False).apply(handle_group, include_groups=False)\n",
        "    if to_drop:\n",
        "        df = df.drop(index=list(to_drop))\n",
        "    df = df.sort_values(sort_cols + [\"_ord\"], kind=\"mergesort\").drop(columns=[\"_ord\"], errors=\"ignore\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Step 8C: CorrSentenceID (robust endings + quotes/ellipsis)\n",
        "# -------------------------\n",
        "ABBREV = {\n",
        "    \"mr.\",\"mrs.\",\"ms.\",\"dr.\",\"prof.\",\"sr.\",\"jr.\",\"st.\",\"vs.\",\"etc.\",\n",
        "    \"e.g.\",\"i.e.\",\"cf.\",\"fig.\",\"ex.\",\"no.\",\"approx.\",\"circa.\",\"ca.\",\n",
        "    \"dept.\",\"est.\",\"misc.\",\"rev.\",\"jan.\",\"feb.\",\"mar.\",\"apr.\",\"jun.\",\n",
        "    \"jul.\",\"aug.\",\"sep.\",\"sept.\",\"oct.\",\"nov.\",\"dec.\"\n",
        "}\n",
        "TERMINALS = {\".\", \"!\", \"?\", \"…\", \"...\", \"?!\", \"!?\"}\n",
        "CLOSERS   = {\")\", \"]\", \"}\", \"”\", \"’\", \"»\"}\n",
        "OPENERS   = {\"(\", \"[\", \"{\", \"“\", \"‘\", \"«\"}\n",
        "\n",
        "RE_INITIAL       = re.compile(r\"^[A-Z]\\.$\")\n",
        "RE_INITIAL_PAIR  = re.compile(r\"^[A-Z]\\.[A-Z]\\.$\")\n",
        "RE_NUM_WITH_DOT  = re.compile(r\"^\\d+\\.$\")\n",
        "RE_SECTION_NUM   = re.compile(r\"^\\d+(?:\\.\\d+){1,3}$\")\n",
        "RE_DOT_TAIL      = re.compile(r\"^\\.\\d+$\")\n",
        "RE_ELLIPSIS      = re.compile(r\"^\\.\\.\\.$\")\n",
        "RE_ALPHA_PAREN   = re.compile(r\"^[A-Za-z]\\)$\")\n",
        "\n",
        "def _tok(x):\n",
        "    if pd.isna(x) or x is None: return \"\"\n",
        "    return str(x)\n",
        "\n",
        "def _is_ellipsis_triplet(i, toks):\n",
        "    return (i+2 < len(toks) and toks[i] == \".\" and toks[i+1] == \".\" and toks[i+2] == \".\")\n",
        "\n",
        "def _is_terminal_token(tok: str, prev_tok: str, next_tok: str) -> bool:\n",
        "    t = tok.strip()\n",
        "    if not t:\n",
        "        return False\n",
        "    if t == \"…\" or t == \"...\" or RE_ELLIPSIS.fullmatch(t):\n",
        "        return True\n",
        "    if t in {\"?!\",\"!?\"}:\n",
        "        return True\n",
        "    if t in {\"!\",\"?\"}:\n",
        "        return True\n",
        "    if t == \".\":\n",
        "        p = (prev_tok or \"\").strip()\n",
        "        n = (next_tok or \"\").strip()\n",
        "        low_prev = p.lower()\n",
        "        if low_prev in ABBREV:\n",
        "            return False\n",
        "        if RE_INITIAL.fullmatch(p) or RE_INITIAL_PAIR.fullmatch(p):\n",
        "            return False\n",
        "        if RE_SECTION_NUM.fullmatch(p):\n",
        "            return False\n",
        "        if RE_NUM_WITH_DOT.fullmatch(p) and (n and re.match(r\"[A-Za-z(“\\\"'\\[]\", n)):\n",
        "            return False\n",
        "        if RE_DOT_TAIL.fullmatch(n):\n",
        "            return False\n",
        "        if n.isdigit():\n",
        "            return False\n",
        "        return True\n",
        "    if t == \")\" and RE_ALPHA_PAREN.fullmatch(prev_tok):\n",
        "        return False\n",
        "    return False\n",
        "\n",
        "def _likely_ascii_opening(prev_tok: str, next_tok: str) -> bool:\n",
        "    prev = (prev_tok or \"\").strip()\n",
        "    nxt  = (next_tok or \"\").strip()\n",
        "    if prev == \"\" or prev in TERMINALS or prev in OPENERS:\n",
        "        return True\n",
        "    if nxt and nxt not in TERMINALS and nxt not in CLOSERS:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def assign_corr_sentence_ids(df_map: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df_map.copy()\n",
        "    if \"RowID\" in df.columns:\n",
        "        df[\"ID\"] = df[\"RowID\"].astype(str)\n",
        "    elif \"ID\" not in df.columns:\n",
        "        df[\"ID\"] = df.index.astype(str)\n",
        "\n",
        "    has_ci = \"corr_index\" in df.columns\n",
        "    if has_ci and \"corr_index_orig\" not in df.columns:\n",
        "        df[\"corr_index_orig\"] = df[\"corr_index\"]\n",
        "\n",
        "    def _stable_sort_key(g: pd.DataFrame) -> pd.Series:\n",
        "        pos = pd.Series(np.arange(len(g)), index=g.index, dtype=float)\n",
        "        if has_ci:\n",
        "            ci = pd.to_numeric(g[\"corr_index\"], errors=\"coerce\")\n",
        "            nan_mask = ci.isna()\n",
        "            bump = (pos - pos.min()) / max((pos.max() - pos.min()), 1) * 1e-6\n",
        "            return ci.where(~nan_mask, 1e9) + bump\n",
        "        return pos\n",
        "\n",
        "    df[\"_sort_key\"] = df.groupby(\"ID\", group_keys=False).apply(_stable_sort_key, include_groups=False)\n",
        "\n",
        "    def _assign(g: pd.DataFrame) -> pd.Series:\n",
        "        g = g.sort_values(\"_sort_key\", kind=\"mergesort\")\n",
        "        toks = (g[\"corr_token\"] if \"corr_token\" in g.columns else g[\"raw_token\"]).map(_tok).tolist()\n",
        "\n",
        "        sids = []\n",
        "        sent_id = 0\n",
        "        pending_end = False\n",
        "        i = 0\n",
        "        while i < len(toks):\n",
        "            tok = toks[i].strip()\n",
        "            prev_tok = toks[i-1].strip() if i > 0 else \"\"\n",
        "            next_tok = toks[i+1].strip() if i+1 < len(toks) else \"\"\n",
        "\n",
        "            if _is_ellipsis_triplet(i, toks):\n",
        "                pending_end = True\n",
        "                sids.append(sent_id)\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            if pending_end:\n",
        "                if tok in CLOSERS or (tok == '\"' and not _likely_ascii_opening(prev_tok, next_tok)):\n",
        "                    sids.append(sent_id); i += 1; continue\n",
        "                if tok in OPENERS or (tok == '\"' and _likely_ascii_opening(prev_tok, next_tok)):\n",
        "                    sent_id += 1; pending_end = False; sids.append(sent_id); i += 1; continue\n",
        "                sent_id += 1; pending_end = False; sids.append(sent_id); i += 1; continue\n",
        "            else:\n",
        "                sids.append(sent_id); i += 1\n",
        "\n",
        "            if _is_terminal_token(tok, prev_tok, next_tok):\n",
        "                pending_end = True\n",
        "\n",
        "        return pd.Series(sids, index=g.index).reindex(g.index)\n",
        "\n",
        "    df[\"CorrSentenceID\"] = (\n",
        "        df.groupby(\"ID\", group_keys=False).apply(_assign, include_groups=False).astype(\"Int64\")\n",
        "    )\n",
        "    df.drop(columns=[\"_sort_key\"], inplace=True, errors=\"ignore\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Step 8D: Title isolation using LLM spans (no extra word)\n",
        "#           + renumber: use s000 only if a title exists\n",
        "# -------------------------\n",
        "def _parse_json_list(s):\n",
        "    try:\n",
        "        v = json.loads(s) if isinstance(s, str) else (s or [])\n",
        "        return v if isinstance(v, list) else []\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def mark_title_tokens(df_map: pd.DataFrame, df_texts_with_tags: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    If a narrative tag of type 'title' exists, mark exactly those corrected\n",
        "    character spans as TITLE=True (no leaked extra word). Otherwise use a fallback heuristic.\n",
        "    After marking, if any TITLE exists for an ID, keep them in sentence 0 and bump others +1.\n",
        "    If no title exists, bump all sentences +1 so 0 is unused (as requested).\n",
        "    \"\"\"\n",
        "    df = df_map.copy()\n",
        "\n",
        "    # Attach per-ID title spans from df_texts_with_tags\n",
        "    title_spans = {}\n",
        "    for _id, corr_text, tags_s in zip(df_texts_with_tags[\"ID\"].astype(str),\n",
        "                                      df_texts_with_tags[\"Corrected text (8)\"].astype(str),\n",
        "                                      df_texts_with_tags[\"NarrativeTagsJSON\"].astype(str)):\n",
        "        tags = _parse_json_list(tags_s)\n",
        "        spans = [(t.get(\"start\", -1), t.get(\"end\", -1))\n",
        "                 for t in tags if isinstance(t, dict) and t.get(\"type\") == \"title\"\n",
        "                 and isinstance(t.get(\"start\"), int) and isinstance(t.get(\"end\"), int)]\n",
        "        if spans:\n",
        "            # Only consider titles that start near the beginning\n",
        "            spans = [sp for sp in spans if 0 <= sp[0] < max(60, len(corr_text)//3)]\n",
        "        title_spans[str(_id)] = spans\n",
        "\n",
        "    if \"Sentence Boundaries\" not in df.columns:\n",
        "        df[\"Sentence Boundaries\"] = \"\"\n",
        "    if \"TITLE\" not in df.columns:\n",
        "        df[\"TITLE\"] = False\n",
        "\n",
        "    # exact span marking based on corr_start/end\n",
        "    def mark_group(g):\n",
        "        gid = str(g[\"ID\"].iloc[0])\n",
        "        spans = title_spans.get(gid, [])\n",
        "        if spans:\n",
        "            for (s0, s1) in spans:\n",
        "                idx = g.index[(g[\"corr_start\"] >= s0) & (g[\"corr_end\"] <= s1)]\n",
        "                if len(idx):\n",
        "                    df.loc[idx, \"TITLE\"] = True\n",
        "                    df.loc[idx, \"Sentence Boundaries\"] = \"Title\"\n",
        "        return g\n",
        "\n",
        "    df.groupby(\"ID\", group_keys=False).apply(mark_group, include_groups=False)\n",
        "\n",
        "    # If no title spans found for an ID, leave TITLE as False (no s000)\n",
        "\n",
        "    # Sentence ID renumbering\n",
        "    def bump_group(g):\n",
        "        has_title = bool(g[\"TITLE\"].any())\n",
        "        sids = g[\"CorrSentenceID\"].astype(\"Int64\").copy()\n",
        "        if has_title:\n",
        "            # keep titles at 0, bump non-title by +1 if they are 0\n",
        "            bump_mask = (~g[\"TITLE\"]) & sids.notna()\n",
        "            sids.loc[bump_mask] = sids.loc[bump_mask] + 1\n",
        "        else:\n",
        "            # no title: bump all sentences by +1 so s000 is unused\n",
        "            mask = sids.notna()\n",
        "            sids.loc[mask] = sids.loc[mask] + 1\n",
        "        g[\"CorrSentenceID\"] = sids\n",
        "        return g\n",
        "\n",
        "    df = df.groupby(\"ID\", group_keys=False).apply(bump_group, include_groups=False).reset_index(drop=True)\n",
        "\n",
        "    # Rebuild SentenceRef\n",
        "    def _sid3(x):\n",
        "        try: return f\"{int(x):03d}\"\n",
        "        except: return \"000\"\n",
        "    df[\"SentenceRef\"] = df[\"ID\"].astype(str) + \"_s\" + df[\"CorrSentenceID\"].map(_sid3)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Step 8E: Boundary flags (case-pattern; skip inserted punct)\n",
        "# -------------------------\n",
        "TERMINALS_HARD = {\".\",\"!\",\"?\",\"…\",\"...\",\"?!\",\"!?\"}\n",
        "OPENING_PUNCT  = {'\"', \"“\", \"‘\", \"«\", \"(\", \"[\", \"{\"}\n",
        "\n",
        "def _first_alpha_case(s: str):\n",
        "    m = re.search(r\"[A-Za-z]\", s or \"\")\n",
        "    if not m:\n",
        "        return None\n",
        "    return s[m.start()].isupper()\n",
        "\n",
        "def _is_wordish(tok: str) -> bool:\n",
        "    return bool(tok) and bool(re.search(r\"\\w\", tok))\n",
        "\n",
        "def add_sentence_boundary_flags(df_map: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df_map.copy()\n",
        "    for col in (\"Sentence Boundaries\", \"BoundaryCheck\"):\n",
        "        if col not in df.columns:\n",
        "            df[col] = \"\"\n",
        "\n",
        "    if \"corr_index\" not in df.columns:\n",
        "        df[\"corr_index\"] = np.nan\n",
        "    df[\"_rowpos\"] = np.arange(len(df))\n",
        "    df[\"_sort_corr\"] = pd.to_numeric(df[\"corr_index\"], errors=\"coerce\").fillna(1e12) + (df[\"_rowpos\"]*1e-9)\n",
        "    df = df.sort_values([\"ID\",\"CorrSentenceID\",\"_sort_corr\"], kind=\"mergesort\")\n",
        "\n",
        "    def _first_content_row(g: pd.DataFrame):\n",
        "        # Prefer first non-insert word-ish token; skip opening quotes\n",
        "        ops = g[\"op\"] if \"op\" in g.columns else pd.Series([\"equal\"]*len(g), index=g.index)\n",
        "        for idx in g.index:\n",
        "            tok = str(g.at[idx, \"corr_token\"])\n",
        "            if tok in OPENING_PUNCT:\n",
        "                continue\n",
        "            if not _is_wordish(tok):\n",
        "                continue\n",
        "            if ops.at[idx] == \"insert\":\n",
        "                continue\n",
        "            return idx\n",
        "        # fallback: any word-ish\n",
        "        for idx in g.index:\n",
        "            tok = str(g.at[idx, \"corr_token\"])\n",
        "            if tok in OPENING_PUNCT:\n",
        "                continue\n",
        "            if _is_wordish(tok):\n",
        "                return idx\n",
        "        return None\n",
        "\n",
        "    def _last_terminal_row(g: pd.DataFrame):\n",
        "        toks = g[\"corr_token\"].astype(str).tolist()\n",
        "        for pos in range(len(toks)-1, -1, -1):\n",
        "            if toks[pos] in TERMINALS_HARD:\n",
        "                return g.index[pos]\n",
        "        return None\n",
        "\n",
        "    for (id_, sid), g in df.groupby([\"ID\",\"CorrSentenceID\"], sort=False):\n",
        "        if \"TITLE\" in g.columns and g[\"TITLE\"].any():\n",
        "            df.loc[g.index, \"Sentence Boundaries\"] = \"Title\"\n",
        "            continue\n",
        "\n",
        "        g = g.sort_values(\"_sort_corr\", kind=\"mergesort\")\n",
        "        b = _first_content_row(g)\n",
        "        e = _last_terminal_row(g)\n",
        "\n",
        "        if b is not None:\n",
        "            prev = df.at[b, \"Sentence Boundaries\"]\n",
        "            if prev.strip() != \"Title\":\n",
        "                df.at[b, \"Sentence Boundaries\"] = prev + (\" | \" if prev else \"\") + \"Sentence Beginning\"\n",
        "                raw_tok = str(df.at[b, \"raw_token\"] or \"\") if \"raw_token\" in df.columns else \"\"\n",
        "                corr_tok = str(df.at[b, \"corr_token\"] or \"\")\n",
        "                ra = _first_alpha_case(raw_tok)\n",
        "                ca = _first_alpha_case(corr_tok)\n",
        "                tag = \"Unknown Beginning\" if (ra is None or ca is None) else (\"Correct Beginning\" if (ra == ca) else \"Incorrect Beginning\")\n",
        "                prev = df.at[b, \"BoundaryCheck\"]\n",
        "                df.at[b, \"BoundaryCheck\"] = prev + (\" | \" if prev else \"\") + tag\n",
        "\n",
        "        if e is not None:\n",
        "            prev = df.at[e, \"Sentence Boundaries\"]\n",
        "            if prev.strip() != \"Title\":\n",
        "                df.at[e, \"Sentence Boundaries\"] = prev + (\" | \" if prev else \"\") + \"Sentence Ending\"\n",
        "                ce_tok = str(df.at[e, \"corr_token\"] or \"\")\n",
        "                tag = \"Correct Ending\" if (ce_tok in TERMINALS_HARD) else \"Incorrect Ending\"\n",
        "                prev = df.at[e, \"BoundaryCheck\"]\n",
        "                df.at[e, \"BoundaryCheck\"] = prev + (\" | \" if prev else \"\") + tag\n",
        "\n",
        "    def _sid3(x):\n",
        "        try: return f\"{int(x):03d}\"\n",
        "        except: return \"000\"\n",
        "    df[\"SentenceRef\"] = df[\"ID\"].astype(str) + \"_s\" + df[\"CorrSentenceID\"].map(_sid3)\n",
        "\n",
        "    df.drop(columns=[\"_rowpos\",\"_sort_corr\"], inplace=True, errors=\"ignore\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Detokenizer (for Step 9)\n",
        "# -------------------------\n",
        "NO_SPACE_BEFORE = set(list(\".,;:!?)]}\\\"'»”’…\"))\n",
        "NO_SPACE_AFTER  = set(list(\"([{\\\"'«“‘\"))\n",
        "\n",
        "def _detok(tokens):\n",
        "    out = []\n",
        "    for t in tokens:\n",
        "        if t is None or (isinstance(t, float) and math.isnan(t)):\n",
        "            continue\n",
        "        t = str(t)\n",
        "        if not out:\n",
        "            out.append(t); continue\n",
        "        prev = out[-1]\n",
        "        if t in NO_SPACE_BEFORE or re.fullmatch(r\"[.]{3}\", t):\n",
        "            out[-1] = prev + t\n",
        "        elif prev in NO_SPACE_AFTER:\n",
        "            out[-1] = prev + t\n",
        "        else:\n",
        "            out.append(\" \" + t)\n",
        "    return \"\".join(out).strip()\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Public Step 8\n",
        "# -------------------------\n",
        "def run_step8(df_preprocessed: pd.DataFrame,\n",
        "              raw_col=\"Raw text\",\n",
        "              id_col=\"ID\",\n",
        "              client=None,\n",
        "              model=MODEL_ID,\n",
        "              use_mock=USE_MOCK):\n",
        "    # 8A: correction + tags\n",
        "    df_corr = run_correct_only(\n",
        "        df_preprocessed,\n",
        "        text_col=raw_col,\n",
        "        id_col=id_col,\n",
        "        client=None if use_mock else client,\n",
        "        model=model,\n",
        "        use_mock=use_mock,\n",
        "        out_col=\"Corrected text (8)\"\n",
        "    )\n",
        "    # extra safety: normalize mojibake\n",
        "    df_corr = apply_mojibake_normalization(df_corr, corrected_col=\"Corrected text (8)\")\n",
        "\n",
        "    # 8B: token map\n",
        "    df_map, df_texts = run_mapping_only(\n",
        "        df_corr, id_col=id_col, raw_col=raw_col, corr_col=\"Corrected text (8)\"\n",
        "    )\n",
        "\n",
        "    # Repair alignment (YAAAY→Yay, lest→Let)\n",
        "    df_map = postprocess_pair_deletions_and_insertions(df_map, max_window=6, min_sim=0.55)\n",
        "\n",
        "    # 8C: sentence IDs\n",
        "    df_map = assign_corr_sentence_ids(df_map)\n",
        "\n",
        "    # 8D: title marking using exact spans + renumbering rule for s000\n",
        "    df_texts[\"ID\"] = df_texts[\"ID\"].astype(str)\n",
        "    df_map = mark_title_tokens(df_map, df_texts_with_tags=df_texts)\n",
        "\n",
        "    # 8E: boundaries with case-pattern correctness\n",
        "    df_map = add_sentence_boundary_flags(df_map)\n",
        "\n",
        "    return df_texts, df_map\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Step 9: Sentence table (exclude titles) + project dialogue/temporal/closure\n",
        "# -------------------------\n",
        "def _overlap(a0, a1, b0, b1):\n",
        "    return max(0, min(a1, b0 if b0>b1 else b1) - max(a0, b0)) > 0 if a1>=a0 and b1>=b0 else False\n",
        "\n",
        "def run_step9(df_map: pd.DataFrame, df_texts_with_tags: pd.DataFrame) -> pd.DataFrame:\n",
        "    need = {\"ID\",\"CorrSentenceID\",\"corr_token\",\"Sentence Boundaries\",\"BoundaryCheck\",\"SentenceRef\",\"TITLE\",\"corr_start\",\"corr_end\"}\n",
        "    miss = need - set(df_map.columns)\n",
        "    if miss:\n",
        "        raise KeyError(f\"df_map missing columns needed for Step 9: {miss}\")\n",
        "\n",
        "    # sort stable\n",
        "    sort_cols = [\"ID\",\"CorrSentenceID\"]\n",
        "    if \"corr_index\" in df_map.columns: sort_cols.append(\"corr_index\")\n",
        "    wm = df_map.sort_values(sort_cols, kind=\"mergesort\").copy()\n",
        "\n",
        "    # Exclude titles from aggregation\n",
        "    core = wm[~wm[\"TITLE\"].astype(bool)].copy()\n",
        "\n",
        "    # build sentence spans (min/max corr char)\n",
        "    sent_spans = (\n",
        "        core.groupby([\"ID\",\"CorrSentenceID\"], as_index=False, sort=False)\n",
        "            .agg(CorrStartMin=(\"corr_start\",\"min\"),\n",
        "                 CorrEndMax=(\"corr_end\",\"max\"))\n",
        "    )\n",
        "\n",
        "    # sentence text and metrics\n",
        "    def _summarize_sentence(g: pd.DataFrame) -> pd.Series:\n",
        "        corr_tokens = g[\"corr_token\"].tolist()\n",
        "        raw_tokens  = [x for x in g[\"raw_token\"].tolist() if not pd.isna(x)] if \"raw_token\" in g.columns else []\n",
        "        corr_text   = _detok(corr_tokens)\n",
        "        raw_text    = _detok(raw_tokens) if raw_tokens else \"\"\n",
        "\n",
        "        b_rows = g[g[\"Sentence Boundaries\"].str.contains(\"Sentence Beginning\", na=False)]\n",
        "        e_rows = g[g[\"Sentence Boundaries\"].str.contains(\"Sentence Ending\",   na=False)]\n",
        "\n",
        "        begin_ok = np.nan\n",
        "        end_ok   = np.nan\n",
        "        if not b_rows.empty:\n",
        "            chk = \" | \".join(b_rows[\"BoundaryCheck\"].dropna().astype(str))\n",
        "            begin_ok = 1 if \"Correct Beginning\" in chk else (0 if \"Incorrect Beginning\" in chk else np.nan)\n",
        "        if not e_rows.empty:\n",
        "            chk = \" | \".join(e_rows[\"BoundaryCheck\"].dropna().astype(str))\n",
        "            end_ok = 1 if \"Correct Ending\" in chk else (0 if \"Incorrect Ending\" in chk else np.nan)\n",
        "\n",
        "        ops = g[\"op\"] if \"op\" in g.columns else pd.Series([], dtype=object)\n",
        "        return pd.Series({\n",
        "            \"SentenceRef\": g[\"SentenceRef\"].iloc[0],\n",
        "            \"CorrectedSentence\": corr_text,\n",
        "            \"RawSentence\": raw_text,\n",
        "            \"TokensInSentence\": int(len(g)),\n",
        "            \"EditsInSentence\": int((ops != \"equal\").sum()) if not ops.empty else np.nan,\n",
        "            \"EqualsInSentence\": int((ops == \"equal\").sum()) if not ops.empty else np.nan,\n",
        "            \"Insertions\": int((ops == \"insert\").sum()) if not ops.empty else np.nan,\n",
        "            \"Deletions\": int((ops == \"delete\").sum()) if not ops.empty else np.nan,\n",
        "            \"Replacements\": int((ops == \"replace\").sum()) if not ops.empty else np.nan,\n",
        "            \"BeginBoundaryRow\": (b_rows.index[0] if not b_rows.empty else np.nan),\n",
        "            \"EndBoundaryRow\":   (e_rows.index[0] if not e_rows.empty else np.nan),\n",
        "            \"CorrectBeginning\": begin_ok,\n",
        "            \"CorrectEnding\":    end_ok,\n",
        "        })\n",
        "\n",
        "    sent_df = (\n",
        "        core.groupby([\"ID\",\"CorrSentenceID\"], as_index=False, sort=False)\n",
        "            .apply(_summarize_sentence, include_groups=False)\n",
        "            .reset_index(drop=True)\n",
        "            .sort_values([\"ID\",\"SentenceRef\"], kind=\"mergesort\")\n",
        "    )\n",
        "\n",
        "    # Project dialogue + temporal + closure flags from df_texts_with_tags (per ID)\n",
        "    # We'll mark a sentence as dialogue if any dialogue span overlaps its [CorrStartMin, CorrEndMax)\n",
        "    df_texts = df_texts_with_tags[[\"ID\",\"Corrected text (8)\",\"NarrativeTagsJSON\",\"DialogueSpansJSON\"]].copy()\n",
        "    df_texts[\"ID\"] = df_texts[\"ID\"].astype(str)\n",
        "\n",
        "    sent_df = sent_df.merge(sent_spans, on=[\"ID\",\"CorrSentenceID\"], how=\"left\")\n",
        "    sent_df = sent_df.merge(df_texts, on=\"ID\", how=\"left\")\n",
        "\n",
        "    def _flags(row):\n",
        "        # dialogue\n",
        "        dlg = _parse_json_list(row.get(\"DialogueSpansJSON\",\"[]\"))\n",
        "        s0, s1 = row[\"CorrStartMin\"], row[\"CorrEndMax\"]\n",
        "        has_dialogue = False\n",
        "        for d in dlg:\n",
        "            try:\n",
        "                ds, de = int(d.get(\"start\", -1)), int(d.get(\"end\", -1))\n",
        "                if ds >= 0 and de >= 0 and _overlap(s0, s1, ds, de):\n",
        "                    has_dialogue = True; break\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # temporal + closure\n",
        "        tags = _parse_json_list(row.get(\"NarrativeTagsJSON\",\"[]\"))\n",
        "        has_temporal = False\n",
        "        has_closure  = False\n",
        "        for t in tags:\n",
        "            try:\n",
        "                if t.get(\"type\") == \"temporal\":\n",
        "                    ts, te = int(t.get(\"start\",-1)), int(t.get(\"end\",-1))\n",
        "                    if ts >= 0 and te >= 0 and _overlap(s0, s1, ts, te):\n",
        "                        has_temporal = True\n",
        "                elif t.get(\"type\") == \"closure\":\n",
        "                    ts, te = int(t.get(\"start\",-1)), int(t.get(\"end\",-1))\n",
        "                    if ts >= 0 and te >= 0 and _overlap(s0, s1, ts, te):\n",
        "                        has_closure = True\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        return pd.Series({\n",
        "            \"IsDialogue\": bool(has_dialogue),\n",
        "            \"HasTemporal\": bool(has_temporal),\n",
        "            \"HasClosure\": bool(has_closure),\n",
        "        })\n",
        "\n",
        "    sent_df = pd.concat([sent_df, sent_df.apply(_flags, axis=1)], axis=1)\n",
        "\n",
        "    # clean columns\n",
        "    drop_cols = [\"CorrStartMin\",\"CorrEndMax\",\"Corrected text (8)\",\"NarrativeTagsJSON\",\"DialogueSpansJSON\"]\n",
        "    for c in drop_cols:\n",
        "        if c in sent_df.columns:\n",
        "            sent_df.drop(columns=[c], inplace=True)\n",
        "\n",
        "    return sent_df\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Save + Download wrapper\n",
        "# -------------------------\n",
        "def _ensure_dir(path):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    return path\n",
        "\n",
        "def save_and_download_step8_9(\n",
        "    df_preprocessed: pd.DataFrame,\n",
        "    *,\n",
        "    raw_col=\"Raw text\",\n",
        "    id_col=\"ID\",\n",
        "    client=None,              # from Step 6\n",
        "    model=\"gpt-4o\",\n",
        "    use_mock=False,           # True for offline test\n",
        "    out_dir=\"/content\"\n",
        "):\n",
        "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    _ensure_dir(out_dir)\n",
        "\n",
        "    # Step 8\n",
        "    df_texts_8, df_map_8 = run_step8(\n",
        "        df_preprocessed,\n",
        "        raw_col=raw_col,\n",
        "        id_col=id_col,\n",
        "        client=client if not use_mock else None,\n",
        "        model=model,\n",
        "        use_mock=use_mock\n",
        "    )\n",
        "\n",
        "    # Step 9\n",
        "    sent_df = run_step9(df_map_8, df_texts_with_tags=df_texts_8)\n",
        "\n",
        "    # Paths\n",
        "    p_texts = os.path.join(out_dir, f\"step8_texts_{ts}.csv\")\n",
        "    p_map   = os.path.join(out_dir, f\"step8_wordmap_checked_{ts}.csv\")\n",
        "    p_sent  = os.path.join(out_dir, f\"step9_sentence_mapping_with_boundaries_{ts}.csv\")\n",
        "    p_zip   = os.path.join(out_dir, f\"step8_9_outputs_{ts}.zip\")\n",
        "\n",
        "    # Save\n",
        "    df_texts_8.to_csv(p_texts, index=False, encoding=\"utf-8\")\n",
        "    df_map_8.to_csv(p_map,   index=False, encoding=\"utf-8\")\n",
        "    sent_df.to_csv(p_sent,   index=False, encoding=\"utf-8\")\n",
        "\n",
        "    # Zip bundle\n",
        "    with zipfile.ZipFile(p_zip, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "        zf.write(p_texts, arcname=os.path.basename(p_texts))\n",
        "        zf.write(p_map,   arcname=os.path.basename(p_map))\n",
        "        zf.write(p_sent,  arcname=os.path.basename(p_sent))\n",
        "\n",
        "    print(\"Saved:\")\n",
        "    print(\"  \", p_texts)\n",
        "    print(\"  \", p_map)\n",
        "    print(\"  \", p_sent)\n",
        "    print(\"  \", p_zip)\n",
        "\n",
        "    if _COLAB:\n",
        "        try:\n",
        "            files.download(p_texts)\n",
        "            files.download(p_map)\n",
        "            files.download(p_sent)\n",
        "            files.download(p_zip)\n",
        "        except Exception as e:\n",
        "            print(\"Download hint:\", e)\n",
        "\n",
        "    return dict(\n",
        "        step8_texts_path=p_texts,\n",
        "        step8_map_path=p_map,\n",
        "        step9_sentences_path=p_sent,\n",
        "        zip_path=p_zip,\n",
        "        df_texts_8=df_texts_8,\n",
        "        df_map_8=df_map_8,\n",
        "        sent_df=sent_df\n",
        "    )\n"
      ],
      "metadata": {
        "id": "75eTgnfFLMYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# STEP 8 & 9 (Final build): correction → mapping → IDs\n",
        "# Uses LLM for corrected text + narrative tags + dialogue,\n",
        "# robust alignment, title exact-span marking, better beginnings,\n",
        "# ellipsis + quotes, and clean downloads.\n",
        "# ===============================================\n",
        "\n",
        "import os, re, io, json, time, math, zipfile, logging, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# -----------------------\n",
        "# Config\n",
        "# -----------------------\n",
        "MODEL_ID   = \"gpt-4o\"\n",
        "MAX_TOKENS = 1500\n",
        "USE_MOCK   = False     # True = offline deterministic correction (no API)\n",
        "OUT_DIR    = \"/content\"\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
        "\n",
        "# Colab download helper\n",
        "try:\n",
        "    from google.colab import files\n",
        "    _COLAB = True\n",
        "except Exception:\n",
        "    _COLAB = False\n",
        "\n",
        "# -----------------------\n",
        "# Mojibake normalization\n",
        "# -----------------------\n",
        "_MOJIBAKE_FIXES = [\n",
        "    (r\"â€”\", \"—\"), (r\"â€“\", \"–\"),\n",
        "    (r\"â€˜\", \"‘\"), (r\"â€™\", \"’\"),\n",
        "    (r\"â€œ\", \"“\"), (r\"â€\", \"”\"),\n",
        "    (r\"â€¦\", \"…\"),\n",
        "    (r\"Â \", \" \"),\n",
        "]\n",
        "\n",
        "def normalize_mojibake(s: str) -> str:\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "    out = str(s)\n",
        "    for pat, repl in _MOJIBAKE_FIXES:\n",
        "        # support string or compiled pattern\n",
        "        if hasattr(pat, \"sub\"):\n",
        "            out = pat.sub(repl, out)\n",
        "        else:\n",
        "            out = re.sub(pat, repl, out)\n",
        "    return out\n",
        "\n",
        "# -----------------------\n",
        "# Utility: extract first JSON object\n",
        "# -----------------------\n",
        "def _extract_first_json_object(txt: str):\n",
        "    if not txt:\n",
        "        return None\n",
        "    start = txt.find(\"{\")\n",
        "    if start < 0:\n",
        "        return None\n",
        "    depth, in_str, esc = 0, False, False\n",
        "    for i in range(start, len(txt)):\n",
        "        ch = txt[i]\n",
        "        if in_str:\n",
        "            if esc: esc = False\n",
        "            elif ch == \"\\\\\": esc = True\n",
        "            elif ch == '\"': in_str = False\n",
        "        else:\n",
        "            if ch == '\"': in_str = True\n",
        "            elif ch == \"{\": depth += 1\n",
        "            elif ch == \"}\":\n",
        "                depth -= 1\n",
        "                if depth == 0:\n",
        "                    frag = txt[start:i+1]\n",
        "                    try:\n",
        "                        return json.loads(frag)\n",
        "                    except Exception:\n",
        "                        return None\n",
        "    return None\n",
        "\n",
        "# -----------------------\n",
        "# 8A. LLM correction + tags\n",
        "# -----------------------\n",
        "def correct_with_tags(raw: str, client=None, model=MODEL_ID, use_mock=USE_MOCK):\n",
        "    \"\"\"Return corrected_text, narrative_tags(list), dialogue_spans(list of {start,end}), source.\"\"\"\n",
        "    s = normalize_mojibake(str(raw or \"\"))\n",
        "\n",
        "    if use_mock or client is None:\n",
        "        # Deterministic light-touch mock: capitalize first alpha, ensure end dot\n",
        "        t = s.strip()\n",
        "        m = re.search(r\"[A-Za-z]\", t)\n",
        "        if m:\n",
        "            i = m.start()\n",
        "            t = t[:i] + t[i].upper() + t[i+1:]\n",
        "        if t and not re.search(r\"[.!?…]\\s*$\", t):\n",
        "            t += \".\"\n",
        "        return t, [], [], \"mock\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a meticulous copy-editor. Fix punctuation, grammar, and spelling.\n",
        "Keep meaning and paragraphing. Use standard English punctuation.\n",
        "\n",
        "Also identify narrative tags and dialogue:\n",
        "\n",
        "- NarrativeTags:\n",
        "  * Titles at the very start (one line/phrase): type=\"title\"\n",
        "  * Temporal transitions (e.g., \"The next day\", \"2 weeks later\", dates like \"March 19th, 3202\"): type=\"temporal\"\n",
        "  * Closures (e.g., \"THE END\", \"To be continued\", \"***\" as a scene break): type=\"closure\"\n",
        "  Each item: {{\"type\":\"title|temporal|closure\",\"text\":\"...\",\"start\":<char_index>,\"end\":<char_index>}}\n",
        "\n",
        "- DialogueSpans:\n",
        "  Contiguous regions of direct quoted speech (opening to closing quotes, include quotes).\n",
        "  Return as array of objects: {{\"start\":<char_index>,\"end\":<char_index>}}\n",
        "\n",
        "Return ONLY JSON:\n",
        "{{\n",
        "  \"corrected_text\": \"...\",\n",
        "  \"narrative_tags\": [ ... ],\n",
        "  \"dialogue_spans\": [ ... ]\n",
        "}}\n",
        "\n",
        "Text:\n",
        "<<<BEGIN>>>\n",
        "{s}\n",
        "<<<END>>>\n",
        "\"\"\".strip()\n",
        "\n",
        "    try:\n",
        "        from openai import OpenAI\n",
        "        _client = client or OpenAI()\n",
        "        resp = _client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "            temperature=0.0,\n",
        "            max_tokens=MAX_TOKENS\n",
        "        )\n",
        "        out = (resp.choices[0].message.content or \"\").strip()\n",
        "        js = _extract_first_json_object(out)\n",
        "        if not isinstance(js, dict):\n",
        "            return s, [], [], \"fallback\"\n",
        "        corrected = normalize_mojibake(js.get(\"corrected_text\",\"\") or \"\").strip()\n",
        "        tags = js.get(\"narrative_tags\") or []\n",
        "        spans = js.get(\"dialogue_spans\") or []\n",
        "        # sanitize spans\n",
        "        def _clip(a,b,n):\n",
        "            a = max(0, int(a)); b = max(a, int(b));\n",
        "            if n is not None: b = min(b, n); a = min(a, b)\n",
        "            return a,b\n",
        "        N = len(corrected)\n",
        "        clean_tags = []\n",
        "        for t in tags:\n",
        "            try:\n",
        "                tt = str(t.get(\"type\",\"\")).lower()\n",
        "                st, en = _clip(t.get(\"start\",0), t.get(\"end\",0), N)\n",
        "                txt = corrected[st:en]\n",
        "                if tt in {\"title\",\"temporal\",\"closure\"} and en>st:\n",
        "                    clean_tags.append({\"type\":tt,\"text\":txt,\"start\":st,\"end\":en})\n",
        "            except Exception:\n",
        "                pass\n",
        "        clean_spans = []\n",
        "        for d in spans:\n",
        "            try:\n",
        "                st, en = _clip(d.get(\"start\",0), d.get(\"end\",0), N)\n",
        "                if en>st:\n",
        "                    clean_spans.append({\"start\":st,\"end\":en})\n",
        "            except Exception:\n",
        "                pass\n",
        "        return corrected, clean_tags, clean_spans, model\n",
        "    except Exception as e:\n",
        "        logger.warning(\"LLM correction failed, using mojibake-normalized text. %s\", e)\n",
        "        return s, [], [], \"error\"\n",
        "\n",
        "def run_correct_only(\n",
        "    df_in: pd.DataFrame,\n",
        "    text_col=\"Raw text\",\n",
        "    id_col=\"ID\",\n",
        "    client=None,\n",
        "    model=MODEL_ID,\n",
        "    use_mock=USE_MOCK,\n",
        "    out_col=\"Corrected text (8)\"\n",
        ") -> pd.DataFrame:\n",
        "    if text_col not in df_in.columns:\n",
        "        raise KeyError(f\"Missing required column: {text_col}\")\n",
        "    df = df_in.copy()\n",
        "\n",
        "    # Normalize ID strings (avoid 123.0 drift)\n",
        "    def _norm_id_series(s: pd.Series) -> pd.Series:\n",
        "        s = s.astype(str).str.replace(r\"\\.0$\", \"\", regex=True)\n",
        "        def _fix(x):\n",
        "            if any(c.isalpha() for c in x):  # alphanumeric → keep\n",
        "                return x\n",
        "            try:\n",
        "                if \".\" in x or \"e\" in x.lower():\n",
        "                    f = float(x)\n",
        "                    if f.is_integer():\n",
        "                        return str(int(f))\n",
        "            except Exception:\n",
        "                pass\n",
        "            return x\n",
        "        return s.map(_fix)\n",
        "\n",
        "    if id_col not in df.columns:\n",
        "        df[id_col] = pd.RangeIndex(len(df)).astype(str)\n",
        "    else:\n",
        "        df[id_col] = _norm_id_series(df[id_col])\n",
        "\n",
        "    corrected, tags_json, dlg_json, sources = [], [], [], []\n",
        "    for raw in df[text_col].astype(str).tolist():\n",
        "        c, tags, spans, src = correct_with_tags(raw, client=client, model=model, use_mock=use_mock)\n",
        "        corrected.append(c)\n",
        "        tags_json.append(json.dumps(tags, ensure_ascii=False))\n",
        "        dlg_json.append(json.dumps(spans, ensure_ascii=False))\n",
        "        sources.append(src)\n",
        "\n",
        "    df[out_col] = corrected\n",
        "    df[\"NarrativeTagsJSON\"] = tags_json\n",
        "    df[\"DialogueSpansJSON\"] = dlg_json\n",
        "    df[\"CorrectedBy\"] = sources\n",
        "    return df\n",
        "\n",
        "# -----------------------\n",
        "# 8B. Tokenize + map (with merged-word split + canonical diff)\n",
        "# -----------------------\n",
        "_WORD_RX = re.compile(r\"\\w\", flags=re.UNICODE)\n",
        "\n",
        "def _split_merged_word(tok: str):\n",
        "    # Split once when ALLCAPS followed by lowercase: YAAYwe -> YAAY + we\n",
        "    if not tok or not tok.isalpha():\n",
        "        return [tok]\n",
        "    m = re.match(r\"^([A-Z]{2,})([a-z].*)$\", tok)\n",
        "    if m:\n",
        "        left, right = m.group(1), m.group(2)\n",
        "        return [left, right]\n",
        "    return [tok]\n",
        "\n",
        "def _tokenize_with_split(s: str):\n",
        "    base = re.findall(r\"\\w+|[^\\w\\s]\", s or \"\", flags=re.UNICODE)\n",
        "    out = []\n",
        "    for t in base:\n",
        "        if re.fullmatch(r\"\\w+\", t):\n",
        "            out.extend(_split_merged_word(t))\n",
        "        else:\n",
        "            out.append(t)\n",
        "    return out\n",
        "\n",
        "def _rebuild_offsets_with_splitting(text, tokens):\n",
        "    spans = []\n",
        "    i = 0\n",
        "    n = len(text)\n",
        "    for tok in tokens:\n",
        "        if tok == \"\" or tok is None:\n",
        "            spans.append((i, i)); continue\n",
        "        pos = text.find(tok, i)\n",
        "        if pos >= 0:\n",
        "            start, end = pos, pos + len(tok)\n",
        "            spans.append((start, end))\n",
        "            i = end\n",
        "        else:\n",
        "            # best-effort slice (keeps map coherent if we introduced a split)\n",
        "            j = i\n",
        "            while j < n and text[j].isspace():\n",
        "                j += 1\n",
        "            start = j\n",
        "            end = min(n, start + len(tok))\n",
        "            spans.append((start, end))\n",
        "            i = end\n",
        "    return spans\n",
        "\n",
        "def _is_word(tok: str) -> bool:\n",
        "    return bool(tok) and bool(_WORD_RX.search(tok))\n",
        "\n",
        "def _canon(tok: str) -> str:\n",
        "    \"\"\"Canonical form for diff: uppercase + collapse repeated letters (YAAY→YAY).\"\"\"\n",
        "    if tok is None:\n",
        "        return \"\"\n",
        "    u = str(tok).upper()\n",
        "    return re.sub(r\"(.)\\1+\", r\"\\1\", u)\n",
        "\n",
        "def build_word_map(raw_text, corr_text):\n",
        "    raw_text  = str(raw_text or \"\")\n",
        "    corr_text = str(corr_text or \"\")\n",
        "\n",
        "    raw_tokens  = _tokenize_with_split(raw_text)\n",
        "    corr_tokens = _tokenize_with_split(corr_text)\n",
        "\n",
        "    raw_spans  = _rebuild_offsets_with_splitting(raw_text, raw_tokens)\n",
        "    corr_spans = _rebuild_offsets_with_splitting(corr_text, corr_tokens)\n",
        "\n",
        "    # Diff on canonical tokens to improve alignment (YAAY↔Yay, etc.)\n",
        "    sm = SequenceMatcher(a=[_canon(t) for t in raw_tokens],\n",
        "                         b=[_canon(t) for t in corr_tokens],\n",
        "                         autojunk=False)\n",
        "\n",
        "    rows = []\n",
        "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
        "        if tag == \"equal\":\n",
        "            for k in range(i2 - i1):\n",
        "                ri = i1 + k; ci = j1 + k\n",
        "                r_tok, c_tok = raw_tokens[ri], corr_tokens[ci]\n",
        "                r_start, r_end = raw_spans[ri]\n",
        "                c_start, c_end = corr_spans[ci]\n",
        "                rows.append({\n",
        "                    \"raw_index\": ri, \"raw_token\": r_tok, \"raw_start\": r_start, \"raw_end\": r_end,\n",
        "                    \"corr_index\": ci, \"corr_token\": c_tok, \"corr_start\": c_start, \"corr_end\": c_end,\n",
        "                    \"op\": \"equal\", \"equal_ci\": (r_tok == c_tok), \"error_type\": \"Equal\"\n",
        "                })\n",
        "\n",
        "        elif tag == \"replace\":\n",
        "            m = min(i2 - i1, j2 - j1)\n",
        "            for k in range(m):\n",
        "                ri = i1 + k; ci = j1 + k\n",
        "                r_tok, c_tok = raw_tokens[ri], corr_tokens[ci]\n",
        "                r_start, r_end = raw_spans[ri]\n",
        "                c_start, c_end = corr_spans[ci]\n",
        "                # Spelling vs Replacement\n",
        "                err = \"Spelling\" if (str(r_tok).isalpha() and str(c_tok).isalpha() and _canon(r_tok) == _canon(c_tok)) else \"Replacement\"\n",
        "                rows.append({\n",
        "                    \"raw_index\": ri, \"raw_token\": r_tok, \"raw_start\": r_start, \"raw_end\": r_end,\n",
        "                    \"corr_index\": ci, \"corr_token\": c_tok, \"corr_start\": c_start, \"corr_end\": c_end,\n",
        "                    \"op\": \"replace\", \"equal_ci\": (_canon(r_tok) == _canon(c_tok)), \"error_type\": err\n",
        "                })\n",
        "            for ri in range(i1 + m, i2):  # deletions\n",
        "                r_tok = raw_tokens[ri]; r_start, r_end = raw_spans[ri]\n",
        "                rows.append({\n",
        "                    \"raw_index\": ri, \"raw_token\": r_tok, \"raw_start\": r_start, \"raw_end\": r_end,\n",
        "                    \"corr_index\": None, \"corr_token\": None, \"corr_start\": None, \"corr_end\": None,\n",
        "                    \"op\": \"delete\", \"equal_ci\": False,\n",
        "                    \"error_type\": \"PunctuationDeletion\" if not _is_word(r_tok) else \"Deletion\"\n",
        "                })\n",
        "            for ci in range(j1 + m, j2):  # insertions\n",
        "                c_tok = corr_tokens[ci]; c_start, c_end = corr_spans[ci]\n",
        "                rows.append({\n",
        "                    \"raw_index\": None, \"raw_token\": None, \"raw_start\": None, \"raw_end\": None,\n",
        "                    \"corr_index\": ci, \"corr_token\": c_tok, \"corr_start\": c_start, \"corr_end\": c_end,\n",
        "                    \"op\": \"insert\", \"equal_ci\": False,\n",
        "                    \"error_type\": \"PunctuationInsertion\" if not _is_word(c_tok) else \"Insertion\"\n",
        "                })\n",
        "\n",
        "        elif tag == \"delete\":\n",
        "            for ri in range(i1, i2):\n",
        "                r_tok = raw_tokens[ri]; r_start, r_end = raw_spans[ri]\n",
        "                rows.append({\n",
        "                    \"raw_index\": ri, \"raw_token\": r_tok, \"raw_start\": r_start, \"raw_end\": r_end,\n",
        "                    \"corr_index\": None, \"corr_token\": None, \"corr_start\": None, \"corr_end\": None,\n",
        "                    \"op\": \"delete\", \"equal_ci\": False,\n",
        "                    \"error_type\": \"PunctuationDeletion\" if not _is_word(r_tok) else \"Deletion\"\n",
        "                })\n",
        "\n",
        "        elif tag == \"insert\":\n",
        "            for ci in range(j1, j2):\n",
        "                c_tok = corr_tokens[ci]; c_start, c_end = corr_spans[ci]\n",
        "                rows.append({\n",
        "                    \"raw_index\": None, \"raw_token\": None, \"raw_start\": None, \"raw_end\": None,\n",
        "                    \"corr_index\": ci, \"corr_token\": c_tok, \"corr_start\": c_start, \"corr_end\": c_end,\n",
        "                    \"op\": \"insert\", \"equal_ci\": False,\n",
        "                    \"error_type\": \"PunctuationInsertion\" if not _is_word(c_tok) else \"Insertion\"\n",
        "                })\n",
        "    return rows\n",
        "\n",
        "def run_mapping_only(df_with_corr, id_col=\"ID\",\n",
        "                     raw_col=\"Raw text\", corr_col=\"Corrected text (8)\"):\n",
        "    need = {raw_col, corr_col}\n",
        "    if not need.issubset(df_with_corr.columns):\n",
        "        raise KeyError(f\"Missing columns: {need - set(df_with_corr.columns)}\")\n",
        "    df = df_with_corr.copy()\n",
        "    if id_col not in df.columns:\n",
        "        df[id_col] = pd.RangeIndex(len(df)).astype(str)\n",
        "    df[id_col] = df[id_col].astype(str)\n",
        "\n",
        "    all_rows = []\n",
        "    for order, (rid, raw, cor) in enumerate(zip(\n",
        "        df[id_col].tolist(),\n",
        "        df[raw_col].astype(str).tolist(),\n",
        "        df[corr_col].astype(str).tolist()\n",
        "    )):\n",
        "        rows = build_word_map(raw, cor)\n",
        "        if not rows:\n",
        "            rows = [{\n",
        "                \"raw_index\": np.nan, \"raw_token\": None, \"raw_start\": np.nan, \"raw_end\": np.nan,\n",
        "                \"corr_index\": np.nan, \"corr_token\": None, \"corr_start\": np.nan, \"corr_end\": np.nan,\n",
        "                \"op\": \"empty\", \"equal_ci\": False, \"error_type\": \"EmptyText\"\n",
        "            }]\n",
        "        for r in rows:\n",
        "            rec = {\"RowID\": rid, \"DocOrder\": order, **r}\n",
        "            rec[\"Changed\"] = (r.get(\"op\") != \"equal\")\n",
        "            all_rows.append(rec)\n",
        "    map_df = pd.DataFrame(all_rows)\n",
        "    texts_out = df.copy()\n",
        "    return map_df, texts_out\n",
        "\n",
        "# -----------------------\n",
        "# 8C. Sentence IDs (ellipsis + quote aware)\n",
        "# -----------------------\n",
        "ABBREV = {\n",
        "    \"mr.\",\"mrs.\",\"ms.\",\"dr.\",\"prof.\",\"sr.\",\"jr.\",\"st.\",\"vs.\",\"etc.\",\n",
        "    \"e.g.\",\"i.e.\",\"cf.\",\"fig.\",\"ex.\",\"no.\",\"approx.\",\"circa.\",\"ca.\",\n",
        "    \"dept.\",\"est.\",\"misc.\",\"rev.\",\"jan.\",\"feb.\",\"mar.\",\"apr.\",\"jun.\",\n",
        "    \"jul.\",\"aug.\",\"sep.\",\"sept.\",\"oct.\",\"nov.\",\"dec.\"\n",
        "}\n",
        "TERMINALS = {\".\", \"!\", \"?\", \"…\", \"...\", \"?!\", \"!?\"}\n",
        "CLOSERS   = {\")\", \"]\", \"}\", \"”\", \"’\", \"»\"}\n",
        "OPENERS   = {\"(\", \"[\", \"{\", \"“\", \"‘\", \"«\"}\n",
        "RE_INITIAL       = re.compile(r\"^[A-Z]\\.$\")\n",
        "RE_INITIAL_PAIR  = re.compile(r\"^[A-Z]\\.[A-Z]\\.$\")\n",
        "RE_NUM_WITH_DOT  = re.compile(r\"^\\d+\\.$\")\n",
        "RE_SECTION_NUM   = re.compile(r\"^\\d+(?:\\.\\d+){1,3}$\")\n",
        "RE_DOT_TAIL      = re.compile(r\"^\\.\\d+$\")\n",
        "RE_ELLIPSIS      = re.compile(r\"^\\.\\.\\.$\")\n",
        "RE_ALPHA_PAREN   = re.compile(r\"^[A-Za-z]\\)$\")\n",
        "\n",
        "def _tok(x):\n",
        "    if pd.isna(x) or x is None: return \"\"\n",
        "    return str(x)\n",
        "\n",
        "def _is_ellipsis_triplet(i, toks):\n",
        "    return (i+2 < len(toks) and toks[i] == \".\" and toks[i+1] == \".\" and toks[i+2] == \".\")\n",
        "\n",
        "def _is_terminal_token(tok: str, prev_tok: str, next_tok: str) -> bool:\n",
        "    t = tok.strip()\n",
        "    if not t:\n",
        "        return False\n",
        "    if t in {\"…\",\"...\"} or RE_ELLIPSIS.fullmatch(t):\n",
        "        return True\n",
        "    if t in {\"?!\",\"!?\"}:\n",
        "        return True\n",
        "    if t in {\"!\",\"?\"}:\n",
        "        return True\n",
        "    if t == \".\":\n",
        "        p = (prev_tok or \"\").strip()\n",
        "        n = (next_tok or \"\").strip()\n",
        "        low_prev = p.lower()\n",
        "        if low_prev in ABBREV: return False\n",
        "        if RE_INITIAL.fullmatch(p) or RE_INITIAL_PAIR.fullmatch(p): return False\n",
        "        if RE_SECTION_NUM.fullmatch(p): return False\n",
        "        if RE_NUM_WITH_DOT.fullmatch(p) and (n and re.match(r\"[A-Za-z(“\\\"'\\[]\", n)): return False\n",
        "        if RE_DOT_TAIL.fullmatch(n): return False\n",
        "        if n.isdigit(): return False\n",
        "        return True\n",
        "    if t == \")\" and RE_ALPHA_PAREN.fullmatch(prev_tok):\n",
        "        return False\n",
        "    return False\n",
        "\n",
        "def _likely_ascii_opening(prev_tok: str, next_tok: str) -> bool:\n",
        "    prev = (prev_tok or \"\").strip()\n",
        "    nxt  = (next_tok or \"\").strip()\n",
        "    if prev == \"\" or prev in TERMINALS or prev in OPENERS:\n",
        "        return True\n",
        "    if nxt and nxt not in TERMINALS and nxt not in CLOSERS:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def assign_corr_sentence_ids(df_map: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df_map.copy()\n",
        "    if \"RowID\" in df.columns:\n",
        "        df[\"ID\"] = df[\"RowID\"].astype(str)\n",
        "    elif \"ID\" not in df.columns:\n",
        "        df[\"ID\"] = df.index.astype(str)\n",
        "\n",
        "    has_ci = \"corr_index\" in df.columns\n",
        "    if has_ci and \"corr_index_orig\" not in df.columns:\n",
        "        df[\"corr_index_orig\"] = df[\"corr_index\"]\n",
        "\n",
        "    def _stable_sort_key(g: pd.DataFrame) -> pd.Series:\n",
        "        pos = pd.Series(np.arange(len(g)), index=g.index, dtype=float)\n",
        "        if has_ci:\n",
        "            ci = pd.to_numeric(g[\"corr_index\"], errors=\"coerce\")\n",
        "            nan_mask = ci.isna()\n",
        "            bump = (pos - pos.min()) / max((pos.max() - pos.min()), 1) * 1e-6\n",
        "            return ci.where(~nan_mask, 1e9) + bump\n",
        "        return pos\n",
        "\n",
        "    df[\"_sort_key\"] = df.groupby(\"ID\", group_keys=False).apply(_stable_sort_key)\n",
        "\n",
        "    def _assign(g: pd.DataFrame) -> pd.Series:\n",
        "        g = g.sort_values(\"_sort_key\", kind=\"mergesort\")\n",
        "        toks = (g[\"corr_token\"] if \"corr_token\" in g.columns else g[\"raw_token\"]).map(_tok).tolist()\n",
        "\n",
        "        sids = []\n",
        "        sent_id = 0\n",
        "        pending_end = False\n",
        "        i = 0\n",
        "        while i < len(toks):\n",
        "            tok = toks[i].strip()\n",
        "            prev_tok = toks[i-1].strip() if i > 0 else \"\"\n",
        "            next_tok = toks[i+1].strip() if i+1 < len(toks) else \"\"\n",
        "\n",
        "            if _is_ellipsis_triplet(i, toks):\n",
        "                pending_end = True\n",
        "                sids.append(sent_id)\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            if pending_end:\n",
        "                if tok in CLOSERS or (tok == '\"' and not _likely_ascii_opening(prev_tok, next_tok)):\n",
        "                    sids.append(sent_id); i += 1; continue\n",
        "                if tok in OPENERS or (tok == '\"' and _likely_ascii_opening(prev_tok, next_tok)):\n",
        "                    sent_id += 1; pending_end = False; sids.append(sent_id); i += 1; continue\n",
        "                sent_id += 1; pending_end = False; sids.append(sent_id); i += 1; continue\n",
        "            else:\n",
        "                sids.append(sent_id); i += 1\n",
        "\n",
        "            if _is_terminal_token(tok, prev_tok, next_tok):\n",
        "                pending_end = True\n",
        "\n",
        "        return pd.Series(sids, index=g.index).reindex(g.index)\n",
        "\n",
        "    df[\"CorrSentenceID\"] = (\n",
        "        df.groupby(\"ID\", group_keys=False)\n",
        "          .apply(_assign)\n",
        "          .astype(\"Int64\")\n",
        "    )\n",
        "    df.drop(columns=[\"_sort_key\"], inplace=True, errors=\"ignore\")\n",
        "    return df\n",
        "\n",
        "# -----------------------\n",
        "# 8D. Mark Title/Dialogue by exact LLM spans\n",
        "# -----------------------\n",
        "def _overlap(a0,a1,b0,b1):\n",
        "    return max(0, min(a1,b1)-max(a0,b0)) > 0\n",
        "\n",
        "def mark_title_and_dialogue(df_map: pd.DataFrame, df_texts: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df_map.copy()\n",
        "\n",
        "    if \"Sentence Boundaries\" not in df.columns:\n",
        "        df[\"Sentence Boundaries\"] = \"\"\n",
        "    df[\"TITLE\"] = False\n",
        "    df[\"DIALOGUE\"] = False\n",
        "\n",
        "    # Fast lookup per ID\n",
        "    tags_by_id = {str(i): (json.loads(t) if isinstance(t,str) and t.strip().startswith(\"[\") else (t or []))\n",
        "                  for i,t in zip(df_texts[\"ID\"].astype(str), df_texts.get(\"NarrativeTagsJSON\",[]))}\n",
        "    dlg_by_id  = {str(i): (json.loads(t) if isinstance(t,str) and t.strip().startswith(\"[\") else (t or []))\n",
        "                  for i,t in zip(df_texts[\"ID\"].astype(str), df_texts.get(\"DialogueSpansJSON\",[]))}\n",
        "\n",
        "    def per_id(g: pd.DataFrame) -> pd.DataFrame:\n",
        "        g = g.sort_values([\"CorrSentenceID\",\"corr_index\"], kind=\"mergesort\")\n",
        "        ID = str(g[\"ID\"].iloc[0]) if \"ID\" in g.columns else str(g[\"RowID\"].iloc[0])\n",
        "\n",
        "        # Mark titles using exact corrected spans\n",
        "        for tag in tags_by_id.get(ID, []):\n",
        "            if tag.get(\"type\") == \"title\":\n",
        "                st, en = int(tag[\"start\"]), int(tag[\"end\"])\n",
        "                mask = g.apply(lambda r: _overlap(r.get(\"corr_start\",0), r.get(\"corr_end\",0), st, en), axis=1)\n",
        "                if mask.any():\n",
        "                    g.loc[mask, \"TITLE\"] = True\n",
        "\n",
        "        # Mark dialogue tokens by span overlap\n",
        "        for sp in dlg_by_id.get(ID, []):\n",
        "            st, en = int(sp[\"start\"]), int(sp[\"end\"])\n",
        "            mask = g.apply(lambda r: _overlap(r.get(\"corr_start\",0), r.get(\"corr_end\",0), st, en), axis=1)\n",
        "            if mask.any():\n",
        "                g.loc[mask, \"DIALOGUE\"] = True\n",
        "\n",
        "        # If any TITLE tokens share CorrSentenceID with following words, keep TITLE strictly inside span\n",
        "        # (We don't move sentence IDs—counts exclude TITLE rows in Step 9)\n",
        "        g.loc[g[\"TITLE\"]==True, \"Sentence Boundaries\"] = \"Title\"\n",
        "        return g\n",
        "\n",
        "    if \"ID\" not in df.columns:\n",
        "        df[\"ID\"] = df[\"RowID\"].astype(str)\n",
        "    df = df.groupby(\"ID\", group_keys=False).apply(per_id).reset_index(drop=True)\n",
        "\n",
        "    # SentenceRef (keeps numbering; sentence 0 exists even without a title)\n",
        "    def _sid3(x):\n",
        "        try: return f\"{int(x):03d}\"\n",
        "        except: return \"000\"\n",
        "    df[\"SentenceRef\"] = df[\"ID\"].astype(str) + \"_s\" + df[\"CorrSentenceID\"].map(_sid3)\n",
        "    return df\n",
        "\n",
        "# -----------------------\n",
        "# 8E. Boundary flags (skip titles), capitalization check only\n",
        "# -----------------------\n",
        "TERMINALS_HARD = {\".\",\"!\",\"?\",\"…\",\"...\",\"?!\",\"!?\"}\n",
        "OPENING_PUNCT  = {'\"', \"“\", \"‘\", \"«\", \"(\", \"[\", \"{\"}\n",
        "\n",
        "def _first_alpha_capitalized(s: str):\n",
        "    if not isinstance(s, str): return None\n",
        "    m = re.search(r\"[A-Za-z]\", s)\n",
        "    if not m: return None\n",
        "    return s[m.start()].isupper()\n",
        "\n",
        "def add_sentence_boundary_flags(df_map: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df_map.copy()\n",
        "    for col in (\"Sentence Boundaries\", \"BoundaryCheck\"):\n",
        "        if col not in df.columns:\n",
        "            df[col] = \"\"\n",
        "\n",
        "    if \"corr_index\" not in df.columns:\n",
        "        df[\"corr_index\"] = np.nan\n",
        "    df[\"_rowpos\"] = np.arange(len(df))\n",
        "    df[\"_sort_corr\"] = pd.to_numeric(df[\"corr_index\"], errors=\"coerce\").fillna(1e12) + (df[\"_rowpos\"]*1e-9)\n",
        "    df = df.sort_values([\"ID\",\"CorrSentenceID\",\"_sort_corr\"], kind=\"mergesort\")\n",
        "\n",
        "    def _first_content_row(g: pd.DataFrame):\n",
        "        for idx, tok, title in zip(g.index, g[\"corr_token\"].astype(str), g[\"TITLE\"]):\n",
        "            if title:  # skip title tokens entirely\n",
        "                continue\n",
        "            if tok in OPENING_PUNCT:\n",
        "                continue\n",
        "            if re.search(r\"\\w\", tok):\n",
        "                return idx\n",
        "        return None\n",
        "\n",
        "    def _last_terminal_row(g: pd.DataFrame):\n",
        "        toks = g[\"corr_token\"].astype(str).tolist()\n",
        "        titl = g[\"TITLE\"].tolist()\n",
        "        for pos in range(len(toks)-1, -1, -1):\n",
        "            if titl[pos]:\n",
        "                continue\n",
        "            if toks[pos] in TERMINALS_HARD:\n",
        "                return g.index[pos]\n",
        "        return None\n",
        "\n",
        "    for (id_, sid), g in df.groupby([\"ID\",\"CorrSentenceID\"], sort=False):\n",
        "        if g[\"TITLE\"].all():\n",
        "            df.loc[g.index, \"Sentence Boundaries\"] = \"Title\"\n",
        "            continue\n",
        "\n",
        "        g = g.sort_values(\"_sort_corr\", kind=\"mergesort\")\n",
        "        b = _first_content_row(g)\n",
        "        e = _last_terminal_row(g)\n",
        "\n",
        "        if b is not None:\n",
        "            prev = df.at[b, \"Sentence Boundaries\"]\n",
        "            if prev.strip() != \"Title\":\n",
        "                df.at[b, \"Sentence Boundaries\"] = (prev + (\" | \" if prev else \"\") + \"Sentence Beginning\")\n",
        "                # Capitalization-only correctness:\n",
        "                raw_tok = str(df.at[b, \"raw_token\"] or \"\")\n",
        "                corr_tok = str(df.at[b, \"corr_token\"] or \"\")\n",
        "                rcap = _first_alpha_capitalized(raw_tok)\n",
        "                ccap = _first_alpha_capitalized(corr_tok)\n",
        "                if rcap is None or ccap is None:\n",
        "                    tag = \"Unknown Beginning\"\n",
        "                else:\n",
        "                    tag = \"Correct Beginning\" if (rcap == ccap) else \"Incorrect Beginning\"\n",
        "                prev = df.at[b, \"BoundaryCheck\"]\n",
        "                df.at[b, \"BoundaryCheck\"] = prev + (\" | \" if prev else \"\") + tag\n",
        "\n",
        "        if e is not None:\n",
        "            prev = df.at[e, \"Sentence Boundaries\"]\n",
        "            if prev.strip() != \"Title\":\n",
        "                df.at[e, \"Sentence Boundaries\"] = prev + (\" | \" if prev else \"\") + \"Sentence Ending\"\n",
        "                # Ending correctness: exact token match only (spelling fixes shouldn’t matter here)\n",
        "                re_tok = str(df.at[e, \"raw_token\"] or \"\")\n",
        "                ce_tok = str(df.at[e, \"corr_token\"] or \"\")\n",
        "                tag = \"Correct Ending\" if re_tok == ce_tok else \"Incorrect Ending\"\n",
        "                prev = df.at[e, \"BoundaryCheck\"]\n",
        "                df.at[e, \"BoundaryCheck\"] = prev + (\" | \" if prev else \"\") + tag\n",
        "\n",
        "    def _sid3(x):\n",
        "        try: return f\"{int(x):03d}\"\n",
        "        except: return \"000\"\n",
        "    df[\"SentenceRef\"] = df[\"ID\"].astype(str) + \"_s\" + df[\"CorrSentenceID\"].map(_sid3)\n",
        "\n",
        "    df.drop(columns=[\"_rowpos\",\"_sort_corr\"], inplace=True, errors=\"ignore\")\n",
        "    return df\n",
        "\n",
        "# -----------------------\n",
        "# 9. Sentences table (exclude titles) + carry tags/dialogue\n",
        "# -----------------------\n",
        "NO_SPACE_BEFORE = set(list(\".,;:!?)]}\\\"'»”’…\"))\n",
        "NO_SPACE_AFTER  = set(list(\"([{\\\"'«“‘\"))\n",
        "\n",
        "def _detok(tokens):\n",
        "    # combine tokens; unify spaced ellipsis \". . .\" → \"...\"\n",
        "    out = []\n",
        "    for t in tokens:\n",
        "        if t is None or (isinstance(t, float) and math.isnan(t)):\n",
        "            continue\n",
        "        t = str(t)\n",
        "        if not out:\n",
        "            out.append(t); continue\n",
        "        prev = out[-1]\n",
        "        if t in NO_SPACE_BEFORE or re.fullmatch(r\"[.]{3}\", t):\n",
        "            out[-1] = prev + t\n",
        "        elif prev in NO_SPACE_AFTER:\n",
        "            out[-1] = prev + t\n",
        "        else:\n",
        "            out.append(\" \" + t)\n",
        "    s = \"\".join(out)\n",
        "    s = re.sub(r\"\\.\\s*\\.\\s*\\.\", \"...\", s)\n",
        "    return s.strip()\n",
        "\n",
        "def _span_coverage(rows):\n",
        "    # fraction of sentence that is dialogue/title by token count\n",
        "    if len(rows)==0:\n",
        "        return 0.0\n",
        "    return float(np.mean(rows))\n",
        "\n",
        "def _summarize_sentence(g: pd.DataFrame, tags_row: pd.Series) -> pd.Series:\n",
        "    corr_tokens = g[\"corr_token\"].tolist()\n",
        "    raw_tokens  = [x for x in g[\"raw_token\"].tolist() if not pd.isna(x)] if \"raw_token\" in g.columns else []\n",
        "    corr_text   = _detok(corr_tokens)\n",
        "    raw_text    = _detok(raw_tokens) if raw_tokens else \"\"\n",
        "\n",
        "    b_rows = g[g[\"Sentence Boundaries\"].str.contains(\"Sentence Beginning\", na=False)]\n",
        "    e_rows = g[g[\"Sentence Boundaries\"].str.contains(\"Sentence Ending\",   na=False)]\n",
        "\n",
        "    begin_ok = np.nan\n",
        "    end_ok   = np.nan\n",
        "    if not b_rows.empty:\n",
        "        chk = \" | \".join(b_rows[\"BoundaryCheck\"].dropna().astype(str))\n",
        "        begin_ok = 1 if \"Correct Beginning\" in chk else (0 if \"Incorrect Beginning\" in chk else np.nan)\n",
        "    if not e_rows.empty:\n",
        "        chk = \" | \".join(e_rows[\"BoundaryCheck\"].dropna().astype(str))\n",
        "        end_ok = 1 if \"Correct Ending\" in chk else (0 if \"Incorrect Ending\" in chk else np.nan)\n",
        "\n",
        "    ops = g[\"op\"] if \"op\" in g.columns else pd.Series([], dtype=object)\n",
        "\n",
        "    # Dialogue/Title coverage\n",
        "    dlg_frac   = _span_coverage(g[\"DIALOGUE\"].astype(bool).tolist())\n",
        "    title_frac = _span_coverage(g[\"TITLE\"].astype(bool).tolist())\n",
        "\n",
        "    rec = {\n",
        "        \"SentenceRef\": g[\"SentenceRef\"].iloc[0],\n",
        "        \"CorrectedSentence\": corr_text,\n",
        "        \"RawSentence\": raw_text,\n",
        "        \"TokensInSentence\": int(len(g)),\n",
        "        \"EditsInSentence\": int((ops != \"equal\").sum()) if not ops.empty else np.nan,\n",
        "        \"EqualsInSentence\": int((ops == \"equal\").sum()) if not ops.empty else np.nan,\n",
        "        \"Insertions\": int((ops == \"insert\").sum()) if not ops.empty else np.nan,\n",
        "        \"Deletions\": int((ops == \"delete\").sum()) if not ops.empty else np.nan,\n",
        "        \"Replacements\": int((ops == \"replace\").sum()) if not ops.empty else np.nan,\n",
        "        \"BeginBoundaryRow\": (b_rows.index[0] if not b_rows.empty else np.nan),\n",
        "        \"EndBoundaryRow\":   (e_rows.index[0] if not e_rows.empty else np.nan),\n",
        "        \"CorrectBeginning\": begin_ok,\n",
        "        \"CorrectEnding\":    end_ok,\n",
        "        \"HasHardTerminal\":  any(t in TERMINALS_HARD for t in corr_tokens),\n",
        "        \"HasOpeningQuote\":  any(t in {'\"', \"“\", \"‘\", \"«\"} for t in corr_tokens),\n",
        "        \"HasClosingQuote\":  any(t in {'\"', \"”\", \"’\", \"»\"} for t in corr_tokens),\n",
        "        \"DialogueTokenFrac\": dlg_frac,\n",
        "        \"TitleTokenFrac\": title_frac,\n",
        "    }\n",
        "\n",
        "    # carry JSON for this doc (same for all sentences of the doc)\n",
        "    rec[\"NarrativeTagsJSON\"] = tags_row.get(\"NarrativeTagsJSON\", \"[]\")\n",
        "    rec[\"DialogueSpansJSON\"] = tags_row.get(\"DialogueSpansJSON\", \"[]\")\n",
        "\n",
        "    return pd.Series(rec)\n",
        "\n",
        "def run_step8(df_preprocessed: pd.DataFrame,\n",
        "              raw_col=\"Raw text\",\n",
        "              id_col=\"ID\",\n",
        "              client=None,\n",
        "              model=MODEL_ID,\n",
        "              use_mock=USE_MOCK):\n",
        "    df_corr = run_correct_only(\n",
        "        df_preprocessed,\n",
        "        text_col=raw_col,\n",
        "        id_col=id_col,\n",
        "        client=None if use_mock else client,\n",
        "        model=model,\n",
        "        use_mock=use_mock,\n",
        "        out_col=\"Corrected text (8)\"\n",
        "    )\n",
        "    df_map, df_texts = run_mapping_only(\n",
        "        df_corr, id_col=id_col, raw_col=raw_col, corr_col=\"Corrected text (8)\"\n",
        "    )\n",
        "    df_map = assign_corr_sentence_ids(df_map)\n",
        "    df_map = mark_title_and_dialogue(df_map, df_texts)\n",
        "    df_map = add_sentence_boundary_flags(df_map)\n",
        "    return df_texts, df_map\n",
        "\n",
        "def run_step9(df_map: pd.DataFrame, df_texts_with_tags: pd.DataFrame) -> pd.DataFrame:\n",
        "    need = {\"ID\",\"CorrSentenceID\",\"corr_token\",\"Sentence Boundaries\",\n",
        "            \"BoundaryCheck\",\"SentenceRef\",\"TITLE\",\"DIALOGUE\"}\n",
        "    missing = need - set(df_map.columns)\n",
        "    if missing:\n",
        "        raise KeyError(f\"df_map missing columns needed for Step 9: {missing}\")\n",
        "\n",
        "    sort_cols = [\"ID\",\"CorrSentenceID\"]\n",
        "    if \"corr_index\" in df_map.columns: sort_cols.append(\"corr_index\")\n",
        "\n",
        "    wm = df_map.sort_values(sort_cols, kind=\"mergesort\").copy()\n",
        "    # Exclude title tokens from aggregation\n",
        "    wm_nontitle = wm[~wm[\"TITLE\"].astype(bool)].copy()\n",
        "\n",
        "    # Per-ID tags rows to carry JSON into sentence rows\n",
        "    tags_by_id = df_texts_with_tags.set_index(\"ID\")[[\"NarrativeTagsJSON\",\"DialogueSpansJSON\"]]\n",
        "\n",
        "    out = []\n",
        "    for (ID, SID), g in wm_nontitle.groupby([\"ID\",\"CorrSentenceID\"], sort=False):\n",
        "        # fallback if ID not present in tags_by_id (shouldn't happen)\n",
        "        tags_row = tags_by_id.loc[ID] if ID in tags_by_id.index else pd.Series({}, dtype=object)\n",
        "        out.append(_summarize_sentence(g, tags_row))\n",
        "\n",
        "    sent_df = pd.DataFrame(out).sort_values([\"SentenceRef\"], kind=\"mergesort\").reset_index(drop=True)\n",
        "    return sent_df\n",
        "\n",
        "# -----------------------\n",
        "# Save + Download wrapper\n",
        "# -----------------------\n",
        "def _ensure_dir(path):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    return path\n",
        "\n",
        "def save_and_download_step8_9(\n",
        "    df_preprocessed: pd.DataFrame,\n",
        "    *,\n",
        "    raw_col=\"Raw text\",\n",
        "    id_col=\"ID\",\n",
        "    client=None,              # OpenAI client from your Step 6\n",
        "    model=MODEL_ID,\n",
        "    use_mock=USE_MOCK,        # True for offline tests\n",
        "    out_dir=OUT_DIR\n",
        "):\n",
        "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    _ensure_dir(out_dir)\n",
        "\n",
        "    # Step 8\n",
        "    df_texts_8, df_map_8 = run_step8(\n",
        "        df_preprocessed,\n",
        "        raw_col=raw_col,\n",
        "        id_col=id_col,\n",
        "        client=client if not use_mock else None,\n",
        "        model=model,\n",
        "        use_mock=use_mock\n",
        "    )\n",
        "\n",
        "    # Step 9\n",
        "    sent_df = run_step9(df_map_8, df_texts_8)\n",
        "\n",
        "    # Paths\n",
        "    p_texts = os.path.join(out_dir, f\"step8_texts_{ts}.csv\")\n",
        "    p_map   = os.path.join(out_dir, f\"step8_wordmap_checked_{ts}.csv\")\n",
        "    p_sent  = os.path.join(out_dir, f\"step9_sentence_mapping_with_boundaries_{ts}.csv\")\n",
        "    p_zip   = os.path.join(out_dir, f\"step8_9_outputs_{ts}.zip\")\n",
        "\n",
        "    # Save\n",
        "    df_texts_8.to_csv(p_texts, index=False, encoding=\"utf-8\")\n",
        "    df_map_8.to_csv(p_map,   index=False, encoding=\"utf-8\")\n",
        "    sent_df.to_csv(p_sent,   index=False, encoding=\"utf-8\")\n",
        "\n",
        "    # Zip bundle\n",
        "    with zipfile.ZipFile(p_zip, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "        zf.write(p_texts, arcname=os.path.basename(p_texts))\n",
        "        zf.write(p_map,   arcname=os.path.basename(p_map))\n",
        "        zf.write(p_sent,  arcname=os.path.basename(p_sent))\n",
        "\n",
        "    print(\"Saved:\")\n",
        "    print(\"  \", p_texts)\n",
        "    print(\"  \", p_map)\n",
        "    print(\"  \", p_sent)\n",
        "    print(\"  \", p_zip)\n",
        "\n",
        "    if _COLAB:\n",
        "        try:\n",
        "            files.download(p_texts)\n",
        "            files.download(p_map)\n",
        "            files.download(p_sent)\n",
        "            files.download(p_zip)\n",
        "        except Exception as e:\n",
        "            print(\"Download hint:\", e)\n",
        "\n",
        "    return dict(\n",
        "        step8_texts_path=p_texts,\n",
        "        step8_map_path=p_map,\n",
        "        step9_sentences_path=p_sent,\n",
        "        zip_path=p_zip,\n",
        "        df_texts_8=df_texts_8,\n",
        "        df_map_8=df_map_8,\n",
        "        sent_df=sent_df\n",
        "    )\n",
        "\n",
        "# ============== RUN IT ==============\n",
        "# Ensure df_preprocessed (with \"ID\" and \"Raw text\") exists and `client` (OpenAI v1) is ready\n",
        "# results = save_and_download_step8_9(\n",
        "#     df_preprocessed,\n",
        "#     raw_col=\"Raw text\",\n",
        "#     id_col=\"ID\",\n",
        "#     client=client,\n",
        "#     model=\"gpt-4o\",\n",
        "#     use_mock=False,     # set True to test without API\n",
        "#     out_dir=\"/content\"\n",
        "# )\n",
        "# results[\"step8_map_path\"], results[\"step9_sentences_path\"], results[\"zip_path\"]\n"
      ],
      "metadata": {
        "id": "uwx9gDVhQqlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l24go7Y5QqgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = save_and_download_step8_9(\n",
        "    df_preprocessed,\n",
        "    raw_col=\"Raw text\",\n",
        "    id_col=\"ID\",\n",
        "    client=client,           # from your Step 6\n",
        "    model=\"gpt-4o\",\n",
        "    use_mock=False,\n",
        "    out_dir=\"/content\"       # change to your Drive folder if you want\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "HonogOwbPW-F",
        "outputId": "18ed524f-3385-4091-ecdb-be3d88a1604d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3862358514.py:474: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df[\"_sort_key\"] = df.groupby(\"ID\", group_keys=False).apply(_stable_sort_key)\n",
            "/tmp/ipython-input-3862358514.py:511: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(_assign)\n",
            "/tmp/ipython-input-3862358514.py:563: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df = df.groupby(\"ID\", group_keys=False).apply(per_id).reset_index(drop=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved:\n",
            "   /content/step8_texts_20251025_010939.csv\n",
            "   /content/step8_wordmap_checked_20251025_010939.csv\n",
            "   /content/step9_sentence_mapping_with_boundaries_20251025_010939.csv\n",
            "   /content/step8_9_outputs_20251025_010939.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d8d3be1e-bd53-4e5c-95cf-ce71521a5289\", \"step8_texts_20251025_010939.csv\", 69730)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7ff44ff9-2d94-46c4-8d9f-eb08336c83d3\", \"step8_wordmap_checked_20251025_010939.csv\", 972436)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6be23d53-562c-4ee0-9097-79b434cfa432\", \"step9_sentence_mapping_with_boundaries_20251025_010939.csv\", 298398)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e71d4eef-bdd0-400b-bd7a-cdfddd573203\", \"step8_9_outputs_20251025_010939.zip\", 216780)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MFZztBXoPVrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fZks_smPPVig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = save_and_download_step8_9(\n",
        "    df_preprocessed,\n",
        "    raw_col=\"Raw text\",\n",
        "    id_col=\"ID\",        # or whatever your ID is; code will synthesize one if missing\n",
        "    client=client,      # from your Step 6 verify\n",
        "    model=\"gpt-4o\",\n",
        "    out_dir=\"/content\"\n",
        ")\n",
        "\n",
        "results[\"step8_map_path\"], results[\"step9_sentences_path\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "cTecSFscPeS8",
        "outputId": "d7e212aa-438f-4771-ff93-cd2adad4a36a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3862358514.py:474: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df[\"_sort_key\"] = df.groupby(\"ID\", group_keys=False).apply(_stable_sort_key)\n",
            "/tmp/ipython-input-3862358514.py:511: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(_assign)\n",
            "/tmp/ipython-input-3862358514.py:563: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df = df.groupby(\"ID\", group_keys=False).apply(per_id).reset_index(drop=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved:\n",
            "   /content/step8_texts_20251025_011131.csv\n",
            "   /content/step8_wordmap_checked_20251025_011131.csv\n",
            "   /content/step9_sentence_mapping_with_boundaries_20251025_011131.csv\n",
            "   /content/step8_9_outputs_20251025_011131.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0df6a08b-fa6a-488c-8809-8ce050936fb7\", \"step8_texts_20251025_011131.csv\", 68678)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3d0f98e2-0499-4bd4-a0e8-ba07e04d8f2f\", \"step8_wordmap_checked_20251025_011131.csv\", 971380)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_80fdb12a-4f88-4a69-bb55-9cba63826bbe\", \"step9_sentence_mapping_with_boundaries_20251025_011131.csv\", 224003)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0d35d575-d8ee-4102-8c51-1b0779e25f84\", \"step8_9_outputs_20251025_011131.zip\", 212278)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/step8_wordmap_checked_20251025_011131.csv',\n",
              " '/content/step9_sentence_mapping_with_boundaries_20251025_011131.csv')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Below is archived - Please ignore\n"
      ],
      "metadata": {
        "id": "oKmA8CKXNtEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the code that will introduce the functions"
      ],
      "metadata": {
        "id": "R8RCwD9aVxM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8.1  Imports and loggers"
      ],
      "metadata": {
        "id": "7rlZ5L9dehQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " ===============================================\n",
        "# Step 8A — Corrector (API or mock)\n",
        " ===============================================\n",
        "import json\n",
        "import re\n",
        "\n",
        "MODEL_ID = \"gpt-4o\"\n",
        "MAX_TOKENS = 1200\n",
        "USE_MOCK = False  # set True to test without API\n",
        "\n",
        "def correct_text(raw: str, client=None, model=MODEL_ID, use_mock=USE_MOCK):\n",
        "    s = str(raw or \"\")\n",
        "    if use_mock or client is None:\n",
        "        t = s.strip()\n",
        "        m = re.search(r\"[A-Za-z]\", t)\n",
        "        if m:\n",
        "            i = m.start()\n",
        "            t = t[:i] + t[i].upper() + t[i+1:]\n",
        "        if t and not re.search(r\"[.!?…]\\s*$\", t):\n",
        "            t += \".\"\n",
        "        return t, \"mock\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a meticulous copy-editor. Fix punctuation, grammar, and spelling in the text.\n",
        "Do not change meaning, voice, or level of formality. Keep paragraphing and spacing sane.\n",
        "If there is a title, or ending But these in title case.\n",
        "Return JSON only: {{\"corrected_text\":\"...\"}}\n",
        "\n",
        "Text:\n",
        "<<<BEGIN>>>\n",
        "{s}\n",
        "<<<END>>>\n",
        "\"\"\".strip()\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "        temperature=0.0,\n",
        "        max_tokens=MAX_TOKENS\n",
        "    )\n",
        "    out = (resp.choices[0].message.content or \"\").strip()\n",
        "\n",
        "    # extract {\"corrected_text\": \"...\"} if present\n",
        "    def _first_obj(txt):\n",
        "        start = txt.find(\"{\")\n",
        "        if start < 0: return None\n",
        "        depth, in_str, esc = 0, False, False\n",
        "        for k in range(start, len(txt)):\n",
        "            ch = txt[k]\n",
        "            if in_str:\n",
        "                if esc: esc = False\n",
        "                elif ch == \"\\\\\": esc = True\n",
        "                elif ch == '\"': in_str = False\n",
        "            else:\n",
        "                if ch == '\"': in_str = True\n",
        "                elif ch == \"{\": depth += 1\n",
        "                elif ch == \"}\":\n",
        "                    depth -= 1\n",
        "                    if depth == 0:\n",
        "                        frag = txt[start:k+1]\n",
        "                        try:\n",
        "                            return json.loads(frag)\n",
        "                        except Exception:\n",
        "                            return None\n",
        "        return None\n",
        "\n",
        "    js = _first_obj(out)\n",
        "    if isinstance(js, dict) and \"corrected_text\" in js:\n",
        "        return str(js[\"corrected_text\"]).strip(), model\n",
        "\n",
        "    # fallback to plain text\n",
        "    return out, model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tn59pIVhqzzj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "29163ef4-d5eb-44a5-ee97-da8a24416455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-2243263149.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2243263149.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ===============================================\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# Step 8B — Define run_correct_only used by 8C\n",
        "# ===============================================\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "def run_correct_only(\n",
        "    df_in: pd.DataFrame,\n",
        "    text_col: str = \"Raw text\",\n",
        "    id_col: str = \"ID\",\n",
        "    client=None,\n",
        "    model: str = MODEL_ID,\n",
        "    use_mock: bool = USE_MOCK,\n",
        "    out_col: str = \"Corrected text (8)\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Returns a copy of df_in with a new column out_col that contains the corrected text.\n",
        "    Uses correct_text(...) which you defined earlier. Respects USE_MOCK and your OpenAI client.\n",
        "    \"\"\"\n",
        "    if text_col not in df_in.columns:\n",
        "        raise KeyError(f\"Missing required column: {text_col}\")\n",
        "\n",
        "    df_out = df_in.copy()\n",
        "\n",
        "    # Ensure ID column exists and is string-normalised\n",
        "    if id_col not in df_out.columns:\n",
        "        df_out[id_col] = pd.RangeIndex(len(df_out)).astype(str)\n",
        "    else:\n",
        "        # reuse your normaliser from Step 2A if present\n",
        "        try:\n",
        "            df_out[id_col] = _normalize_id_series(df_out[id_col])\n",
        "        except NameError:\n",
        "            df_out[id_col] = df_out[id_col].astype(str)\n",
        "\n",
        "    corrected = []\n",
        "    for raw in tqdm(df_out[text_col].astype(str).tolist(), desc=\"Correcting\", total=len(df_out)):\n",
        "        fixed, _src = correct_text(raw, client=client, model=model, use_mock=use_mock)\n",
        "        corrected.append(fixed)\n",
        "\n",
        "    df_out[out_col] = corrected\n",
        "    return df_out\n"
      ],
      "metadata": {
        "id": "1S3fOC76BU-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# Patch: minimal build_word_map if missing\n",
        "# Produces rows with raw_index, raw_token, raw_start, raw_end,\n",
        "#         corr_index, corr_token, corr_start, corr_end,\n",
        "#         op, equal_ci, error_type\n",
        "# ===============================================\n",
        "# --- PATCH: safe build_word_map (no 're' shadowing) ---\n",
        "import re\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "_WORD_RX = re.compile(r\"\\w\", flags=re.UNICODE)\n",
        "\n",
        "def _simple_tokenize(s):\n",
        "    # keep quotes and punctuation as separate tokens\n",
        "    return re.findall(r\"\\w+|[^\\w\\s]\", s or \"\", flags=re.UNICODE)\n",
        "\n",
        "def _rebuild_offsets(text, tokens):\n",
        "    spans = []\n",
        "    i = 0\n",
        "    for tok in tokens:\n",
        "        start = text.find(tok, i)\n",
        "        if start < 0:\n",
        "            start = i\n",
        "        end = start + len(tok)\n",
        "        spans.append((start, end))\n",
        "        i = end\n",
        "    return spans\n",
        "\n",
        "def _is_word(tok: str) -> bool:\n",
        "    return bool(tok) and bool(_WORD_RX.search(tok))\n",
        "\n",
        "def build_word_map(raw_text, corr_text, use_unmerge=True):\n",
        "    raw_tokens  = _simple_tokenize(raw_text or \"\")\n",
        "    corr_tokens = _simple_tokenize(corr_text or \"\")\n",
        "\n",
        "    raw_spans  = _rebuild_offsets(raw_text or \"\", raw_tokens)\n",
        "    corr_spans = _rebuild_offsets(corr_text or \"\", corr_tokens)\n",
        "\n",
        "    sm = SequenceMatcher(a=[t.lower() for t in raw_tokens],\n",
        "                         b=[t.lower() for t in corr_tokens],\n",
        "                         autojunk=False)\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
        "        if tag == \"equal\":\n",
        "            for k in range(i2 - i1):\n",
        "                r_tok = raw_tokens[i1 + k]; c_tok = corr_tokens[j1 + k]\n",
        "                r_start, r_end = raw_spans[i1 + k]\n",
        "                c_start, c_end = corr_spans[j1 + k]\n",
        "                rows.append({\n",
        "                    \"raw_index\": i1 + k, \"raw_token\": r_tok, \"raw_start\": r_start, \"raw_end\": r_end,\n",
        "                    \"corr_index\": j1 + k, \"corr_token\": c_tok, \"corr_start\": c_start, \"corr_end\": c_end,\n",
        "                    \"op\": \"equal\",\n",
        "                    \"equal_ci\": (r_tok == c_tok),\n",
        "                    \"error_type\": \"Equal\"\n",
        "                })\n",
        "\n",
        "        elif tag == \"replace\":\n",
        "            # pair as many as possible\n",
        "            m = min(i2 - i1, j2 - j1)\n",
        "            for k in range(m):\n",
        "                r_tok = raw_tokens[i1 + k]; c_tok = corr_tokens[j1 + k]\n",
        "                r_start, r_end = raw_spans[i1 + k]\n",
        "                c_start, c_end = corr_spans[j1 + k]\n",
        "                err = \"Spelling\" if (r_tok.lower() != c_tok.lower() and r_tok.isalpha() and c_tok.isalpha()) else \"Replacement\"\n",
        "                rows.append({\n",
        "                    \"raw_index\": i1 + k, \"raw_token\": r_tok, \"raw_start\": r_start, \"raw_end\": r_end,\n",
        "                    \"corr_index\": j1 + k, \"corr_token\": c_tok, \"corr_start\": c_start, \"corr_end\": c_end,\n",
        "                    \"op\": \"replace\",\n",
        "                    \"equal_ci\": (r_tok.lower() == c_tok.lower()),\n",
        "                    \"error_type\": err\n",
        "                })\n",
        "            # spill extra raw as deletions\n",
        "            for k in range(i1 + m, i2):\n",
        "                r_tok = raw_tokens[k]; r_start, r_end = raw_spans[k]\n",
        "                rows.append({\n",
        "                    \"raw_index\": k, \"raw_token\": r_tok, \"raw_start\": r_start, \"raw_end\": r_end,\n",
        "                    \"corr_index\": None, \"corr_token\": None, \"corr_start\": None, \"corr_end\": None,\n",
        "                    \"op\": \"delete\",\n",
        "                    \"equal_ci\": False,\n",
        "                    \"error_type\": \"PunctuationDeletion\" if not _is_word(r_tok) else \"Deletion\"\n",
        "                })\n",
        "            # spill extra corr as insertions\n",
        "            for k in range(j1 + m, j2):\n",
        "                c_tok = corr_tokens[k]; c_start, c_end = corr_spans[k]\n",
        "                rows.append({\n",
        "                    \"raw_index\": None, \"raw_token\": None, \"raw_start\": None, \"raw_end\": None,\n",
        "                    \"corr_index\": k, \"corr_token\": c_tok, \"corr_start\": c_start, \"corr_end\": c_end,\n",
        "                    \"op\": \"insert\",\n",
        "                    \"equal_ci\": False,\n",
        "                    \"error_type\": \"PunctuationInsertion\" if not _is_word(c_tok) else \"Insertion\"\n",
        "                })\n",
        "\n",
        "        elif tag == \"delete\":\n",
        "            for k in range(i1, i2):\n",
        "                r_tok = raw_tokens[k]; r_start, r_end = raw_spans[k]\n",
        "                rows.append({\n",
        "                    \"raw_index\": k, \"raw_token\": r_tok, \"raw_start\": r_start, \"raw_end\": r_end,\n",
        "                    \"corr_index\": None, \"corr_token\": None, \"corr_start\": None, \"corr_end\": None,\n",
        "                    \"op\": \"delete\",\n",
        "                    \"equal_ci\": False,\n",
        "                    \"error_type\": \"PunctuationDeletion\" if not _is_word(r_tok) else \"Deletion\"\n",
        "                })\n",
        "\n",
        "        elif tag == \"insert\":\n",
        "            for k in range(j1, j2):\n",
        "                c_tok = corr_tokens[k]; c_start, c_end = corr_spans[k]\n",
        "                rows.append({\n",
        "                    \"raw_index\": None, \"raw_token\": None, \"raw_start\": None, \"raw_end\": None,\n",
        "                    \"corr_index\": k, \"corr_token\": c_tok, \"corr_start\": c_start, \"corr_end\": c_end,\n",
        "                    \"op\": \"insert\",\n",
        "                    \"equal_ci\": False,\n",
        "                    \"error_type\": \"PunctuationInsertion\" if not _is_word(c_tok) else \"Insertion\"\n",
        "                })\n",
        "\n",
        "    # For Step 8C, return rows and an \"unmerged\" echo of raw text\n",
        "    return rows, (raw_text or \"\")\n",
        "\n"
      ],
      "metadata": {
        "id": "UlldMikhEJHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# Step 8D — Run correction and mapping, then save\n",
        "# ===============================================\n",
        "def run_mapping_only(df_with_corr,\n",
        "                     id_col=\"ID\",\n",
        "                     raw_col=\"Raw text\",\n",
        "                     corr_col=\"Corrected text (8)\",\n",
        "                     add_unmerged_to_texts=True):\n",
        "    \"\"\"\n",
        "    Build per-token map between raw and corrected text.\n",
        "    Guarantees at least one row per document (placeholder) so RowID coverage\n",
        "    matches the number of rows in df_with_corr.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    # --- checks ---\n",
        "    need = {raw_col, corr_col}\n",
        "    missing = need - set(df_with_corr.columns)\n",
        "    if missing:\n",
        "        raise KeyError(f\"Need both raw and corrected columns present: missing {missing}\")\n",
        "\n",
        "    # --- safe IDs as strings (no '.0', no NaN) ---\n",
        "    def _norm_id_series(s: pd.Series) -> pd.Series:\n",
        "        s = s.astype(str)\n",
        "        s = s.str.replace(r\"\\.0$\", \"\", regex=True)\n",
        "        def _fix(x):\n",
        "            if any(c.isalpha() for c in x):  # alphanumeric: keep as-is\n",
        "                return x\n",
        "            try:\n",
        "                if \".\" in x or \"e\" in x.lower():\n",
        "                    f = float(x)\n",
        "                    if f.is_integer():\n",
        "                        return str(int(f))\n",
        "            except Exception:\n",
        "                pass\n",
        "            return x\n",
        "        return s.map(_fix)\n",
        "\n",
        "    if id_col in df_with_corr.columns:\n",
        "        row_ids = _norm_id_series(df_with_corr[id_col])\n",
        "    else:\n",
        "        row_ids = pd.Series(pd.RangeIndex(len(df_with_corr)).astype(str), index=df_with_corr.index)\n",
        "\n",
        "    raws = df_with_corr[raw_col].astype(str)\n",
        "    cors = df_with_corr[corr_col].astype(str)\n",
        "\n",
        "    # deterministic per-doc order (for later joins if needed)\n",
        "    doc_orders = pd.RangeIndex(len(df_with_corr))\n",
        "\n",
        "    all_rows = []\n",
        "    per_row_unmerged = []\n",
        "\n",
        "    for order, (rid, raw, cor) in tqdm(\n",
        "        enumerate(zip(row_ids.tolist(), raws.tolist(), cors.tolist())),\n",
        "        total=len(df_with_corr), desc=\"Mapping\"\n",
        "    ):\n",
        "        rows, raw_unmerged = build_word_map(raw, cor, use_unmerge=True)\n",
        "        per_row_unmerged.append(raw_unmerged)\n",
        "\n",
        "        if not rows:\n",
        "            # Ensure at least one record for this doc so the ID exists in map_df\n",
        "            rows = [{\n",
        "                \"raw_index\": np.nan, \"raw_token\": None, \"raw_start\": np.nan, \"raw_end\": np.nan,\n",
        "                \"corr_index\": np.nan, \"corr_token\": None, \"corr_start\": np.nan, \"corr_end\": np.nan,\n",
        "                \"op\": \"empty\", \"equal_ci\": False, \"error_type\": \"EmptyText\"\n",
        "            }]\n",
        "\n",
        "        for r in rows:\n",
        "            rec = {\"RowID\": rid, \"DocOrder\": order, **r}\n",
        "            rec[\"Changed\"] = (r.get(\"op\") != \"equal\")\n",
        "            all_rows.append(rec)\n",
        "\n",
        "    map_df = pd.DataFrame(all_rows)\n",
        "\n",
        "    # tidy column order\n",
        "    pref = [\n",
        "        \"DocOrder\", \"RowID\",\n",
        "        \"raw_index\",\"raw_token\",\"raw_start\",\"raw_end\",\n",
        "        \"corr_index\",\"corr_token\",\"corr_start\",\"corr_end\",\n",
        "        \"op\",\"equal_ci\",\"error_type\",\"Changed\"\n",
        "    ]\n",
        "    map_df = map_df.reindex(columns=[c for c in pref if c in map_df.columns] +\n",
        "                                   [c for c in map_df.columns if c not in pref])\n",
        "\n",
        "    # carry unmerged raw text back out for reference\n",
        "    df_out = df_with_corr.copy()\n",
        "    if add_unmerged_to_texts:\n",
        "        df_out[\"Raw text (unmerged)\"] = per_row_unmerged\n",
        "\n",
        "    return map_df, df_out\n",
        "\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "# Safety: tiny saver\n",
        "def _save_outputs(df_texts, df_map,\n",
        "                  path_text=\"/content/step8_texts.csv\",\n",
        "                  path_map=\"/content/step8_wordmap.csv\"):\n",
        "    df_texts.to_csv(path_text, index=False, encoding=\"utf-8\")\n",
        "    df_map.to_csv(path_map, index=False, encoding=\"utf-8\")\n",
        "    print(\"Saved:\", path_text)\n",
        "    print(\"Saved:\", path_map)\n",
        "    # Try to download in Colab\n",
        "    for p in (path_text, path_map):\n",
        "        try:\n",
        "            files.download(p)\n",
        "        except Exception as e:\n",
        "            print(f\"(Download hint for {p}: {e})\")\n",
        "\n",
        "# --- 1) Correct (build df_corr) ---\n",
        "# Uses your previously defined: correct_text(), MODEL_ID, USE_MOCK, client, and run_correct_only()\n",
        "df_corr = run_correct_only(\n",
        "    df_preprocessed,\n",
        "    text_col=\"Raw text\",\n",
        "    id_col=\"ID\",\n",
        "    client=None if USE_MOCK else client,\n",
        "    model=MODEL_ID,\n",
        "    use_mock=USE_MOCK\n",
        ")\n",
        "\n",
        "# --- 2) Map raw→corrected tokens; always guarantees one RowID row per doc ---\n",
        "df_map, df_corr_with_unmerged = run_mapping_only(\n",
        "    df_corr,\n",
        "    id_col=\"ID\",\n",
        "    raw_col=\"Raw text\",\n",
        "    corr_col=\"Corrected text (8)\",\n",
        "    add_unmerged_to_texts=True\n",
        ")\n",
        "\n",
        "# --- 3) Sanity prints (counts should match thanks to placeholders) ---\n",
        "n_docs_texts = len(df_corr_with_unmerged)\n",
        "n_docs_map   = df_map[\"RowID\"].nunique() if \"RowID\" in df_map.columns else df_map[\"ID\"].nunique()\n",
        "print(\"Docs in step8_texts:\", n_docs_texts)\n",
        "print(\"Unique RowID in map :\", n_docs_map)\n",
        "\n",
        "if n_docs_map != n_docs_texts:\n",
        "    print(\"⚠️  Note: counts differ. Placeholders should still ensure each doc appears at least once in the map.\")\n",
        "else:\n",
        "    print(\"✅ Map coverage matches text rows.\")\n",
        "\n",
        "# Quick peek\n",
        "print(\"\\nstep8_texts preview:\")\n",
        "print(df_corr_with_unmerged[[\"ID\",\"Raw text\",\"Corrected text (8)\"]].head(3).to_string(index=False))\n",
        "\n",
        "print(\"\\ndf_map preview:\")\n",
        "show_cols = [c for c in [\"RowID\",\"DocOrder\",\"corr_index\",\"corr_token\",\"op\",\"error_type\"] if c in df_map.columns]\n",
        "print(df_map[show_cols].head(10).to_string(index=False))\n",
        "\n",
        "# --- 4) Save outputs ---\n",
        "_save_outputs(\n",
        "    df_texts=df_corr_with_unmerged,\n",
        "    df_map=df_map,\n",
        "    path_text=\"/content/step8_texts.csv\",\n",
        "    path_map=\"/content/step8_wordmap.csv\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "cc_Iq7rr6snn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# 8DE — CorrSentenceID with quote-aware boundaries\n",
        "# ===============================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "if \"df_map\" not in globals():\n",
        "    raise NameError(\"df_map is not defined. Run Step 8 first.\")\n",
        "if \"corr_token\" not in df_map.columns and \"raw_token\" not in df_map.columns:\n",
        "    raise KeyError(\"Need at least 'corr_token' or 'raw_token' in df_map.\")\n",
        "\n",
        "# Ensure ID present\n",
        "if \"RowID\" in df_map.columns:\n",
        "    df_map[\"ID\"] = df_map[\"RowID\"].astype(str)\n",
        "elif \"ID\" in df_map.columns:\n",
        "    df_map[\"ID\"] = df_map[\"ID\"].astype(str)\n",
        "else:\n",
        "    df_map[\"ID\"] = df_map.index.astype(str)\n",
        "\n",
        "# Stable order per ID\n",
        "has_corr_index = \"corr_index\" in df_map.columns\n",
        "if has_corr_index and \"corr_index_orig\" not in df_map.columns:\n",
        "    df_map[\"corr_index_orig\"] = df_map[\"corr_index\"]\n",
        "\n",
        "def _stable_sort_key(group: pd.DataFrame) -> pd.Series:\n",
        "    pos = pd.Series(np.arange(len(group)), index=group.index, dtype=float)\n",
        "    if has_corr_index:\n",
        "        ci = pd.to_numeric(group[\"corr_index\"], errors=\"coerce\")\n",
        "        nan_mask = ci.isna()\n",
        "        bump = (pos - pos.min()) / max((pos.max() - pos.min()), 1) * 1e-6\n",
        "        return ci.where(~nan_mask, 1e9) + bump\n",
        "    return pos\n",
        "\n",
        "df_map[\"_sort_key\"] = df_map.groupby(\"ID\", group_keys=False).apply(_stable_sort_key)\n",
        "\n",
        "# Heuristics\n",
        "TERMINALS = {\".\", \"!\", \"?\", \"…\", \"...\", \"?!\", \"!?\"}\n",
        "CLOSERS   = {\")\", \"]\", \"}\", \"”\", \"’\", \"»\"}             # definite closers\n",
        "OPENERS   = {\"(\", \"[\", \"{\", \"“\", \"‘\", \"«\"}             # definite openers\n",
        "\n",
        "ABBREV = {\n",
        "    \"mr.\", \"mrs.\", \"ms.\", \"dr.\", \"prof.\", \"sr.\", \"jr.\", \"st.\", \"vs.\", \"etc.\",\n",
        "    \"e.g.\", \"i.e.\", \"cf.\", \"fig.\", \"ex.\", \"no.\", \"approx.\", \"circa.\", \"ca.\",\n",
        "    \"dept.\", \"est.\", \"misc.\", \"rev.\", \"jan.\", \"feb.\", \"mar.\", \"apr.\", \"jun.\",\n",
        "    \"jul.\", \"aug.\", \"sep.\", \"sept.\", \"oct.\", \"nov.\", \"dec.\"\n",
        "}\n",
        "\n",
        "_RE_INITIAL      = re.compile(r\"^[A-Z]\\.$\")\n",
        "_RE_NUM_WITH_DOT = re.compile(r\"^\\d+\\.$\")\n",
        "_RE_ELLIPSIS     = re.compile(r\"^\\.\\.\\.$\")\n",
        "\n",
        "def _tok(x):\n",
        "    if x is None or pd.isna(x):\n",
        "        return \"\"\n",
        "    return str(x)\n",
        "\n",
        "def _is_terminal_token(tok: str, prev_tok: str, next_tok: str) -> bool:\n",
        "    t = tok.strip()\n",
        "    if not t:\n",
        "        return False\n",
        "    if _RE_ELLIPSIS.fullmatch(t):\n",
        "        return True\n",
        "    if t in {\"...\", \"?!\", \"!?\"}:\n",
        "        return True\n",
        "    if t in {\".\", \"!\", \"?\"}:\n",
        "        lower_prev = prev_tok.lower().strip()\n",
        "        if lower_prev in ABBREV:\n",
        "            return False\n",
        "        if _RE_INITIAL.fullmatch(prev_tok):\n",
        "            return False\n",
        "        if _RE_NUM_WITH_DOT.fullmatch(prev_tok):\n",
        "            return False\n",
        "        if len(prev_tok) == 1 and prev_tok.isalpha():\n",
        "            return False\n",
        "        if next_tok.isdigit():\n",
        "            return False\n",
        "        return True\n",
        "    if t == \"…\":\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def _likely_ascii_opening(double_quote_prev: str, double_quote_next: str) -> bool:\n",
        "    \"\"\"\n",
        "    Decide if an ASCII double quote should be treated as an opening mark.\n",
        "    Simple cues:\n",
        "      • opening if at document start, or previous token is a terminal or an opener\n",
        "      • opening if next token is a letter/word and not a closer/terminal\n",
        "    \"\"\"\n",
        "    prev = double_quote_prev.strip()\n",
        "    nxt  = double_quote_next.strip()\n",
        "    if prev == \"\" or prev in TERMINALS or prev in OPENERS:\n",
        "        return True\n",
        "    if nxt and nxt not in TERMINALS and nxt not in CLOSERS:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def _assign_corr_sentence_ids(group: pd.DataFrame) -> pd.Series:\n",
        "    g = group.sort_values(\"_sort_key\", kind=\"mergesort\").copy()\n",
        "    toks = g[\"corr_token\"] if \"corr_token\" in g.columns else g[\"raw_token\"]\n",
        "    toks = toks.map(_tok).tolist()\n",
        "\n",
        "    sids = []\n",
        "    sent_id = 0\n",
        "    pending_end = False\n",
        "\n",
        "    for i, tok in enumerate(toks):\n",
        "        t = tok.strip()\n",
        "        prev_tok = toks[i-1].strip() if i > 0 else \"\"\n",
        "        next_tok = toks[i+1].strip() if i+1 < len(toks) else \"\"\n",
        "\n",
        "        if pending_end:\n",
        "            # closers stick to the old sentence\n",
        "            if t in CLOSERS or (t == '\"' and not _likely_ascii_opening(prev_tok, next_tok)):\n",
        "                sids.append(sent_id)\n",
        "                continue\n",
        "            # openers belong to the new sentence, including ASCII \" when it looks opening\n",
        "            if t in OPENERS or (t == '\"' and _likely_ascii_opening(prev_tok, next_tok)):\n",
        "                sent_id += 1\n",
        "                sids.append(sent_id)\n",
        "                pending_end = False\n",
        "                continue\n",
        "            # first real token after the gap starts the next sentence\n",
        "            sent_id += 1\n",
        "            pending_end = False\n",
        "            sids.append(sent_id)\n",
        "        else:\n",
        "            sids.append(sent_id)\n",
        "\n",
        "        # Does current token end a sentence given its context?\n",
        "        if _is_terminal_token(t, prev_tok, next_tok):\n",
        "            pending_end = True\n",
        "\n",
        "    return pd.Series(sids, index=g.index).reindex(group.index)\n",
        "\n",
        "df_map[\"CorrSentenceID\"] = (\n",
        "    df_map.groupby(\"ID\", group_keys=False)\n",
        "          .apply(_assign_corr_sentence_ids)\n",
        "          .astype(\"Int64\")\n",
        ")\n",
        "\n",
        "df_map.drop(columns=[\"_sort_key\"], inplace=True, errors=\"ignore\")\n",
        "print(\"✅ CorrSentenceID rebuilt with quote-aware logic.\")\n",
        "print(\"Sample maxima:\", df_map.groupby(\"ID\")[\"CorrSentenceID\"].max().head(8).to_dict())\n",
        "\n",
        "# Save checkpoint for downstream\n",
        "df_map.to_csv(\"/content/step8_wordmap_checked.csv\", index=False, encoding=\"utf-8\")\n"
      ],
      "metadata": {
        "id": "297KEIsCC_rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wkWd7TXxHwoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# 8F — Sentence counts per ID (robust types)\n",
        "# ===============================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "_sid_num = pd.to_numeric(df_map[\"CorrSentenceID\"], errors=\"coerce\")\n",
        "max_sid = _sid_num.groupby(df_map[\"ID\"]).max()\n",
        "num_sentences = (max_sid.fillna(-1) + 1).astype(int).clip(lower=0)\n",
        "\n",
        "print(\"Sentence counts per ID (first 10):\")\n",
        "print(num_sentences.head(10).rename(\"NumSentences\"))\n",
        "\n",
        "# Compact sample (safe sorts even if corr_index missing)\n",
        "sort_cols = [\"ID\"]\n",
        "if \"CorrSentenceID\" in df_map.columns: sort_cols.append(\"CorrSentenceID\")\n",
        "if \"corr_index\" in df_map.columns:     sort_cols.append(\"corr_index\")\n",
        "\n",
        "cols_to_show = [c for c in [\"ID\",\"CorrSentenceID\",\"corr_index\",\"corr_token\",\"op\",\"error_type\"] if c in df_map.columns]\n",
        "sample = (df_map.sort_values(sort_cols, kind=\"mergesort\")\n",
        "               .groupby([\"ID\",\"CorrSentenceID\"], group_keys=False)\n",
        "               .head(8)[cols_to_show])\n",
        "\n",
        "print(\"\\nSample tokens per sentence (first ~50 rows):\")\n",
        "print(sample.head(50).to_string(index=False))\n",
        "\n"
      ],
      "metadata": {
        "id": "RYaFVNWR_3pZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# 8G — Post-pass: move stray opening quotes to the next sentence\n",
        "# Works directly on df_map and CorrSentenceID\n",
        "# ===============================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "if \"df_map\" not in globals():\n",
        "    raise NameError(\"df_map is not defined.\")\n",
        "\n",
        "REQUIRED = {\"ID\",\"CorrSentenceID\",\"corr_token\"}\n",
        "missing = REQUIRED - set(df_map.columns)\n",
        "if missing:\n",
        "    raise KeyError(f\"Missing columns for quote fix: {missing}\")\n",
        "\n",
        "df = df_map.copy()\n",
        "df.sort_values([\"ID\",\n",
        "                \"CorrSentenceID\" if \"CorrSentenceID\" in df.columns else \"ID\",\n",
        "                \"corr_index\" if \"corr_index\" in df.columns else df.index.name or \"ID\"],\n",
        "               inplace=True, kind=\"mergesort\")\n",
        "\n",
        "ids   = df[\"ID\"].to_numpy()\n",
        "sids  = pd.to_numeric(df[\"CorrSentenceID\"], errors=\"coerce\").to_numpy()\n",
        "toks  = df[\"corr_token\"].astype(str).to_numpy()\n",
        "\n",
        "def _likely_ascii_opening(prev_tok, next_tok):\n",
        "    TERMINALS = {\".\", \"!\", \"?\", \"…\", \"...\", \"?!\", \"!?\"}\n",
        "    CLOSERS   = {\")\", \"]\", \"}\", \"”\", \"’\", \"»\"}\n",
        "    OPENERS   = {\"(\", \"[\", \"{\", \"“\", \"‘\", \"«\"}\n",
        "    prev = prev_tok.strip()\n",
        "    nxt  = next_tok.strip()\n",
        "    if prev == \"\" or prev in TERMINALS or prev in OPENERS:\n",
        "        return True\n",
        "    if nxt and nxt not in TERMINALS and nxt not in CLOSERS:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "moved = np.zeros(len(df), dtype=bool)\n",
        "\n",
        "i = 0\n",
        "n = len(df)\n",
        "while i < n:\n",
        "    j = i + 1\n",
        "    while j < n and ids[j] == ids[i]:\n",
        "        j += 1\n",
        "\n",
        "    for k in range(i, j):\n",
        "        tok = toks[k]\n",
        "        if tok not in {'\"', '“', '‘', '«'}:\n",
        "            continue\n",
        "\n",
        "        cur_sid = sids[k]\n",
        "        nxt = k + 1\n",
        "        if nxt >= j:\n",
        "            continue\n",
        "\n",
        "        next_sid = sids[nxt]\n",
        "        next_tok = toks[nxt]\n",
        "\n",
        "        # If the very next token is on a newer sentence, this quote is leading noise\n",
        "        if next_sid > cur_sid:\n",
        "            if tok in {'“', '‘', '«'} or (tok == '\"' and _likely_ascii_opening(toks[k-1] if k > i else \"\", next_tok)):\n",
        "                sids[k] = next_sid\n",
        "                moved[k] = True\n",
        "\n",
        "    i = j\n",
        "\n",
        "df_map[\"CorrSentenceID\"] = sids\n",
        "moved_count = int(moved.sum())\n",
        "total_quotes = int(np.isin(toks, ['\"', '“', '”', '‘', '’', '«', '»']).sum())\n",
        "print(f\"Moved opening quotes: {moved_count} of {total_quotes} quote tokens.\")\n",
        "df_map.to_csv(\"/content/step8_wordmap_checked.csv\", index=False, encoding=\"utf-8\")\n"
      ],
      "metadata": {
        "id": "clF5KFLTDZd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compact preview - a few rows per ID\n",
        "import pandas as pd\n",
        "\n",
        "need = {\"ID\", \"CorrSentenceID\", \"RowID\", \"corr_index\", \"corr_token\", \"op\", \"error_type\"}\n",
        "show = [c for c in [\"ID\",\"CorrSentenceID\",\"RowID\",\"corr_index\",\"corr_token\",\"op\",\"error_type\"] if c in df_map.columns]\n",
        "\n",
        "print(\"Columns present:\", show)\n",
        "\n",
        "sample_per_id = (\n",
        "    df_map\n",
        "      .sort_values([\"ID\",\"CorrSentenceID\",\"corr_index\"], kind=\"mergesort\")\n",
        "      .groupby(\"ID\", group_keys=False)\n",
        "      .head(8)   # up to 8 tokens per ID - bump if you want more\n",
        "      [show]\n",
        ")\n",
        "\n",
        "print(sample_per_id.to_string(index=False)[:5000])  # trim long output\n"
      ],
      "metadata": {
        "id": "pAdaPJcz-SFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 8T: Title detection + quote surgery (post-pass on df_map) ---\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load from disk if needed (adjust the path as necessary)\n",
        "if \"df_map\" not in globals():\n",
        "    PATH = \"/content/step8_wordmap_checked.csv\"  # or your /mnt/data/... path\n",
        "    df_map = pd.read_csv(PATH, dtype={\"ID\": str, \"RowID\": str}, low_memory=False)\n",
        "\n",
        "# --- helpers ---\n",
        "TERMINALS = {\".\", \"!\", \"?\", \"…\", \"...\", \"?!\", \"!?\"}\n",
        "CLOSERS   = {\")\", \"]\", \"}\", \"”\", \"’\", \"»\"}\n",
        "OPENERS   = {\"(\", \"[\", \"{\", \"“\", \"‘\", \"«\"}\n",
        "\n",
        "WORD_RX = re.compile(r\"\\w\", flags=re.UNICODE)\n",
        "\n",
        "def is_word(tok: str) -> bool:\n",
        "    return bool(tok) and bool(WORD_RX.search(tok))\n",
        "\n",
        "def is_ascii_quote_opening(prev_tok: str, next_tok: str) -> bool:\n",
        "    prev = (prev_tok or \"\").strip()\n",
        "    nxt  = (next_tok or \"\").strip()\n",
        "    if prev == \"\" or prev in TERMINALS or prev in OPENERS:\n",
        "        return True\n",
        "    if nxt and (nxt not in TERMINALS) and (nxt not in CLOSERS):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def is_titleish(tokens):\n",
        "    \"\"\"\n",
        "    Heuristic: short, no hard terminal, mostly TitleCase/ALLCAPS tokens, few stopwords.\n",
        "    Safe defaults; tweak thresholds as you like.\n",
        "    \"\"\"\n",
        "    words = [t for t in tokens if is_word(t)]\n",
        "    if not words:\n",
        "        return False\n",
        "    n = len(words)\n",
        "    if not (1 <= n <= 12):\n",
        "        return False\n",
        "\n",
        "    # Ends with obvious terminal? probably not a title.\n",
        "    if tokens and tokens[-1] in TERMINALS:\n",
        "        return False\n",
        "\n",
        "    # Capitalization signal\n",
        "    def cap_score(w):\n",
        "        # TitleCase, ALLCAPS, MixedCaps score 1; lowercase score 0\n",
        "        return 1 if (w.isupper() or (w[:1].isupper() and w[1:].islower()) or any(c.isupper() for c in w[1:])) else 0\n",
        "    cap_ratio = (sum(cap_score(w) for w in words) / max(1, n))\n",
        "\n",
        "    # tiny stopword penalty\n",
        "    STOP = {\"the\",\"a\",\"an\",\"and\",\"or\",\"but\",\"to\",\"of\",\"in\",\"for\",\"on\",\"with\",\"at\",\"by\",\"from\",\"as\"}\n",
        "    stop_ratio = (sum(w.lower() in STOP for w in words) / max(1, n))\n",
        "\n",
        "    return (cap_ratio >= 0.6) and (stop_ratio <= 0.5)\n",
        "\n",
        "# Ensure types we need\n",
        "for col in [\"ID\",\"CorrSentenceID\",\"corr_index\",\"corr_token\"]:\n",
        "    if col not in df_map.columns:\n",
        "        raise KeyError(f\"Missing required column: {col}\")\n",
        "\n",
        "df_map[\"ID\"] = df_map[\"ID\"].astype(str)\n",
        "if df_map[\"CorrSentenceID\"].dtype.kind not in \"iu\":\n",
        "    df_map[\"CorrSentenceID\"] = pd.to_numeric(df_map[\"CorrSentenceID\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "# --- 1) Title detection & reindexing of CorrSentenceID ---\n",
        "df_map[\"TITLE\"] = False  # initialize\n",
        "\n",
        "def reassign_title_sentence(g: pd.DataFrame) -> pd.DataFrame:\n",
        "    g = g.sort_values([\"CorrSentenceID\",\"corr_index\"], kind=\"mergesort\").copy()\n",
        "\n",
        "    # Consider the very first sentence as potential title\n",
        "    head = g[g[\"CorrSentenceID\"] == g[\"CorrSentenceID\"].min()]\n",
        "\n",
        "    # Pull the actual token string sequence for that first sentence\n",
        "    toks = head[\"corr_token\"].astype(str).tolist()\n",
        "\n",
        "    if is_titleish(toks):\n",
        "        # mark title\n",
        "        g.loc[head.index, \"TITLE\"] = True\n",
        "\n",
        "        # If it's not already sentence 0, normalize to 0 and shift others +1\n",
        "        min_sid = g[\"CorrSentenceID\"].min()\n",
        "        if min_sid != 0:\n",
        "            g[\"CorrSentenceID\"] = g[\"CorrSentenceID\"] - min_sid  # normalize so title is 0\n",
        "        # Ensure everything after title bumps by +1 (title stays 0)\n",
        "        g.loc[g[\"CorrSentenceID\"] > 0, \"CorrSentenceID\"] += 1\n",
        "        # title remains 0\n",
        "    else:\n",
        "        # no title: ensure sids are 0..N contiguous (normalize)\n",
        "        min_sid = g[\"CorrSentenceID\"].min()\n",
        "        if min_sid != 0:\n",
        "            g[\"CorrSentenceID\"] = g[\"CorrSentenceID\"] - min_sid\n",
        "\n",
        "    return g\n",
        "\n",
        "df_map = (\n",
        "    df_map\n",
        "      .groupby(\"ID\", group_keys=False)\n",
        "      .apply(reassign_title_sentence)\n",
        "      .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# --- 2) Quote marriage: fix closing/opening quotes at sentence edges ---\n",
        "\n",
        "def fix_quotes(g: pd.DataFrame) -> pd.DataFrame:\n",
        "    g = g.sort_values([\"CorrSentenceID\",\"corr_index\"], kind=\"mergesort\").copy()\n",
        "    sids = g[\"CorrSentenceID\"].to_numpy()\n",
        "    toks = g[\"corr_token\"].astype(str).to_numpy()\n",
        "\n",
        "    n = len(g)\n",
        "    if n == 0:\n",
        "        return g\n",
        "\n",
        "    # Pass A: pull stray closing quotes that start a sentence back to previous sentence\n",
        "    # e.g., [..., '.', SID=0] , ['”', SID=1]  -> make '”' SID=0\n",
        "    for i in range(1, n):\n",
        "        if toks[i] in {'\"', '”', '’', '»'} and sids[i] > sids[i-1]:\n",
        "            # Only if previous token looks like a sentence-ending token\n",
        "            if toks[i-1] in TERMINALS or toks[i-1] in CLOSERS or not is_ascii_quote_opening(toks[i-1], toks[i]):\n",
        "                sids[i] = sids[i-1]\n",
        "\n",
        "    # Pass B: keep opening quotes at the start of a sentence with that sentence\n",
        "    # If an opening quote was left attached to previous sentence, move it forward.\n",
        "    for i in range(1, n):\n",
        "        if toks[i] == '\"':\n",
        "            prev_tok = toks[i-1]\n",
        "            next_tok = toks[i+1] if i+1 < n else \"\"\n",
        "            if is_ascii_quote_opening(prev_tok, next_tok) and sids[i] == sids[i-1]:\n",
        "                # If looks like an opening quote but stuck to previous sid, nudge it forward if possible\n",
        "                # Only shift when the next token already starts a newer sentence\n",
        "                if i+1 < n and sids[i+1] > sids[i]:\n",
        "                    sids[i] = sids[i+1]\n",
        "\n",
        "    # Pass C: conventional .” or !” patterns — keep the terminal and the quote together in the same sentence\n",
        "    for i in range(1, n):\n",
        "        # Case: terminal at i-1, closing quote at i → keep same SID (already true by Pass A usually)\n",
        "        if toks[i-1] in TERMINALS and toks[i] in {'\"', '”', '’', '»'} and sids[i] != sids[i-1]:\n",
        "            sids[i] = sids[i-1]\n",
        "\n",
        "        # Case: closing quote at i-1, terminal at i → also keep together\n",
        "        if toks[i-1] in {'\"', '”', '’', '»'} and toks[i] in TERMINALS and sids[i] != sids[i-1]:\n",
        "            sids[i] = sids[i-1]\n",
        "\n",
        "    g[\"CorrSentenceID\"] = sids\n",
        "    return g\n",
        "\n",
        "df_map = (\n",
        "    df_map\n",
        "      .groupby(\"ID\", group_keys=False)\n",
        "      .apply(fix_quotes)\n",
        "      .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# Optional: persist a checkpoint\n",
        "out_path = \"/content/step8_wordmap_checked_title_quotes.csv\"\n",
        "df_map.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
        "print(\"✅ Updated map saved:\", out_path)\n",
        "\n",
        "# Quick audit\n",
        "print(\"Sample: IDs → max CorrSentenceID\")\n",
        "print(df_map.groupby(\"ID\")[\"CorrSentenceID\"].max().head(10))\n",
        "print(\"\\nTITLE counts:\")\n",
        "print(df_map.groupby([\"ID\",\"TITLE\"]).size().unstack(fill_value=0).head(10))\n"
      ],
      "metadata": {
        "id": "AMP4tSWGMKfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8H Full head and a few stats\n",
        "display(df_map.head(20))\n",
        "\n",
        "print(\"\\nCounts by ID and sentence:\")\n",
        "print(df_map.groupby([\"ID\",\"CorrSentenceID\"]).size().head(20))\n",
        "\n",
        "print(\"\\nMax CorrSentenceID per ID:\")\n",
        "print(df_map.groupby(\"ID\")[\"CorrSentenceID\"].max().head(20))\n"
      ],
      "metadata": {
        "id": "djH_iIS5-YkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 8Y — Sentence boundary flags + correctness checks\n",
        "# =========================\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- requirements ---\n",
        "need = {\"ID\",\"CorrSentenceID\",\"corr_index\",\"corr_token\",\"raw_token\"}\n",
        "missing = need - set(df_map.columns)\n",
        "if missing:\n",
        "    raise KeyError(f\"df_map missing required columns: {missing}\")\n",
        "\n",
        "# sort for stable per-sentence order\n",
        "df_map = df_map.sort_values([\"ID\",\"CorrSentenceID\",\"corr_index\"], kind=\"mergesort\").copy()\n",
        "\n",
        "# initialize target columns\n",
        "if \"Sentence Boundaries\" not in df_map.columns:\n",
        "    df_map[\"Sentence Boundaries\"] = \"\"\n",
        "if \"BoundaryCheck\" not in df_map.columns:\n",
        "    df_map[\"BoundaryCheck\"] = \"\"\n",
        "\n",
        "# helpers\n",
        "TERMINALS = {\".\",\"!\",\"?\",\"…\",\"...\",\"?!\",\"!?\"}\n",
        "OPENING_PUNCT = {'\"', \"“\", \"‘\", \"«\", \"(\", \"[\", \"{\"}\n",
        "\n",
        "def is_word(tok: str) -> bool:\n",
        "    return bool(tok) and bool(re.search(r\"\\w\", str(tok), flags=re.UNICODE))\n",
        "\n",
        "def first_content_row(g: pd.DataFrame) -> int | None:\n",
        "    \"\"\"\n",
        "    Find the row index (label) of the first *word* token in this sentence,\n",
        "    skipping opening punctuation like \\\" or ( if they occur first.\n",
        "    \"\"\"\n",
        "    for idx, tok in zip(g.index, g[\"corr_token\"].astype(str)):\n",
        "        if is_word(tok):\n",
        "            return idx\n",
        "        # if it's opening punctuation, keep scanning\n",
        "        if tok in OPENING_PUNCT:\n",
        "            continue\n",
        "        # if it's other punctuation, keep scanning until we hit a word\n",
        "        continue\n",
        "    return None\n",
        "\n",
        "def last_boundary_row(g: pd.DataFrame) -> int | None:\n",
        "    \"\"\"\n",
        "    Find the row index (label) of the final sentence-boundary marker token\n",
        "    in corr_token within this sentence (., !, ?, …, ... , ?!, !?).\n",
        "    If none present, return None (we won’t mark an ending).\n",
        "    \"\"\"\n",
        "    # Go from the end toward the start\n",
        "    toks = g[\"corr_token\"].astype(str).tolist()\n",
        "    for pos in range(len(toks)-1, -1, -1):\n",
        "        t = toks[pos]\n",
        "        if t in TERMINALS:\n",
        "            return g.index[pos]\n",
        "    return None\n",
        "\n",
        "def begins_with_upper(raw_tok: str) -> bool | None:\n",
        "    \"\"\"\n",
        "    True if the first alphabetic char in the *raw* token is uppercase.\n",
        "    Returns None if no alphabetic char (e.g., insertion or pure punctuation).\n",
        "    \"\"\"\n",
        "    s = str(raw_tok or \"\")\n",
        "    m = re.search(r\"[A-Za-z]\", s)\n",
        "    if not m:\n",
        "        return None\n",
        "    return s[m.start()].isupper()\n",
        "\n",
        "# work per sentence\n",
        "for (id_, sid), g in df_map.groupby([\"ID\",\"CorrSentenceID\"], sort=False):\n",
        "    g = g.sort_values(\"corr_index\", kind=\"mergesort\")\n",
        "    b_row = first_content_row(g)\n",
        "    e_row = last_boundary_row(g)\n",
        "\n",
        "    # mark boundaries\n",
        "    marks = {}\n",
        "    if b_row is not None:\n",
        "        marks.setdefault(b_row, []).append(\"Sentence Beginning\")\n",
        "    if e_row is not None:\n",
        "        marks.setdefault(e_row, []).append(\"Sentence Ending\")\n",
        "\n",
        "    # apply boundary marks\n",
        "    for idx, tags in marks.items():\n",
        "        prev = df_map.at[idx, \"Sentence Boundaries\"]\n",
        "        tag = \" | \".join(tags)\n",
        "        df_map.at[idx, \"Sentence Boundaries\"] = tag if not prev else (prev + \" | \" + tag)\n",
        "\n",
        "    # correctness checks\n",
        "    # 1) Beginning correctness (capitalisation of RAW token on the beginning row)\n",
        "    if b_row is not None:\n",
        "        rb = df_map.at[b_row, \"raw_token\"]\n",
        "        cap = begins_with_upper(rb)\n",
        "        if cap is True:\n",
        "            v = \"Correct Beginning\"\n",
        "        elif cap is False:\n",
        "            v = \"Incorrect Beginning\"\n",
        "        else:\n",
        "            v = \"Unknown Beginning\"\n",
        "        prev = df_map.at[b_row, \"BoundaryCheck\"]\n",
        "        df_map.at[b_row, \"BoundaryCheck\"] = v if not prev else (prev + \" | \" + v)\n",
        "\n",
        "    # 2) Ending correctness (RAW token equals CORR token on the ending row)\n",
        "    if e_row is not None:\n",
        "        re_tok = str(df_map.at[e_row, \"raw_token\"] or \"\")\n",
        "        ce_tok = str(df_map.at[e_row, \"corr_token\"] or \"\")\n",
        "        # exact match requested (punct/quotes matter)\n",
        "        end_ok = (re_tok == ce_tok)\n",
        "        v = \"Correct Ending\" if end_ok else \"Incorrect Ending\"\n",
        "        prev = df_map.at[e_row, \"BoundaryCheck\"]\n",
        "        df_map.at[e_row, \"BoundaryCheck\"] = v if not prev else (prev + \" | \" + v)\n",
        "\n",
        "# optional: ensure a stable SentenceRef if you want to join with sentence table\n",
        "def _sid_str(x):\n",
        "    try: return f\"{int(x):03d}\"\n",
        "    except: return \"000\"\n",
        "if \"SentenceRef\" not in df_map.columns:\n",
        "    df_map[\"SentenceRef\"] = df_map[\"ID\"].astype(str) + \"_s\" + df_map[\"CorrSentenceID\"].map(_sid_str)\n",
        "\n",
        "# quick peek\n",
        "display(df_map.head(20)[[\n",
        "    \"ID\",\"CorrSentenceID\",\"corr_index\",\n",
        "    \"raw_token\",\"corr_token\",\n",
        "    \"Sentence Boundaries\",\"BoundaryCheck\"\n",
        "]])\n"
      ],
      "metadata": {
        "id": "LIgBhF4784u2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Patch A: Sentence boundaries + checks + SentenceRef on df_map ===\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "REQUIRED = {\"ID\",\"CorrSentenceID\",\"corr_token\"}\n",
        "missing = REQUIRED - set(df_map.columns)\n",
        "if missing:\n",
        "    raise KeyError(f\"df_map missing required columns: {missing}\")\n",
        "\n",
        "# stable sort (works even if corr_index has NaNs/gaps)\n",
        "if \"corr_index\" not in df_map.columns:\n",
        "    df_map[\"corr_index\"] = np.nan\n",
        "df_map[\"_rowpos\"] = np.arange(len(df_map))\n",
        "df_map[\"_sort_corr\"] = pd.to_numeric(df_map[\"corr_index\"], errors=\"coerce\").fillna(1e12) + (df_map[\"_rowpos\"]*1e-9)\n",
        "df_map = df_map.sort_values([\"ID\",\"CorrSentenceID\",\"_sort_corr\"], kind=\"mergesort\")\n",
        "\n",
        "# columns to fill\n",
        "if \"Sentence Boundaries\" not in df_map.columns:\n",
        "    df_map[\"Sentence Boundaries\"] = \"\"\n",
        "if \"BoundaryCheck\" not in df_map.columns:\n",
        "    df_map[\"BoundaryCheck\"] = \"\"\n",
        "\n",
        "# helpers\n",
        "TERMINALS = {\".\",\"!\",\"?\",\"…\",\"...\",\"?!\",\"!?\"}\n",
        "OPENING_PUNCT = {'\"', \"“\", \"‘\", \"«\", \"(\", \"[\", \"{\"}\n",
        "\n",
        "def is_word(tok: str) -> bool:\n",
        "    return bool(tok) and bool(re.search(r\"\\w\", str(tok), flags=re.UNICODE))\n",
        "\n",
        "def first_content_row(g: pd.DataFrame):\n",
        "    for idx, tok in zip(g.index, g[\"corr_token\"].astype(str)):\n",
        "        if tok in OPENING_PUNCT:\n",
        "            continue\n",
        "        if is_word(tok):\n",
        "            return idx\n",
        "    return None\n",
        "\n",
        "def last_terminal_row(g: pd.DataFrame):\n",
        "    toks = g[\"corr_token\"].astype(str).tolist()\n",
        "    for pos in range(len(toks)-1, -1, -1):\n",
        "        if toks[pos] in TERMINALS:\n",
        "            return g.index[pos]\n",
        "    return None\n",
        "\n",
        "def begins_with_upper_raw(raw_tok: str):\n",
        "    s = str(raw_tok or \"\")\n",
        "    m = re.search(r\"[A-Za-z]\", s)\n",
        "    if not m:\n",
        "        return None\n",
        "    return s[m.start()].isupper()\n",
        "\n",
        "# fill per sentence\n",
        "for (id_, sid), g in df_map.groupby([\"ID\",\"CorrSentenceID\"], sort=False):\n",
        "    g = g.sort_values(\"_sort_corr\", kind=\"mergesort\")\n",
        "    b = first_content_row(g)\n",
        "    e = last_terminal_row(g)\n",
        "\n",
        "    if b is not None:\n",
        "        prev = df_map.at[b, \"Sentence Boundaries\"]\n",
        "        df_map.at[b, \"Sentence Boundaries\"] = prev + (\" | \" if prev else \"\") + \"Sentence Beginning\"\n",
        "        # correctness on the beginning row uses RAW token capitalisation\n",
        "        rawb = df_map.at[b, \"raw_token\"] if \"raw_token\" in df_map.columns else None\n",
        "        cap = begins_with_upper_raw(rawb)\n",
        "        tag = \"Correct Beginning\" if cap is True else (\"Incorrect Beginning\" if cap is False else \"Unknown Beginning\")\n",
        "        prev = df_map.at[b, \"BoundaryCheck\"]\n",
        "        df_map.at[b, \"BoundaryCheck\"] = prev + (\" | \" if prev else \"\") + tag\n",
        "\n",
        "    if e is not None:\n",
        "        prev = df_map.at[e, \"Sentence Boundaries\"]\n",
        "        df_map.at[e, \"Sentence Boundaries\"] = prev + (\" | \" if prev else \"\") + \"Sentence Ending\"\n",
        "        rawe = str(df_map.at[e, \"raw_token\"] or \"\") if \"raw_token\" in df_map.columns else \"\"\n",
        "        corre = str(df_map.at[e, \"corr_token\"] or \"\")\n",
        "        tag = \"Correct Ending\" if rawe == corre else \"Incorrect Ending\"\n",
        "        prev = df_map.at[e, \"BoundaryCheck\"]\n",
        "        df_map.at[e, \"BoundaryCheck\"] = prev + (\" | \" if prev else \"\") + tag\n",
        "\n",
        "# stable sentence key for joins\n",
        "def _sid3(x):\n",
        "    try: return f\"{int(x):03d}\"\n",
        "    except: return \"000\"\n",
        "df_map[\"SentenceRef\"] = df_map[\"ID\"].astype(str) + \"_s\" + df_map[\"CorrSentenceID\"].map(_sid3)\n",
        "\n",
        "# cleanup temps\n",
        "df_map.drop(columns=[\"_rowpos\",\"_sort_corr\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "print(\"✅ Sentence boundaries + SentenceRef added to df_map.\")\n",
        "print(df_map[[\"ID\",\"CorrSentenceID\",\"SentenceRef\",\"corr_index\",\"raw_token\",\"corr_token\",\"Sentence Boundaries\",\"BoundaryCheck\"]]\n",
        "      .head(12).to_string(index=False))\n"
      ],
      "metadata": {
        "id": "_58xg_X1CsmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8I Save and download\n",
        "from google.colab import files\n",
        "\n",
        "out_path = \"/content/step8_wordmap_checked.csv\"\n",
        "df_map.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
        "print(\"Saved to:\", out_path)\n",
        "\n",
        "try:\n",
        "    files.download(out_path)\n",
        "except Exception as e:\n",
        "    print(\"If automatic download is blocked, grab it from the Files pane on the left.\")\n"
      ],
      "metadata": {
        "id": "bC9sizMC-bAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: Segment Text by Sentence\n",
        "## Purpose:\n",
        "To process raw text annd corrected text by segmenting it into sentences, structuring the results into a standardized format.\n",
        "\n",
        "## Actions:\n",
        "\n",
        "* Define segment_and_correct Function:\n",
        " * Parameters: Accepts text (the raw input text).\n",
        " * Prompt Creation: Constructs a detailed prompt with instructions for the AI to perform specific tasks on the text.\n",
        " * API Call: Uses the previously defined call_chatgpt function to send the prompt to the OpenAI API.\n",
        " * Response Handling: Extracts JSON from the API response and parses it into a Python dictionary.\n",
        " * Error Handling: Catches JSON decoding errors and returns an empty dictionary if parsing fails.\n",
        "*Mock Text for Testing:\n",
        "In Step 8.3, a mock version of segment_and_correct is used to simulate API behavior for testing purposes."
      ],
      "metadata": {
        "id": "PH6xO_PJwWXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 9 (drop-in): build sentence table using df_map’s flags ===\n",
        "import pandas as pd, numpy as np, re\n",
        "from IPython.display import display, HTML\n",
        "from google.colab import files\n",
        "\n",
        "# we expect df_map from Step 8 + Patch A\n",
        "NEED = {\"ID\",\"CorrSentenceID\",\"corr_token\",\"raw_token\",\"Sentence Boundaries\",\"BoundaryCheck\",\"SentenceRef\"}\n",
        "missing = NEED - set(df_map.columns)\n",
        "if missing:\n",
        "    raise KeyError(f\"df_map missing columns needed for Step 9: {missing}\")\n",
        "\n",
        "# sorting\n",
        "sort_cols = [\"ID\",\"CorrSentenceID\"]\n",
        "if \"corr_index\" in df_map.columns: sort_cols.append(\"corr_index\")\n",
        "wm = df_map.sort_values(sort_cols, kind=\"mergesort\").copy()\n",
        "\n",
        "NO_SPACE_BEFORE = set(list(\".,;:!?)]}\\\"'»”’…\"))\n",
        "NO_SPACE_AFTER  = set(list(\"([{\\\"'«“‘\"))\n",
        "\n",
        "def detok(tokens):\n",
        "    out = []\n",
        "    for t in tokens:\n",
        "        if t is None or pd.isna(t):\n",
        "            continue\n",
        "        t = str(t)\n",
        "        if not out:\n",
        "            out.append(t); continue\n",
        "        prev = out[-1]\n",
        "        if t in NO_SPACE_BEFORE or re.fullmatch(r\"[.]{3}\", t):\n",
        "            out[-1] = prev + t\n",
        "        elif prev in NO_SPACE_AFTER:\n",
        "            out[-1] = prev + t\n",
        "        else:\n",
        "            out.append(\" \" + t)\n",
        "    return \"\".join(out).strip()\n",
        "\n",
        "def summarize_sentence(g: pd.DataFrame):\n",
        "    corr_tokens = g[\"corr_token\"].tolist()\n",
        "    raw_tokens  = [x for x in g[\"raw_token\"].tolist() if not pd.isna(x)]\n",
        "    corr_text   = detok(corr_tokens)\n",
        "    raw_text    = detok(raw_tokens)\n",
        "\n",
        "    # flags from boundary rows (may be empty if heuristics didn’t find one)\n",
        "    b_rows = g[g[\"Sentence Boundaries\"].str.contains(\"Sentence Beginning\", na=False)]\n",
        "    e_rows = g[g[\"Sentence Boundaries\"].str.contains(\"Sentence Ending\",   na=False)]\n",
        "\n",
        "    begin_ok = np.nan\n",
        "    end_ok   = np.nan\n",
        "    if not b_rows.empty:\n",
        "        chk = \" | \".join(b_rows[\"BoundaryCheck\"].dropna().astype(str))\n",
        "        begin_ok = ((\"Correct Beginning\" in chk) * 1) if chk else np.nan\n",
        "    if not e_rows.empty:\n",
        "        chk = \" | \".join(e_rows[\"BoundaryCheck\"].dropna().astype(str))\n",
        "        end_ok = ((\"Correct Ending\" in chk) * 1) if chk else np.nan\n",
        "\n",
        "    # convenience booleans\n",
        "    has_hard_terminal = any(t in {\".\",\"!\",\"?\",\"…\",\"...\",\"?!\",\"!?\"} for t in corr_tokens)\n",
        "    has_opening_quote = any(t in {'\"', \"“\", \"‘\", \"«\"} for t in corr_tokens)\n",
        "    has_closing_quote = any(t in {'\"', \"”\", \"’\", \"»\"} for t in corr_tokens)\n",
        "\n",
        "    # edits summary (if present)\n",
        "    op  = g[\"op\"] if \"op\" in g.columns else pd.Series([], dtype=object)\n",
        "    et  = g[\"error_type\"] if \"error_type\" in g.columns else pd.Series([], dtype=object)\n",
        "\n",
        "    return pd.Series({\n",
        "        \"SentenceRef\": g[\"SentenceRef\"].iloc[0],\n",
        "        \"CorrectedSentence\": corr_text,\n",
        "        \"RawSentence\": raw_text,\n",
        "        \"TokensInSentence\": int(len(g)),\n",
        "        \"EditsInSentence\": int((op != \"equal\").sum()) if not op.empty else np.nan,\n",
        "        \"EqualsInSentence\": int((op == \"equal\").sum()) if not op.empty else np.nan,\n",
        "        \"Insertions\": int((op == \"insert\").sum()) if not op.empty else np.nan,\n",
        "        \"Deletions\": int((op == \"delete\").sum()) if not op.empty else np.nan,\n",
        "        \"Replacements\": int((op == \"replace\").sum()) if not op.empty else np.nan,\n",
        "        \"BeginBoundaryRow\": (b_rows.index[0] if not b_rows.empty else np.nan),\n",
        "        \"EndBoundaryRow\":   (e_rows.index[0] if not e_rows.empty else np.nan),\n",
        "        \"CorrectBeginning\": begin_ok,        # 1=true, 0=false, NaN=unknown/missing\n",
        "        \"CorrectEnding\":    end_ok,          # 1=true, 0=false, NaN=missing\n",
        "        \"HasHardTerminal\":  bool(has_hard_terminal),\n",
        "        \"HasOpeningQuote\":  bool(has_opening_quote),\n",
        "        \"HasClosingQuote\":  bool(has_closing_quote),\n",
        "        \"CorrIndexMin\": g[\"corr_index\"].min() if \"corr_index\" in g.columns else np.nan,\n",
        "        \"CorrIndexMax\": g[\"corr_index\"].max() if \"corr_index\" in g.columns else np.nan,\n",
        "    })\n",
        "\n",
        "sent_df = (\n",
        "    wm.groupby([\"ID\",\"CorrSentenceID\"], as_index=False, sort=False)\n",
        "      .apply(summarize_sentence)\n",
        "      .reset_index(drop=True)\n",
        "      .sort_values([\"ID\",\"SentenceRef\"], kind=\"mergesort\")\n",
        ")\n",
        "\n",
        "# Save + preview\n",
        "out_path = \"/content/step9_sentence_mapping_with_boundaries.csv\"\n",
        "sent_df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
        "print(\"✅ Step 9 sentence mapping saved:\", out_path)\n",
        "\n",
        "display(HTML(\"<b>Sentence mapping preview (with boundary flags)</b>\"))\n",
        "display(sent_df.head(50))\n",
        "\n",
        "try:\n",
        "    files.download(out_path)\n",
        "except Exception:\n",
        "    print(\"If the download is blocked, fetch it from the Files pane:\", out_path)\n"
      ],
      "metadata": {
        "id": "WOpnfK35CxEr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMRXCxetw16+oL3NJXkGNdd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d4f89076d0014a87b9582ed6cb558263": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64a2e17abeed4e9fbc896414eb641db4",
            "placeholder": "​",
            "style": "IPY_MODEL_47f43b36aa64429494e8d36ffe9f61a8",
            "value": "<h4>Download a CSV Template</h4>"
          }
        },
        "64a2e17abeed4e9fbc896414eb641db4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47f43b36aa64429494e8d36ffe9f61a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7cd87b1198a44966b2faf9fad40121e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35e9e180931444ffb9218a5febe1bac4",
            "placeholder": "​",
            "style": "IPY_MODEL_5a5d436b1ca347ab91f8c58a50316d03",
            "value": "Use the first column as <b>ID</b> and the last as <b>Raw text</b>.<br>Everything in between is optional."
          }
        },
        "35e9e180931444ffb9218a5febe1bac4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a5d436b1ca347ab91f8c58a50316d03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92625d59828e4ae9bfa197d3c2771102": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72b9e7a3158b4c61a97ce17495f1b810",
              "IPY_MODEL_6ae7e2b733924c4a8d198873ad11d2f2"
            ],
            "layout": "IPY_MODEL_0f8569feb49a4581a179b0349898aea3"
          }
        },
        "72b9e7a3158b4c61a97ce17495f1b810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "primary",
            "description": "⬇️ Download Minimal Template (ID + Raw text)",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_8cb21460dc8140f6adc8c6f09db4e81e",
            "style": "IPY_MODEL_7bedc70a3b9748789c1e4b1e6bdf62e0",
            "tooltip": ""
          }
        },
        "6ae7e2b733924c4a8d198873ad11d2f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "info",
            "description": "⬇️ Download Extended Template (Full NAPLAN-style)",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_26769bfa26c0453f8f9a4d9386f65fae",
            "style": "IPY_MODEL_cbf8a378dcc44df0822d086935dfe783",
            "tooltip": ""
          }
        },
        "0f8569feb49a4581a179b0349898aea3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cb21460dc8140f6adc8c6f09db4e81e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bedc70a3b9748789c1e4b1e6bdf62e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "26769bfa26c0453f8f9a4d9386f65fae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbf8a378dcc44df0822d086935dfe783": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "7a413d2005ac4f2cbcc5e61ead188a60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff522767bea043f89ee150d71167adbb",
              "IPY_MODEL_f7ea14bca55e4fda98460c224eecf8ea"
            ],
            "layout": "IPY_MODEL_79178bb01ca64c59b35aa0b30b74503b"
          }
        },
        "ff522767bea043f89ee150d71167adbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "CSV",
              "Excel (.xlsx)"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Format:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_434d5481eb5543519ad64973b9d9d45a",
            "style": "IPY_MODEL_c6a517473f714b5a9e9714d4e82d953f"
          }
        },
        "f7ea14bca55e4fda98460c224eecf8ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "primary",
            "description": "Download DataFrame now",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_aa3568f0b58848cd9ddd8785c1c4c356",
            "style": "IPY_MODEL_e2960dfbce354f6e97986449102eb0b2",
            "tooltip": "Click to download the DataFrame with WordCount and TokenCount"
          }
        },
        "79178bb01ca64c59b35aa0b30b74503b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "434d5481eb5543519ad64973b9d9d45a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "240px"
          }
        },
        "c6a517473f714b5a9e9714d4e82d953f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa3568f0b58848cd9ddd8785c1c4c356": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2960dfbce354f6e97986449102eb0b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "5219c9f4cf214eada10d2cee9e77914d": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_f6c6d8f04637434e8ca943f020e4a205",
            "msg_id": "",
            "outputs": []
          }
        },
        "f6c6d8f04637434e8ca943f020e4a205": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27fcaf055eb1429db925e3c6c683d178": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6b8460e0c6424fc8b72d7604f862eadd",
              "IPY_MODEL_d5e8edc5018d421c8fe9f69feecf382d",
              "IPY_MODEL_7017c3a5f42e4947b35dbca068c97b10",
              "IPY_MODEL_2db8955796e54fb185461a83837d05b3",
              "IPY_MODEL_e09538b3076f44d48ef9c8977d505adc"
            ],
            "layout": "IPY_MODEL_ffbe3d0376de4810ad3d891c28b9d228"
          }
        },
        "6b8460e0c6424fc8b72d7604f862eadd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "LOG_FILE:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_d9238a7c4dda44689c2d99bcb4b8f664",
            "placeholder": "​",
            "style": "IPY_MODEL_d882205f28804c9790f518f99c9a259c",
            "value": "text_correction.log"
          }
        },
        "d5e8edc5018d421c8fe9f69feecf382d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff7d720013fc4549bc1363afc40853bb",
              "IPY_MODEL_c91a9bcfb177498dbf270e9130376904"
            ],
            "layout": "IPY_MODEL_9d3919a18ab5458da205ad354a0a8932"
          }
        },
        "7017c3a5f42e4947b35dbca068c97b10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_096a5c8749034475be7bb41eb0ffb7d9",
              "IPY_MODEL_31e6d092e79c47099de1e61cf6f56a04"
            ],
            "layout": "IPY_MODEL_b31adef57a01463189c274a7298e514b"
          }
        },
        "2db8955796e54fb185461a83837d05b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_27ab465b8c30476e858dac4778e8d69d",
              "IPY_MODEL_16323fc0498244b0bebb5f757592844e",
              "IPY_MODEL_9626750eb4944c90bead5f005db6dc90"
            ],
            "layout": "IPY_MODEL_6b2a29a858b24ca2aff04abb8b51cb4e"
          }
        },
        "e09538b3076f44d48ef9c8977d505adc": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_7a51a1f593fb4a1ea986286ece75ed85",
            "msg_id": "",
            "outputs": []
          }
        },
        "ffbe3d0376de4810ad3d891c28b9d228": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9238a7c4dda44689c2d99bcb4b8f664": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "420px"
          }
        },
        "d882205f28804c9790f518f99c9a259c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff7d720013fc4549bc1363afc40853bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "DEBUG (most verbose)",
              "INFO (standard)",
              "WARNING (only important)",
              "ERROR (failures only)",
              "CRITICAL"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Console:",
            "description_tooltip": null,
            "disabled": false,
            "index": 1,
            "layout": "IPY_MODEL_cfb6b36cd7374daf8ba866f9f0b0ea67",
            "style": "IPY_MODEL_bf674f21c260434387d7ab9bc1c285fc"
          }
        },
        "c91a9bcfb177498dbf270e9130376904": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "DEBUG (most verbose)",
              "INFO (standard)",
              "WARNING (only important)",
              "ERROR (failures only)",
              "CRITICAL"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "File:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_59ed1d3577b84053ac65ad5bfcfac94d",
            "style": "IPY_MODEL_8ea5835ed77444beaaefeab148a839e3"
          }
        },
        "9d3919a18ab5458da205ad354a0a8932": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "096a5c8749034475be7bb41eb0ffb7d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntSliderView",
            "continuous_update": false,
            "description": "Rotate MB:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_8954110dcfe844e88d4b33d76ae67951",
            "max": 50,
            "min": 1,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": "d",
            "step": 1,
            "style": "IPY_MODEL_53f82bd2f03c467981b2598c3d19a33f",
            "value": 5
          }
        },
        "31e6d092e79c47099de1e61cf6f56a04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntSliderView",
            "continuous_update": false,
            "description": "Backups:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_bf6193f186cd473bb5ffb70c7c5072fb",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": "d",
            "step": 1,
            "style": "IPY_MODEL_8c0f453912ad4f7bbc7bb185f7dc36e3",
            "value": 3
          }
        },
        "b31adef57a01463189c274a7298e514b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27ab465b8c30476e858dac4778e8d69d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "primary",
            "description": "Apply logging settings",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_6170857048a344b6889ed45df0882bbf",
            "style": "IPY_MODEL_2eb7c3e50526407c8395cbf25c673445",
            "tooltip": "Configure handlers and run a quick self-test"
          }
        },
        "16323fc0498244b0bebb5f757592844e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Show log tail",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_3330ec0e435f45779a6a56154410258a",
            "style": "IPY_MODEL_2002b828679d427bbdfc1cf4b1289ba9",
            "tooltip": "Display the last lines of the current log file"
          }
        },
        "9626750eb4944c90bead5f005db6dc90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Download log",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_5e5d9f96c0f94fa4a1fcefdc42be93c2",
            "style": "IPY_MODEL_ba1f49a8c1424d79bae7967563884e34",
            "tooltip": "Download the current log file"
          }
        },
        "6b2a29a858b24ca2aff04abb8b51cb4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfb6b36cd7374daf8ba866f9f0b0ea67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "420px"
          }
        },
        "bf674f21c260434387d7ab9bc1c285fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59ed1d3577b84053ac65ad5bfcfac94d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "420px"
          }
        },
        "8ea5835ed77444beaaefeab148a839e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8954110dcfe844e88d4b33d76ae67951": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53f82bd2f03c467981b2598c3d19a33f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "bf6193f186cd473bb5ffb70c7c5072fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c0f453912ad4f7bbc7bb185f7dc36e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "6170857048a344b6889ed45df0882bbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2eb7c3e50526407c8395cbf25c673445": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "3330ec0e435f45779a6a56154410258a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2002b828679d427bbdfc1cf4b1289ba9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "5e5d9f96c0f94fa4a1fcefdc42be93c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba1f49a8c1424d79bae7967563884e34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "7a51a1f593fb4a1ea986286ece75ed85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}